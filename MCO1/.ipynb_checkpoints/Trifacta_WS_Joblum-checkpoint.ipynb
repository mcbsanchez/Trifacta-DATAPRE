{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comfortable-timber",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Job Portal - JOBLUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-married",
   "metadata": {},
   "source": [
    "## Imports used (to be described)\n",
    "\n",
    "* `os` - a module that provides functions to interact with the operating system.\n",
    "* `pandas` - is a tool that helps analyze data.\n",
    "* `numpy` - Library that contains multiple functions that help ease the work with arrays, matrices, and alike to better reassemble data.\n",
    "* `json` - enables import and export from and to JSON files\n",
    "* `re` - Short for Regular Expressions, help recognize patterns on strings of data and is used to orderly reassemble them.\n",
    "* `gensim` - Library that efficiently handles large, unmanaged text collections of data.\n",
    "* `nltk` - Short for Natural Language Toolkit. It helps the program to apply human language data to statistical natural language.\n",
    "* `requests` - Requests allows the program to send HTTP requests easily.\n",
    "* `gensim.utils` `simple_preprocess` - used to preprocess text by making them lower-cased, and transforming the words to their original form (de-tokenizing)\n",
    "* `gensim.parsing.preprocessing` `STOPWORDS` - stop words common words that do not have value and are often removed in pre-processing\n",
    "* `gensim` `corpora` - used to work with corpus and words\n",
    "* `gensim` `models` - used for topic modelling and model training\n",
    "* `nltk.stem` `WordNetLemmatizer` - used for grouping similar strings together\n",
    "* `bs4` `BeautifulSoup` - library used to web scrape HTML from websites\n",
    "* `datetime` `datetime` - An imported module in python to create an object that properly resembles date and time. Used for converting string of time into datetime format to month, day, and year.\n",
    "* `datetime` `timedelta` - used for finding delta of time ago with time scraped if date has minutes, hours, days, or weeks ago\n",
    "* `dateutil.relativedelta` `relativedelta` - used for finding delta of time ago with time scraped if date has months and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liked-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-attraction",
   "metadata": {},
   "source": [
    "Joblum.com is an online career portal that features relevant and up-to-the-minute job listings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-fruit",
   "metadata": {},
   "source": [
    "### Check number of jobs in soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-interview",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get number of jobs per category (JOBLUM)\n",
    "def getNumJobs(soup):\n",
    "    NUM_JOBS = soup.find_all('p',{'class':'no-of-jobs'})\n",
    "    NUM_JOBS =  re.findall(r'(?s)(?<= of ).*?(?= jobs)',str(NUM_JOBS[1]))\n",
    "    if NUM_JOBS:\n",
    "        return int(NUM_JOBS[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-distinction",
   "metadata": {},
   "source": [
    "### Souptest\n",
    "\n",
    "Getting the html of the URL of the Information Communications Technology job openings, it can be observed that it contains the list of jobs that we interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the page and parsing HTML data (JOBLUM)\n",
    "def getSoup(JOBLUM_JOB_URL):\n",
    "    page = requests.get(JOBLUM_JOB_URL)\n",
    "    return BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-curve",
   "metadata": {},
   "source": [
    "### Check number of pages in soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number of pages (JOBLUM)\n",
    "def getNumPages(NUM_JOBS):\n",
    "    return math.ceil(NUM_JOBS/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-allowance",
   "metadata": {},
   "source": [
    "### Get links for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the links of each page (JOBLUM)\n",
    "def getLinks(Num_Pages, JOBLUM_URLs):\n",
    "    JOB_LINKS = []\n",
    "    for j in range(1,NUM_PAGES+1):\n",
    "        JOB_LINKS.append(JOBLUM_URLs + str(j))\n",
    "    return JOB_LINKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-arctic",
   "metadata": {},
   "source": [
    "### Get url for each job in each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting the URLs of each job posting (JOBLUM)\n",
    "def getJobURL(JOBLUM_SOUP):\n",
    "    JOBLUM_JOBS_URL = JOBLUM_SOUP.find_all('div',{'class':'mobile-company-logo hidden-md hidden-lg'})\n",
    "    return re.findall(r'(?s)(?<= href=\").*?(?=\" )',str(JOBLUM_JOBS_URL))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-venice",
   "metadata": {},
   "source": [
    "### Get the job description of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Description (JOBLUM)\n",
    "def getJobDescription(JOB_SOUP):\n",
    "    try:\n",
    "        JOB_INFO = JOB_SOUP.find('span',{'itemprop':'description'})\n",
    "        JOB_INFO_ARRAY = []\n",
    "        for n in range(len(JOB_INFO.contents[1].contents[0])):\n",
    "            if(isinstance(JOB_INFO.contents[1].contents[0].contents[n],NavigableString)):\n",
    "                JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n])\n",
    "            else:\n",
    "                for s in range(len(JOB_INFO.contents[1].contents[0].contents[n])):\n",
    "                    if(isinstance(JOB_INFO.contents[1].contents[0].contents[n].contents[s], Tag)):\n",
    "                        JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n].contents[s].text)\n",
    "                    else:\n",
    "                        JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n].contents[s])\n",
    "        JOB_DESCRIPTION = ' '.join(JOB_INFO_ARRAY)\n",
    "        JOB_DESCRIPTION = JOB_DESCRIPTION.replace(\"\\xa0\",\" \")\n",
    "        JOB_DESCRIPTION = re.sub(' +', ' ', JOB_DESCRIPTION) \n",
    "        return JOB_DESCRIPTION\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-christianity",
   "metadata": {},
   "source": [
    "### Get the job salary of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Salary (JOBLUM)\n",
    "def getJobSalary(JOB_SOUP):\n",
    "    JOB_INFO = JOB_SOUP.find('span',{'itemprop':'description'})\n",
    "    JOB_SALARY_FINDER = JOB_SOUP.find('p',{'class':'job-subinfo'})\n",
    "    JOB_SALARY_ARRAY = re.findall('[0-9]+,[0-9]+',str(JOB_SALARY_FINDER))\n",
    "    JOB_SALARY = \"\"\n",
    "    JOB_SALARY_MIN = \"Not Specified\"\n",
    "    JOB_SALARY_MAX = \"Not Specified\"\n",
    "    JOB_SALARY_CHECKER = re.findall('(?i)Salary',str(JOB_SALARY_FINDER))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)Salary',str(JOB_INFO))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)PHP',str(JOB_INFO))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)pesos',str(JOB_INFO))\n",
    "    if not JOB_SALARY_ARRAY:\n",
    "        JOB_SALARY_ARRAY = re.findall('[0-9]+,[0-9]+',str(JOB_INFO))\n",
    "    if JOB_SALARY_CHECKER:\n",
    "        if (len(JOB_SALARY_ARRAY)==2):\n",
    "            if (int(JOB_SALARY_ARRAY[1].replace(\",\",\"\")) > int(JOB_SALARY_ARRAY[0].replace(\",\",\"\"))):\n",
    "                if (int(JOB_SALARY_ARRAY[1].replace(\",\",\"\")) > 1000):\n",
    "                    JOB_SALARY = '-'.join(JOB_SALARY_ARRAY)\n",
    "                    JOB_SALARY_MIN = JOB_SALARY_ARRAY[0]\n",
    "                    JOB_SALARY_MAX = JOB_SALARY_ARRAY[1]                \n",
    "        elif (len(JOB_SALARY_ARRAY)==1):\n",
    "            JOB_SALARY = JOB_SALARY_ARRAY[0]\n",
    "            JOB_SALARY_MIN = JOB_SALARY_ARRAY[0]\n",
    "            JOB_SALARY_MAX = JOB_SALARY_ARRAY[0]\n",
    "    return JOB_SALARY, JOB_SALARY_MIN, JOB_SALARY_MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-edward",
   "metadata": {},
   "source": [
    "### Get the job type of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Type (JOBLUM)\n",
    "def getJobType(JOB_SOUP):\n",
    "    JOB_INFO = JOB_SOUP.find('div',{'class':'col-sm-8 job-main-col'})\n",
    "    if (len(re.findall('(?i)Full-time',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Fulltime',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Full time',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Part-time',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    elif (len(re.findall('(?i)Parttime',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    elif (len(re.findall('(?i)Part time',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    else:\n",
    "        return \"Not Specified\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-thing",
   "metadata": {},
   "source": [
    "### Web Scraping function based on selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of each job posting (JOBLUM)\n",
    "def scrapeJob(URL, JOB_STATUS_LIST, JOB_SALARY_LIST, JOB_SALARY_MIN_LIST, \n",
    "              JOB_SALARY_MAX_LIST, JOB_DESCRIPTION_LIST):\n",
    "    \n",
    "    for m in range(len(URL)):\n",
    "        JOB_SOUP = getSoup('https://ph.joblum.com' + URL[m])\n",
    "        JOB_DESCRIPTION = getJobDescription(JOB_SOUP)\n",
    "        JOB_SALARY, JOB_SALARY_MIN, JOB_SALARY_MAX = getJobSalary(JOB_SOUP)\n",
    "        JOB_STATUS = getJobType(JOB_SOUP)\n",
    "        JOB_STATUS_LIST.append(JOB_STATUS)\n",
    "        JOB_SALARY_LIST.append(JOB_SALARY)\n",
    "        JOB_SALARY_MIN_LIST.append(JOB_SALARY_MIN)\n",
    "        JOB_SALARY_MAX_LIST.append(JOB_SALARY_MAX)\n",
    "        JOB_DESCRIPTION_LIST.append(JOB_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-international",
   "metadata": {},
   "source": [
    "### For Categorizing\n",
    "\n",
    "<a href=\"https://www.onetonline.org/find/stem?t=0\">\n",
    "    onetonline.org\n",
    "</a> and <a href=\"https://www.istemnetwork.org/parents-students/stem-career-opportunities/\">\n",
    "    careerwise.minnstate.edu\n",
    "</a>\n",
    "\n",
    "- Basis for careers from AGRI were classified as Agriculture\n",
    "\n",
    "<a href=\"https://www.bestcolleges.com/careers/stem/\">\n",
    "    bestcolleges.com\n",
    "</a> \n",
    "\n",
    "- Basis for careers from T_HARDWARE, IT_SYS, IT_SOFTWARE  were classified as IT\n",
    "- Basis for careers from ARCHI, AVI, CHEMENG, CIVILENG, CONSTRUCTION, ELEC, ELECENG, ELECTRO, ELECTROENG, ENVIENG, INDUSENG, MAINTENANCE, MECH, MECHENG, NURSE, OIL, OILENG, ENG, QUALITY, QUANTITY were classified as Engineering\n",
    "- Basis for careers from STAT were classified as Mathematics\n",
    "- Basis for careers from BIOMED, BIOTECH, DIAGNOSIS, DOCTOR, PHARMA, and PRAC were classified as Medicine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-temple",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic First Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Actuarial/Statistics (JOBLUM) - FIRST HALF\n",
    "\n",
    "STAT_TITLE_FIRST = []\n",
    "STAT_COMPANY_FIRST = []\n",
    "STAT_DATE_FIRST = []\n",
    "STAT_LOCATION_FIRST = []\n",
    "STAT_STATUS_FIRST = []\n",
    "STAT_SALARY_FIRST = []\n",
    "STAT_SALARY_MIN_FIRST = []\n",
    "STAT_SALARY_MAX_FIRST = []\n",
    "STAT_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-actuarial-statistics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        STAT_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        STAT_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        STAT_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        STAT_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, STAT_STATUS_FIRST, STAT_SALARY_FIRST, STAT_SALARY_MIN_FIRST, \n",
    "              STAT_SALARY_MAX_FIRST, STAT_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-dynamics",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic Second Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Actuarial/Statistics (JOBLUM) - SECOND HALF\n",
    "\n",
    "STAT_TITLE_SECOND = []\n",
    "STAT_COMPANY_SECOND = []\n",
    "STAT_DATE_SECOND = []\n",
    "STAT_LOCATION_SECOND = []\n",
    "STAT_STATUS_SECOND = []\n",
    "STAT_SALARY_SECOND = []\n",
    "STAT_SALARY_MIN_SECOND = []\n",
    "STAT_SALARY_MAX_SECOND = []\n",
    "STAT_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        STAT_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        STAT_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        STAT_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        STAT_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, STAT_STATUS_SECOND, STAT_SALARY_SECOND, STAT_SALARY_MIN_SECOND, \n",
    "              STAT_SALARY_MAX_SECOND, STAT_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-quick",
   "metadata": {},
   "source": [
    "### CATEGORY - Joined Actuarial/Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Actuarial/Statistics (JOBLUM) \n",
    "\n",
    "STAT_TITLE_LIST = np.concatenate((STAT_TITLE_FIRST, STAT_TITLE_SECOND))\n",
    "STAT_COMPANY_LIST = np.concatenate((STAT_COMPANY_FIRST, STAT_COMPANY_SECOND))\n",
    "STAT_DATE_LIST = np.concatenate((STAT_DATE_FIRST, STAT_DATE_SECOND))\n",
    "STAT_LOCATION_LIST = np.concatenate((STAT_LOCATION_FIRST, STAT_LOCATION_SECOND))\n",
    "STAT_STATUS_LIST = np.concatenate((STAT_STATUS_FIRST, STAT_STATUS_SECOND))\n",
    "STAT_SALARY_LIST = np.concatenate((STAT_SALARY_FIRST, STAT_SALARY_SECOND))\n",
    "STAT_SALARY_MIN_LIST = np.concatenate((STAT_SALARY_MIN_FIRST, STAT_SALARY_MIN_SECOND))\n",
    "STAT_SALARY_MAX_LIST = np.concatenate((STAT_SALARY_MAX_FIRST, STAT_SALARY_MAX_SECOND))\n",
    "STAT_DESCRIPTION_LIST = np.concatenate((STAT_DESCRIPTION_FIRST, STAT_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-panama",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-subsection",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Actuarial/Statistics (JOBLUM) \n",
    "STAT={'Website': \"Joblum\",\n",
    "      'Job Title': STAT_TITLE_LIST, \n",
    "      'Category': \"Actuarial/Statistics\", \n",
    "      'Company': STAT_COMPANY_LIST, \n",
    "      'Date Posted': STAT_DATE_LIST, \n",
    "      'Location': STAT_LOCATION_LIST, \n",
    "      'Status': STAT_STATUS_LIST, \n",
    "      'Salary': STAT_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': STAT_DESCRIPTION_LIST,\n",
    "      'Min Salary': STAT_SALARY_MIN_LIST,\n",
    "      'Max Salary': STAT_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Mathematics\"}\n",
    "STAT_df = pd.DataFrame(data=STAT)\n",
    "STAT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-infrared",
   "metadata": {},
   "source": [
    "### Parse data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_df.to_csv ('Joblum Data\\JOBLUM-STAT.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-treasury",
   "metadata": {},
   "source": [
    "### CATEGORY - Agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Agriculture (JOBLUM) - FIRST HALF\n",
    "\n",
    "AGRI_TITLE_FIRST = []\n",
    "AGRI_COMPANY_FIRST = []\n",
    "AGRI_DATE_FIRST = []\n",
    "AGRI_LOCATION_FIRST = []\n",
    "AGRI_STATUS_FIRST = []\n",
    "AGRI_SALARY_FIRST = []\n",
    "AGRI_SALARY_MIN_FIRST = []\n",
    "AGRI_SALARY_MAX_FIRST = []\n",
    "AGRI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-agriculture?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AGRI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AGRI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AGRI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AGRI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AGRI_STATUS_FIRST, AGRI_SALARY_FIRST, AGRI_SALARY_MIN_FIRST, \n",
    "              AGRI_SALARY_MAX_FIRST, AGRI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Agriculture (JOBLUM) - SECOND HALF\n",
    "\n",
    "AGRI_TITLE_SECOND = []\n",
    "AGRI_COMPANY_SECOND = []\n",
    "AGRI_DATE_SECOND = []\n",
    "AGRI_LOCATION_SECOND = []\n",
    "AGRI_STATUS_SECOND = []\n",
    "AGRI_SALARY_SECOND = []\n",
    "AGRI_SALARY_MIN_SECOND = []\n",
    "AGRI_SALARY_MAX_SECOND = []\n",
    "AGRI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AGRI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AGRI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AGRI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AGRI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AGRI_STATUS_SECOND, AGRI_SALARY_SECOND, AGRI_SALARY_MIN_SECOND, \n",
    "              AGRI_SALARY_MAX_SECOND, AGRI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Agriculture (JOBLUM) \n",
    "\n",
    "AGRI_TITLE_LIST = np.concatenate((AGRI_TITLE_FIRST, AGRI_TITLE_SECOND))\n",
    "AGRI_COMPANY_LIST = np.concatenate((AGRI_COMPANY_FIRST, AGRI_COMPANY_SECOND))\n",
    "AGRI_DATE_LIST = np.concatenate((AGRI_DATE_FIRST, AGRI_DATE_SECOND))\n",
    "AGRI_LOCATION_LIST = np.concatenate((AGRI_LOCATION_FIRST, AGRI_LOCATION_SECOND))\n",
    "AGRI_STATUS_LIST = np.concatenate((AGRI_STATUS_FIRST, AGRI_STATUS_SECOND))\n",
    "AGRI_SALARY_LIST = np.concatenate((AGRI_SALARY_FIRST, AGRI_SALARY_SECOND))\n",
    "AGRI_SALARY_MIN_LIST = np.concatenate((AGRI_SALARY_MIN_FIRST, AGRI_SALARY_MIN_SECOND))\n",
    "AGRI_SALARY_MAX_LIST = np.concatenate((AGRI_SALARY_MAX_FIRST, AGRI_SALARY_MAX_SECOND))\n",
    "AGRI_DESCRIPTION_LIST = np.concatenate((AGRI_DESCRIPTION_FIRST, AGRI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-activation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Agriculture (JOBLUM) \n",
    "AGRI={'Website': \"Joblum\",\n",
    "      'Job Title': AGRI_TITLE_LIST, \n",
    "      'Category': \"Agriculture\", \n",
    "      'Company': AGRI_COMPANY_LIST, \n",
    "      'Date Posted': AGRI_DATE_LIST, \n",
    "      'Location': AGRI_LOCATION_LIST, \n",
    "      'Status': AGRI_STATUS_LIST, \n",
    "      'Salary': AGRI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': AGRI_DESCRIPTION_LIST,\n",
    "      'Min Salary': AGRI_SALARY_MIN_LIST,\n",
    "      'Max Salary': AGRI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Agriculture\"}\n",
    "AGRI_df = pd.DataFrame(data=AGRI)\n",
    "AGRI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGRI_df.to_csv ('Joblum Data\\JOBLUM-AGRI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-delight",
   "metadata": {},
   "source": [
    "### CATEGORY - Architect/Interior Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Architect/Interior Design (JOBLUM) - FIRST HALF\n",
    "\n",
    "ARCHI_TITLE_FIRST = []\n",
    "ARCHI_COMPANY_FIRST = []\n",
    "ARCHI_DATE_FIRST = []\n",
    "ARCHI_LOCATION_FIRST = []\n",
    "ARCHI_STATUS_FIRST = []\n",
    "ARCHI_SALARY_FIRST = []\n",
    "ARCHI_SALARY_MIN_FIRST = []\n",
    "ARCHI_SALARY_MAX_FIRST = []\n",
    "ARCHI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-architect-interior-design?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ARCHI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ARCHI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ARCHI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ARCHI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ARCHI_STATUS_FIRST, ARCHI_SALARY_FIRST, ARCHI_SALARY_MIN_FIRST, \n",
    "              ARCHI_SALARY_MAX_FIRST, ARCHI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Architect/Interior Design (JOBLUM) - SECOND HALF\n",
    "\n",
    "ARCHI_TITLE_SECOND = []\n",
    "ARCHI_COMPANY_SECOND = []\n",
    "ARCHI_DATE_SECOND = []\n",
    "ARCHI_LOCATION_SECOND = []\n",
    "ARCHI_STATUS_SECOND = []\n",
    "ARCHI_SALARY_SECOND = []\n",
    "ARCHI_SALARY_MIN_SECOND = []\n",
    "ARCHI_SALARY_MAX_SECOND = []\n",
    "ARCHI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ARCHI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ARCHI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ARCHI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ARCHI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ARCHI_STATUS_SECOND, ARCHI_SALARY_SECOND, ARCHI_SALARY_MIN_SECOND, \n",
    "              ARCHI_SALARY_MAX_SECOND, ARCHI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Architect/Interior Design (JOBLUM) \n",
    "\n",
    "ARCHI_TITLE_LIST = np.concatenate((ARCHI_TITLE_FIRST, ARCHI_TITLE_SECOND))\n",
    "ARCHI_COMPANY_LIST = np.concatenate((ARCHI_COMPANY_FIRST, ARCHI_COMPANY_SECOND))\n",
    "ARCHI_DATE_LIST = np.concatenate((ARCHI_DATE_FIRST, ARCHI_DATE_SECOND))\n",
    "ARCHI_LOCATION_LIST = np.concatenate((ARCHI_LOCATION_FIRST, ARCHI_LOCATION_SECOND))\n",
    "ARCHI_STATUS_LIST = np.concatenate((ARCHI_STATUS_FIRST, ARCHI_STATUS_SECOND))\n",
    "ARCHI_SALARY_LIST = np.concatenate((ARCHI_SALARY_FIRST, ARCHI_SALARY_SECOND))\n",
    "ARCHI_SALARY_MIN_LIST = np.concatenate((ARCHI_SALARY_MIN_FIRST, ARCHI_SALARY_MIN_SECOND))\n",
    "ARCHI_SALARY_MAX_LIST = np.concatenate((ARCHI_SALARY_MAX_FIRST, ARCHI_SALARY_MAX_SECOND))\n",
    "ARCHI_DESCRIPTION_LIST = np.concatenate((ARCHI_DESCRIPTION_FIRST, ARCHI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-arbitration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Architect/Interior Design (JOBLUM) \n",
    "ARCHI={'Website': \"Joblum\",\n",
    "      'Job Title': ARCHI_TITLE_LIST, \n",
    "      'Category': \"Architect/Interior Design\", \n",
    "      'Company': ARCHI_COMPANY_LIST, \n",
    "      'Date Posted': ARCHI_DATE_LIST, \n",
    "      'Location': ARCHI_LOCATION_LIST, \n",
    "      'Status': ARCHI_STATUS_LIST, \n",
    "      'Salary': ARCHI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ARCHI_DESCRIPTION_LIST,\n",
    "      'Min Salary': ARCHI_SALARY_MIN_LIST,\n",
    "      'Max Salary': ARCHI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ARCHI_df = pd.DataFrame(data=ARCHI)\n",
    "ARCHI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHI_df.to_csv ('Joblum Data\\JOBLUM-ARCHI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-imperial",
   "metadata": {},
   "source": [
    "### CATEGORY - Aviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Aviation (JOBLUM) - FIRST HALF\n",
    "\n",
    "AVI_TITLE_FIRST = []\n",
    "AVI_COMPANY_FIRST = []\n",
    "AVI_DATE_FIRST = []\n",
    "AVI_LOCATION_FIRST = []\n",
    "AVI_STATUS_FIRST = []\n",
    "AVI_SALARY_FIRST = []\n",
    "AVI_SALARY_MIN_FIRST = []\n",
    "AVI_SALARY_MAX_FIRST = []\n",
    "AVI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-aviation?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AVI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AVI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AVI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AVI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AVI_STATUS_FIRST, AVI_SALARY_FIRST, AVI_SALARY_MIN_FIRST, \n",
    "              AVI_SALARY_MAX_FIRST, AVI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Aviation (JOBLUM) - SECOND HALF\n",
    "\n",
    "AVI_TITLE_SECOND = []\n",
    "AVI_COMPANY_SECOND = []\n",
    "AVI_DATE_SECOND = []\n",
    "AVI_LOCATION_SECOND = []\n",
    "AVI_STATUS_SECOND = []\n",
    "AVI_SALARY_SECOND = []\n",
    "AVI_SALARY_MIN_SECOND = []\n",
    "AVI_SALARY_MAX_SECOND = []\n",
    "AVI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AVI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AVI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AVI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AVI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AVI_STATUS_SECOND, AVI_SALARY_SECOND, AVI_SALARY_MIN_SECOND, \n",
    "              AVI_SALARY_MAX_SECOND, AVI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Aviation (JOBLUM) \n",
    "\n",
    "AVI_TITLE_LIST = np.concatenate((AVI_TITLE_FIRST, AVI_TITLE_SECOND))\n",
    "AVI_COMPANY_LIST = np.concatenate((AVI_COMPANY_FIRST, AVI_COMPANY_SECOND))\n",
    "AVI_DATE_LIST = np.concatenate((AVI_DATE_FIRST, AVI_DATE_SECOND))\n",
    "AVI_LOCATION_LIST = np.concatenate((AVI_LOCATION_FIRST, AVI_LOCATION_SECOND))\n",
    "AVI_STATUS_LIST = np.concatenate((AVI_STATUS_FIRST, AVI_STATUS_SECOND))\n",
    "AVI_SALARY_LIST = np.concatenate((AVI_SALARY_FIRST, AVI_SALARY_SECOND))\n",
    "AVI_SALARY_MIN_LIST = np.concatenate((AVI_SALARY_MIN_FIRST, AVI_SALARY_MIN_SECOND))\n",
    "AVI_SALARY_MAX_LIST = np.concatenate((AVI_SALARY_MAX_FIRST, AVI_SALARY_MAX_SECOND))\n",
    "AVI_DESCRIPTION_LIST = np.concatenate((AVI_DESCRIPTION_FIRST, AVI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-project",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Aviation (JOBLUM) \n",
    "AVI={'Website': \"Joblum\",\n",
    "      'Job Title': AVI_TITLE_LIST, \n",
    "      'Category': \"Aviation\", \n",
    "      'Company': AVI_COMPANY_LIST, \n",
    "      'Date Posted': AVI_DATE_LIST, \n",
    "      'Location': AVI_LOCATION_LIST, \n",
    "      'Status': AVI_STATUS_LIST, \n",
    "      'Salary': AVI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': AVI_DESCRIPTION_LIST,\n",
    "      'Min Salary': AVI_SALARY_MIN_LIST,\n",
    "      'Max Salary': AVI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "AVI_df = pd.DataFrame(data=AVI)\n",
    "AVI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVI_df.to_csv ('Joblum Data\\JOBLUM-AVI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-array",
   "metadata": {},
   "source": [
    "### CATEGORY - Biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biomedical (JOBLUM) - FIRST HALF\n",
    "\n",
    "BIOMED_TITLE_FIRST = []\n",
    "BIOMED_COMPANY_FIRST = []\n",
    "BIOMED_DATE_FIRST = []\n",
    "BIOMED_LOCATION_FIRST = []\n",
    "BIOMED_STATUS_FIRST = []\n",
    "BIOMED_SALARY_FIRST = []\n",
    "BIOMED_SALARY_MIN_FIRST = []\n",
    "BIOMED_SALARY_MAX_FIRST = []\n",
    "BIOMED_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-biomedical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOMED_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOMED_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOMED_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOMED_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOMED_STATUS_FIRST, BIOMED_SALARY_FIRST, BIOMED_SALARY_MIN_FIRST, \n",
    "              BIOMED_SALARY_MAX_FIRST, BIOMED_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biomedical (JOBLUM) - SECOND HALF\n",
    "\n",
    "BIOMED_TITLE_SECOND = []\n",
    "BIOMED_COMPANY_SECOND = []\n",
    "BIOMED_DATE_SECOND = []\n",
    "BIOMED_LOCATION_SECOND = []\n",
    "BIOMED_STATUS_SECOND = []\n",
    "BIOMED_SALARY_SECOND = []\n",
    "BIOMED_SALARY_MIN_SECOND = []\n",
    "BIOMED_SALARY_MAX_SECOND = []\n",
    "BIOMED_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOMED_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOMED_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOMED_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOMED_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOMED_STATUS_SECOND, BIOMED_SALARY_SECOND, BIOMED_SALARY_MIN_SECOND, \n",
    "              BIOMED_SALARY_MAX_SECOND, BIOMED_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Biomedical (JOBLUM) \n",
    "\n",
    "BIOMED_TITLE_LIST = np.concatenate((BIOMED_TITLE_FIRST, BIOMED_TITLE_SECOND))\n",
    "BIOMED_COMPANY_LIST = np.concatenate((BIOMED_COMPANY_FIRST, BIOMED_COMPANY_SECOND))\n",
    "BIOMED_DATE_LIST = np.concatenate((BIOMED_DATE_FIRST, BIOMED_DATE_SECOND))\n",
    "BIOMED_LOCATION_LIST = np.concatenate((BIOMED_LOCATION_FIRST, BIOMED_LOCATION_SECOND))\n",
    "BIOMED_STATUS_LIST = np.concatenate((BIOMED_STATUS_FIRST, BIOMED_STATUS_SECOND))\n",
    "BIOMED_SALARY_LIST = np.concatenate((BIOMED_SALARY_FIRST, BIOMED_SALARY_SECOND))\n",
    "BIOMED_SALARY_MIN_LIST = np.concatenate((BIOMED_SALARY_MIN_FIRST, BIOMED_SALARY_MIN_SECOND))\n",
    "BIOMED_SALARY_MAX_LIST = np.concatenate((BIOMED_SALARY_MAX_FIRST, BIOMED_SALARY_MAX_SECOND))\n",
    "BIOMED_DESCRIPTION_LIST = np.concatenate((BIOMED_DESCRIPTION_FIRST, BIOMED_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-segment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Biomedical (JOBLUM) \n",
    "BIOMED={'Website': \"Joblum\",\n",
    "      'Job Title': BIOMED_TITLE_LIST, \n",
    "      'Category': \"Biomedical\", \n",
    "      'Company': BIOMED_COMPANY_LIST, \n",
    "      'Date Posted': BIOMED_DATE_LIST, \n",
    "      'Location': BIOMED_LOCATION_LIST, \n",
    "      'Status': BIOMED_STATUS_LIST, \n",
    "      'Salary': BIOMED_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': BIOMED_DESCRIPTION_LIST,\n",
    "      'Min Salary': BIOMED_SALARY_MIN_LIST,\n",
    "      'Max Salary': BIOMED_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "BIOMED_df = pd.DataFrame(data=BIOMED)\n",
    "BIOMED_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOMED_df.to_csv ('Joblum Data\\JOBLUM-BIOMED.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-attribute",
   "metadata": {},
   "source": [
    "### CATEGORY - Biotechnology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biotechnology (JOBLUM) - FIRST HALF\n",
    "\n",
    "BIOTECH_TITLE_FIRST = []\n",
    "BIOTECH_COMPANY_FIRST = []\n",
    "BIOTECH_DATE_FIRST = []\n",
    "BIOTECH_LOCATION_FIRST = []\n",
    "BIOTECH_STATUS_FIRST = []\n",
    "BIOTECH_SALARY_FIRST = []\n",
    "BIOTECH_SALARY_MIN_FIRST = []\n",
    "BIOTECH_SALARY_MAX_FIRST = []\n",
    "BIOTECH_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-biotechnology?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOTECH_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOTECH_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOTECH_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOTECH_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOTECH_STATUS_FIRST, BIOTECH_SALARY_FIRST, BIOTECH_SALARY_MIN_FIRST, \n",
    "              BIOTECH_SALARY_MAX_FIRST, BIOTECH_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biotechnology (JOBLUM) - SECOND HALF\n",
    "\n",
    "BIOTECH_TITLE_SECOND = []\n",
    "BIOTECH_COMPANY_SECOND = []\n",
    "BIOTECH_DATE_SECOND = []\n",
    "BIOTECH_LOCATION_SECOND = []\n",
    "BIOTECH_STATUS_SECOND = []\n",
    "BIOTECH_SALARY_SECOND = []\n",
    "BIOTECH_SALARY_MIN_SECOND = []\n",
    "BIOTECH_SALARY_MAX_SECOND = []\n",
    "BIOTECH_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOTECH_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOTECH_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOTECH_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOTECH_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOTECH_STATUS_SECOND, BIOTECH_SALARY_SECOND, BIOTECH_SALARY_MIN_SECOND, \n",
    "              BIOTECH_SALARY_MAX_SECOND, BIOTECH_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Biotechnology (JOBLUM) \n",
    "\n",
    "BIOTECH_TITLE_LIST = np.concatenate((BIOTECH_TITLE_FIRST, BIOTECH_TITLE_SECOND))\n",
    "BIOTECH_COMPANY_LIST = np.concatenate((BIOTECH_COMPANY_FIRST, BIOTECH_COMPANY_SECOND))\n",
    "BIOTECH_DATE_LIST = np.concatenate((BIOTECH_DATE_FIRST, BIOTECH_DATE_SECOND))\n",
    "BIOTECH_LOCATION_LIST = np.concatenate((BIOTECH_LOCATION_FIRST, BIOTECH_LOCATION_SECOND))\n",
    "BIOTECH_STATUS_LIST = np.concatenate((BIOTECH_STATUS_FIRST, BIOTECH_STATUS_SECOND))\n",
    "BIOTECH_SALARY_LIST = np.concatenate((BIOTECH_SALARY_FIRST, BIOTECH_SALARY_SECOND))\n",
    "BIOTECH_SALARY_MIN_LIST = np.concatenate((BIOTECH_SALARY_MIN_FIRST, BIOTECH_SALARY_MIN_SECOND))\n",
    "BIOTECH_SALARY_MAX_LIST = np.concatenate((BIOTECH_SALARY_MAX_FIRST, BIOTECH_SALARY_MAX_SECOND))\n",
    "BIOTECH_DESCRIPTION_LIST = np.concatenate((BIOTECH_DESCRIPTION_FIRST, BIOTECH_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-learning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Biotechnology (JOBLUM) \n",
    "BIOTECH={'Website': \"Joblum\",\n",
    "      'Job Title': BIOTECH_TITLE_LIST, \n",
    "      'Category': \"Biotechnology\", \n",
    "      'Company': BIOTECH_COMPANY_LIST, \n",
    "      'Date Posted': BIOTECH_DATE_LIST, \n",
    "      'Location': BIOTECH_LOCATION_LIST, \n",
    "      'Status': BIOTECH_STATUS_LIST, \n",
    "      'Salary': BIOTECH_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': BIOTECH_DESCRIPTION_LIST,\n",
    "      'Min Salary': BIOTECH_SALARY_MIN_LIST,\n",
    "      'Max Salary': BIOTECH_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "BIOTECH_df = pd.DataFrame(data=BIOTECH)\n",
    "BIOTECH_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOTECH_df.to_csv ('Joblum Data\\JOBLUM-BIOTECH.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-trail",
   "metadata": {},
   "source": [
    "### CATEGORY - Chemical Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemical Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "CHEMENG_TITLE_FIRST = []\n",
    "CHEMENG_COMPANY_FIRST = []\n",
    "CHEMENG_DATE_FIRST = []\n",
    "CHEMENG_LOCATION_FIRST = []\n",
    "CHEMENG_STATUS_FIRST = []\n",
    "CHEMENG_SALARY_FIRST = []\n",
    "CHEMENG_SALARY_MIN_FIRST = []\n",
    "CHEMENG_SALARY_MAX_FIRST = []\n",
    "CHEMENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-chemical-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEMENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEMENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEMENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEMENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEMENG_STATUS_FIRST, CHEMENG_SALARY_FIRST, CHEMENG_SALARY_MIN_FIRST, \n",
    "              CHEMENG_SALARY_MAX_FIRST, CHEMENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemical Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "CHEMENG_TITLE_SECOND = []\n",
    "CHEMENG_COMPANY_SECOND = []\n",
    "CHEMENG_DATE_SECOND = []\n",
    "CHEMENG_LOCATION_SECOND = []\n",
    "CHEMENG_STATUS_SECOND = []\n",
    "CHEMENG_SALARY_SECOND = []\n",
    "CHEMENG_SALARY_MIN_SECOND = []\n",
    "CHEMENG_SALARY_MAX_SECOND = []\n",
    "CHEMENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEMENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEMENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEMENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEMENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEMENG_STATUS_SECOND, CHEMENG_SALARY_SECOND, CHEMENG_SALARY_MIN_SECOND, \n",
    "              CHEMENG_SALARY_MAX_SECOND, CHEMENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Chemical Engineering (JOBLUM) \n",
    "\n",
    "CHEMENG_TITLE_LIST = np.concatenate((CHEMENG_TITLE_FIRST, CHEMENG_TITLE_SECOND))\n",
    "CHEMENG_COMPANY_LIST = np.concatenate((CHEMENG_COMPANY_FIRST, CHEMENG_COMPANY_SECOND))\n",
    "CHEMENG_DATE_LIST = np.concatenate((CHEMENG_DATE_FIRST, CHEMENG_DATE_SECOND))\n",
    "CHEMENG_LOCATION_LIST = np.concatenate((CHEMENG_LOCATION_FIRST, CHEMENG_LOCATION_SECOND))\n",
    "CHEMENG_STATUS_LIST = np.concatenate((CHEMENG_STATUS_FIRST, CHEMENG_STATUS_SECOND))\n",
    "CHEMENG_SALARY_LIST = np.concatenate((CHEMENG_SALARY_FIRST, CHEMENG_SALARY_SECOND))\n",
    "CHEMENG_SALARY_MIN_LIST = np.concatenate((CHEMENG_SALARY_MIN_FIRST, CHEMENG_SALARY_MIN_SECOND))\n",
    "CHEMENG_SALARY_MAX_LIST = np.concatenate((CHEMENG_SALARY_MAX_FIRST, CHEMENG_SALARY_MAX_SECOND))\n",
    "CHEMENG_DESCRIPTION_LIST = np.concatenate((CHEMENG_DESCRIPTION_FIRST, CHEMENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-remark",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Chemical Engineering (JOBLUM) \n",
    "CHEMENG={'Website': \"Joblum\",\n",
    "      'Job Title': CHEMENG_TITLE_LIST, \n",
    "      'Category': \"Chemical Engineering\", \n",
    "      'Company': CHEMENG_COMPANY_LIST, \n",
    "      'Date Posted': CHEMENG_DATE_LIST, \n",
    "      'Location': CHEMENG_LOCATION_LIST, \n",
    "      'Status': CHEMENG_STATUS_LIST, \n",
    "      'Salary': CHEMENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CHEMENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': CHEMENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': CHEMENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CHEMENG_df = pd.DataFrame(data=CHEMENG)\n",
    "CHEMENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEMENG_df.to_csv ('Joblum Data\\JOBLUM-CHEMENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-letters",
   "metadata": {},
   "source": [
    "### CATEGORY - Chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemistry (JOBLUM) - FIRST HALF\n",
    "\n",
    "CHEM_TITLE_FIRST = []\n",
    "CHEM_COMPANY_FIRST = []\n",
    "CHEM_DATE_FIRST = []\n",
    "CHEM_LOCATION_FIRST = []\n",
    "CHEM_STATUS_FIRST = []\n",
    "CHEM_SALARY_FIRST = []\n",
    "CHEM_SALARY_MIN_FIRST = []\n",
    "CHEM_SALARY_MAX_FIRST = []\n",
    "CHEM_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-chemistry?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEM_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEM_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEM_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEM_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEM_STATUS_FIRST, CHEM_SALARY_FIRST, CHEM_SALARY_MIN_FIRST, \n",
    "              CHEM_SALARY_MAX_FIRST, CHEM_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemistry (JOBLUM) - SECOND HALF\n",
    "\n",
    "CHEM_TITLE_SECOND = []\n",
    "CHEM_COMPANY_SECOND = []\n",
    "CHEM_DATE_SECOND = []\n",
    "CHEM_LOCATION_SECOND = []\n",
    "CHEM_STATUS_SECOND = []\n",
    "CHEM_SALARY_SECOND = []\n",
    "CHEM_SALARY_MIN_SECOND = []\n",
    "CHEM_SALARY_MAX_SECOND = []\n",
    "CHEM_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEM_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEM_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEM_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEM_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEM_STATUS_SECOND, CHEM_SALARY_SECOND, CHEM_SALARY_MIN_SECOND, \n",
    "              CHEM_SALARY_MAX_SECOND, CHEM_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Chemistry (JOBLUM) \n",
    "\n",
    "CHEM_TITLE_LIST = np.concatenate((CHEM_TITLE_FIRST, CHEM_TITLE_SECOND))\n",
    "CHEM_COMPANY_LIST = np.concatenate((CHEM_COMPANY_FIRST, CHEM_COMPANY_SECOND))\n",
    "CHEM_DATE_LIST = np.concatenate((CHEM_DATE_FIRST, CHEM_DATE_SECOND))\n",
    "CHEM_LOCATION_LIST = np.concatenate((CHEM_LOCATION_FIRST, CHEM_LOCATION_SECOND))\n",
    "CHEM_STATUS_LIST = np.concatenate((CHEM_STATUS_FIRST, CHEM_STATUS_SECOND))\n",
    "CHEM_SALARY_LIST = np.concatenate((CHEM_SALARY_FIRST, CHEM_SALARY_SECOND))\n",
    "CHEM_SALARY_MIN_LIST = np.concatenate((CHEM_SALARY_MIN_FIRST, CHEM_SALARY_MIN_SECOND))\n",
    "CHEM_SALARY_MAX_LIST = np.concatenate((CHEM_SALARY_MAX_FIRST, CHEM_SALARY_MAX_SECOND))\n",
    "CHEM_DESCRIPTION_LIST = np.concatenate((CHEM_DESCRIPTION_FIRST, CHEM_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-dividend",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Chemistry (JOBLUM) \n",
    "CHEM={'Website': \"Joblum\",\n",
    "      'Job Title': CHEM_TITLE_LIST, \n",
    "      'Category': \"Chemistry\", \n",
    "      'Company': CHEM_COMPANY_LIST, \n",
    "      'Date Posted': CHEM_DATE_LIST, \n",
    "      'Location': CHEM_LOCATION_LIST, \n",
    "      'Status': CHEM_STATUS_LIST, \n",
    "      'Salary': CHEM_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CHEM_DESCRIPTION_LIST,\n",
    "      'Min Salary': CHEM_SALARY_MIN_LIST,\n",
    "      'Max Salary': CHEM_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "CHEM_df = pd.DataFrame(data=CHEM)\n",
    "CHEM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEM_df.to_csv ('Joblum Data\\JOBLUM-CHEM.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-identity",
   "metadata": {},
   "source": [
    "### CATEGORY - Civil Engineering/Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil Engineering/Construction (JOBLUM) - FIRST HALF\n",
    "\n",
    "CIVILENG_TITLE_FIRST = []\n",
    "CIVILENG_COMPANY_FIRST = []\n",
    "CIVILENG_DATE_FIRST = []\n",
    "CIVILENG_LOCATION_FIRST = []\n",
    "CIVILENG_STATUS_FIRST = []\n",
    "CIVILENG_SALARY_FIRST = []\n",
    "CIVILENG_SALARY_MIN_FIRST = []\n",
    "CIVILENG_SALARY_MAX_FIRST = []\n",
    "CIVILENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-civil-engineering-construction?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CIVILENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CIVILENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CIVILENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CIVILENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CIVILENG_STATUS_FIRST, CIVILENG_SALARY_FIRST, CIVILENG_SALARY_MIN_FIRST, \n",
    "              CIVILENG_SALARY_MAX_FIRST, CIVILENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil Engineering/Construction (JOBLUM) - SECOND HALF\n",
    "\n",
    "CIVILENG_TITLE_SECOND = []\n",
    "CIVILENG_COMPANY_SECOND = []\n",
    "CIVILENG_DATE_SECOND = []\n",
    "CIVILENG_LOCATION_SECOND = []\n",
    "CIVILENG_STATUS_SECOND = []\n",
    "CIVILENG_SALARY_SECOND = []\n",
    "CIVILENG_SALARY_MIN_SECOND = []\n",
    "CIVILENG_SALARY_MAX_SECOND = []\n",
    "CIVILENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CIVILENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CIVILENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CIVILENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CIVILENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CIVILENG_STATUS_SECOND, CIVILENG_SALARY_SECOND, CIVILENG_SALARY_MIN_SECOND, \n",
    "              CIVILENG_SALARY_MAX_SECOND, CIVILENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Civil Engineering/Construction (JOBLUM) \n",
    "\n",
    "CIVILENG_TITLE_LIST = np.concatenate((CIVILENG_TITLE_FIRST, CIVILENG_TITLE_SECOND))\n",
    "CIVILENG_COMPANY_LIST = np.concatenate((CIVILENG_COMPANY_FIRST, CIVILENG_COMPANY_SECOND))\n",
    "CIVILENG_DATE_LIST = np.concatenate((CIVILENG_DATE_FIRST, CIVILENG_DATE_SECOND))\n",
    "CIVILENG_LOCATION_LIST = np.concatenate((CIVILENG_LOCATION_FIRST, CIVILENG_LOCATION_SECOND))\n",
    "CIVILENG_STATUS_LIST = np.concatenate((CIVILENG_STATUS_FIRST, CIVILENG_STATUS_SECOND))\n",
    "CIVILENG_SALARY_LIST = np.concatenate((CIVILENG_SALARY_FIRST, CIVILENG_SALARY_SECOND))\n",
    "CIVILENG_SALARY_MIN_LIST = np.concatenate((CIVILENG_SALARY_MIN_FIRST, CIVILENG_SALARY_MIN_SECOND))\n",
    "CIVILENG_SALARY_MAX_LIST = np.concatenate((CIVILENG_SALARY_MAX_FIRST, CIVILENG_SALARY_MAX_SECOND))\n",
    "CIVILENG_DESCRIPTION_LIST = np.concatenate((CIVILENG_DESCRIPTION_FIRST, CIVILENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-garbage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Civil Engineering/Construction (JOBLUM) \n",
    "CIVILENG={'Website': \"Joblum\",\n",
    "      'Job Title': CIVILENG_TITLE_LIST, \n",
    "      'Category': \"Civil Engineering/Construction\", \n",
    "      'Company': CIVILENG_COMPANY_LIST, \n",
    "      'Date Posted': CIVILENG_DATE_LIST, \n",
    "      'Location': CIVILENG_LOCATION_LIST, \n",
    "      'Status': CIVILENG_STATUS_LIST, \n",
    "      'Salary': CIVILENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CIVILENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': CIVILENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': CIVILENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CIVILENG_df = pd.DataFrame(data=CIVILENG)\n",
    "CIVILENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIVILENG_df.to_csv ('Joblum Data\\JOBLUM-CIVILENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-passenger",
   "metadata": {},
   "source": [
    "### CATEGORY - Civil/Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-dairy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Scraping data of Civil/Construction (JOBLUM) - FIRST HALF\n",
    "\n",
    "CONSTRUCTION_TITLE_FIRST = []\n",
    "CONSTRUCTION_COMPANY_FIRST = []\n",
    "CONSTRUCTION_DATE_FIRST = []\n",
    "CONSTRUCTION_LOCATION_FIRST = []\n",
    "CONSTRUCTION_STATUS_FIRST = []\n",
    "CONSTRUCTION_SALARY_FIRST = []\n",
    "CONSTRUCTION_SALARY_MIN_FIRST = []\n",
    "CONSTRUCTION_SALARY_MAX_FIRST = []\n",
    "CONSTRUCTION_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-civil-construction?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CONSTRUCTION_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CONSTRUCTION_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CONSTRUCTION_STATUS_FIRST, CONSTRUCTION_SALARY_FIRST, CONSTRUCTION_SALARY_MIN_FIRST, \n",
    "              CONSTRUCTION_SALARY_MAX_FIRST, CONSTRUCTION_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil/Construction (JOBLUM) - SECOND HALF\n",
    "\n",
    "CONSTRUCTION_TITLE_SECOND = []\n",
    "CONSTRUCTION_COMPANY_SECOND = []\n",
    "CONSTRUCTION_DATE_SECOND = []\n",
    "CONSTRUCTION_LOCATION_SECOND = []\n",
    "CONSTRUCTION_STATUS_SECOND = []\n",
    "CONSTRUCTION_SALARY_SECOND = []\n",
    "CONSTRUCTION_SALARY_MIN_SECOND = []\n",
    "CONSTRUCTION_SALARY_MAX_SECOND = []\n",
    "CONSTRUCTION_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CONSTRUCTION_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CONSTRUCTION_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CONSTRUCTION_STATUS_SECOND, CONSTRUCTION_SALARY_SECOND, CONSTRUCTION_SALARY_MIN_SECOND, \n",
    "              CONSTRUCTION_SALARY_MAX_SECOND, CONSTRUCTION_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Civil/Construction (JOBLUM) \n",
    "\n",
    "CONSTRUCTION_TITLE_LIST = np.concatenate((CONSTRUCTION_TITLE_FIRST, CONSTRUCTION_TITLE_SECOND))\n",
    "CONSTRUCTION_COMPANY_LIST = np.concatenate((CONSTRUCTION_COMPANY_FIRST, CONSTRUCTION_COMPANY_SECOND))\n",
    "CONSTRUCTION_DATE_LIST = np.concatenate((CONSTRUCTION_DATE_FIRST, CONSTRUCTION_DATE_SECOND))\n",
    "CONSTRUCTION_LOCATION_LIST = np.concatenate((CONSTRUCTION_LOCATION_FIRST, CONSTRUCTION_LOCATION_SECOND))\n",
    "CONSTRUCTION_STATUS_LIST = np.concatenate((CONSTRUCTION_STATUS_FIRST, CONSTRUCTION_STATUS_SECOND))\n",
    "CONSTRUCTION_SALARY_LIST = np.concatenate((CONSTRUCTION_SALARY_FIRST, CONSTRUCTION_SALARY_SECOND))\n",
    "CONSTRUCTION_SALARY_MIN_LIST = np.concatenate((CONSTRUCTION_SALARY_MIN_FIRST, CONSTRUCTION_SALARY_MIN_SECOND))\n",
    "CONSTRUCTION_SALARY_MAX_LIST = np.concatenate((CONSTRUCTION_SALARY_MAX_FIRST, CONSTRUCTION_SALARY_MAX_SECOND))\n",
    "CONSTRUCTION_DESCRIPTION_LIST = np.concatenate((CONSTRUCTION_DESCRIPTION_FIRST, CONSTRUCTION_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Civil/Construction (JOBLUM) \n",
    "CONSTRUCTION={'Website': \"Joblum\",\n",
    "      'Job Title': CONSTRUCTION_TITLE_LIST, \n",
    "      'Category': \"Civil/Construction\", \n",
    "      'Company': CONSTRUCTION_COMPANY_LIST, \n",
    "      'Date Posted': CONSTRUCTION_DATE_LIST, \n",
    "      'Location': CONSTRUCTION_LOCATION_LIST, \n",
    "      'Status': CONSTRUCTION_STATUS_LIST, \n",
    "      'Salary': CONSTRUCTION_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CONSTRUCTION_DESCRIPTION_LIST,\n",
    "      'Min Salary': CONSTRUCTION_SALARY_MIN_LIST,\n",
    "      'Max Salary': CONSTRUCTION_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CONSTRUCTION_df = pd.DataFrame(data=CONSTRUCTION)\n",
    "CONSTRUCTION_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRUCTION_df.to_csv ('Joblum Data\\JOBLUM-CONSTRUCTION.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-christopher",
   "metadata": {},
   "source": [
    "### CATEGORY - Diagnosis/Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Diagnosis/Others (JOBLUM) - FIRST HALF\n",
    "\n",
    "DIAGNOSIS_TITLE_FIRST = []\n",
    "DIAGNOSIS_COMPANY_FIRST = []\n",
    "DIAGNOSIS_DATE_FIRST = []\n",
    "DIAGNOSIS_LOCATION_FIRST = []\n",
    "DIAGNOSIS_STATUS_FIRST = []\n",
    "DIAGNOSIS_SALARY_FIRST = []\n",
    "DIAGNOSIS_SALARY_MIN_FIRST = []\n",
    "DIAGNOSIS_SALARY_MAX_FIRST = []\n",
    "DIAGNOSIS_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-diagnosis-others?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DIAGNOSIS_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DIAGNOSIS_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DIAGNOSIS_STATUS_FIRST, DIAGNOSIS_SALARY_FIRST, DIAGNOSIS_SALARY_MIN_FIRST, \n",
    "              DIAGNOSIS_SALARY_MAX_FIRST, DIAGNOSIS_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Diagnosis/Others (JOBLUM) - SECOND HALF\n",
    "\n",
    "DIAGNOSIS_TITLE_SECOND = []\n",
    "DIAGNOSIS_COMPANY_SECOND = []\n",
    "DIAGNOSIS_DATE_SECOND = []\n",
    "DIAGNOSIS_LOCATION_SECOND = []\n",
    "DIAGNOSIS_STATUS_SECOND = []\n",
    "DIAGNOSIS_SALARY_SECOND = []\n",
    "DIAGNOSIS_SALARY_MIN_SECOND = []\n",
    "DIAGNOSIS_SALARY_MAX_SECOND = []\n",
    "DIAGNOSIS_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DIAGNOSIS_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DIAGNOSIS_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DIAGNOSIS_STATUS_SECOND, DIAGNOSIS_SALARY_SECOND, DIAGNOSIS_SALARY_MIN_SECOND, \n",
    "              DIAGNOSIS_SALARY_MAX_SECOND, DIAGNOSIS_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Diagnosis/Others (JOBLUM) \n",
    "\n",
    "DIAGNOSIS_TITLE_LIST = np.concatenate((DIAGNOSIS_TITLE_FIRST, DIAGNOSIS_TITLE_SECOND))\n",
    "DIAGNOSIS_COMPANY_LIST = np.concatenate((DIAGNOSIS_COMPANY_FIRST, DIAGNOSIS_COMPANY_SECOND))\n",
    "DIAGNOSIS_DATE_LIST = np.concatenate((DIAGNOSIS_DATE_FIRST, DIAGNOSIS_DATE_SECOND))\n",
    "DIAGNOSIS_LOCATION_LIST = np.concatenate((DIAGNOSIS_LOCATION_FIRST, DIAGNOSIS_LOCATION_SECOND))\n",
    "DIAGNOSIS_STATUS_LIST = np.concatenate((DIAGNOSIS_STATUS_FIRST, DIAGNOSIS_STATUS_SECOND))\n",
    "DIAGNOSIS_SALARY_LIST = np.concatenate((DIAGNOSIS_SALARY_FIRST, DIAGNOSIS_SALARY_SECOND))\n",
    "DIAGNOSIS_SALARY_MIN_LIST = np.concatenate((DIAGNOSIS_SALARY_MIN_FIRST, DIAGNOSIS_SALARY_MIN_SECOND))\n",
    "DIAGNOSIS_SALARY_MAX_LIST = np.concatenate((DIAGNOSIS_SALARY_MAX_FIRST, DIAGNOSIS_SALARY_MAX_SECOND))\n",
    "DIAGNOSIS_DESCRIPTION_LIST = np.concatenate((DIAGNOSIS_DESCRIPTION_FIRST, DIAGNOSIS_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-monroe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Diagnosis/Others (JOBLUM) \n",
    "DIAGNOSIS={'Website': \"Joblum\",\n",
    "      'Job Title': DIAGNOSIS_TITLE_LIST, \n",
    "      'Category': \"Diagnosis/Others\", \n",
    "      'Company': DIAGNOSIS_COMPANY_LIST, \n",
    "      'Date Posted': DIAGNOSIS_DATE_LIST, \n",
    "      'Location': DIAGNOSIS_LOCATION_LIST, \n",
    "      'Status': DIAGNOSIS_STATUS_LIST, \n",
    "      'Salary': DIAGNOSIS_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': DIAGNOSIS_DESCRIPTION_LIST,\n",
    "      'Min Salary': DIAGNOSIS_SALARY_MIN_LIST,\n",
    "      'Max Salary': DIAGNOSIS_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "DIAGNOSIS_df = pd.DataFrame(data=DIAGNOSIS)\n",
    "DIAGNOSIS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_df.to_csv ('Joblum Data\\JOBLUM-DIAGNOSIS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-multimedia",
   "metadata": {},
   "source": [
    "### CATEGORY - Doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Doctor/Diagnosis (JOBLUM) - FIRST HALF\n",
    "\n",
    "DOCTOR_TITLE_FIRST = []\n",
    "DOCTOR_COMPANY_FIRST = []\n",
    "DOCTOR_DATE_FIRST = []\n",
    "DOCTOR_LOCATION_FIRST = []\n",
    "DOCTOR_STATUS_FIRST = []\n",
    "DOCTOR_SALARY_FIRST = []\n",
    "DOCTOR_SALARY_MIN_FIRST = []\n",
    "DOCTOR_SALARY_MAX_FIRST = []\n",
    "DOCTOR_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-doctor-diagnosis?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DOCTOR_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DOCTOR_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DOCTOR_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DOCTOR_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DOCTOR_STATUS_FIRST, DOCTOR_SALARY_FIRST, DOCTOR_SALARY_MIN_FIRST, \n",
    "              DOCTOR_SALARY_MAX_FIRST, DOCTOR_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Doctor/Diagnosis (JOBLUM) - SECOND HALF\n",
    "\n",
    "DOCTOR_TITLE_SECOND = []\n",
    "DOCTOR_COMPANY_SECOND = []\n",
    "DOCTOR_DATE_SECOND = []\n",
    "DOCTOR_LOCATION_SECOND = []\n",
    "DOCTOR_STATUS_SECOND = []\n",
    "DOCTOR_SALARY_SECOND = []\n",
    "DOCTOR_SALARY_MIN_SECOND = []\n",
    "DOCTOR_SALARY_MAX_SECOND = []\n",
    "DOCTOR_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DOCTOR_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DOCTOR_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DOCTOR_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DOCTOR_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DOCTOR_STATUS_SECOND, DOCTOR_SALARY_SECOND, DOCTOR_SALARY_MIN_SECOND, \n",
    "              DOCTOR_SALARY_MAX_SECOND, DOCTOR_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Doctor/Diagnosis (JOBLUM) \n",
    "\n",
    "DOCTOR_TITLE_LIST = np.concatenate((DOCTOR_TITLE_FIRST, DOCTOR_TITLE_SECOND))\n",
    "DOCTOR_COMPANY_LIST = np.concatenate((DOCTOR_COMPANY_FIRST, DOCTOR_COMPANY_SECOND))\n",
    "DOCTOR_DATE_LIST = np.concatenate((DOCTOR_DATE_FIRST, DOCTOR_DATE_SECOND))\n",
    "DOCTOR_LOCATION_LIST = np.concatenate((DOCTOR_LOCATION_FIRST, DOCTOR_LOCATION_SECOND))\n",
    "DOCTOR_STATUS_LIST = np.concatenate((DOCTOR_STATUS_FIRST, DOCTOR_STATUS_SECOND))\n",
    "DOCTOR_SALARY_LIST = np.concatenate((DOCTOR_SALARY_FIRST, DOCTOR_SALARY_SECOND))\n",
    "DOCTOR_SALARY_MIN_LIST = np.concatenate((DOCTOR_SALARY_MIN_FIRST, DOCTOR_SALARY_MIN_SECOND))\n",
    "DOCTOR_SALARY_MAX_LIST = np.concatenate((DOCTOR_SALARY_MAX_FIRST, DOCTOR_SALARY_MAX_SECOND))\n",
    "DOCTOR_DESCRIPTION_LIST = np.concatenate((DOCTOR_DESCRIPTION_FIRST, DOCTOR_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Doctor/Diagnosis (JOBLUM) \n",
    "DOCTOR={'Website': \"Joblum\",\n",
    "      'Job Title': DOCTOR_TITLE_LIST, \n",
    "      'Category': \"Doctor/DOCTOR\", \n",
    "      'Company': DOCTOR_COMPANY_LIST, \n",
    "      'Date Posted': DOCTOR_DATE_LIST, \n",
    "      'Location': DOCTOR_LOCATION_LIST, \n",
    "      'Status': DOCTOR_STATUS_LIST, \n",
    "      'Salary': DOCTOR_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': DOCTOR_DESCRIPTION_LIST,\n",
    "      'Min Salary': DOCTOR_SALARY_MIN_LIST,\n",
    "      'Max Salary': DOCTOR_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "DOCTOR_df = pd.DataFrame(data=DOCTOR)\n",
    "DOCTOR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCTOR_df.to_csv ('Joblum Data\\JOBLUM-DOCTOR.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-milwaukee",
   "metadata": {},
   "source": [
    "### CATEGORY - Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELEC_TITLE_FIRST = []\n",
    "ELEC_COMPANY_FIRST = []\n",
    "ELEC_DATE_FIRST = []\n",
    "ELEC_LOCATION_FIRST = []\n",
    "ELEC_STATUS_FIRST = []\n",
    "ELEC_SALARY_FIRST = []\n",
    "ELEC_SALARY_MIN_FIRST = []\n",
    "ELEC_SALARY_MAX_FIRST = []\n",
    "ELEC_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electrical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELEC_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELEC_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELEC_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELEC_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELEC_STATUS_FIRST, ELEC_SALARY_FIRST, ELEC_SALARY_MIN_FIRST, \n",
    "              ELEC_SALARY_MAX_FIRST, ELEC_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELEC_TITLE_SECOND = []\n",
    "ELEC_COMPANY_SECOND = []\n",
    "ELEC_DATE_SECOND = []\n",
    "ELEC_LOCATION_SECOND = []\n",
    "ELEC_STATUS_SECOND = []\n",
    "ELEC_SALARY_SECOND = []\n",
    "ELEC_SALARY_MIN_SECOND = []\n",
    "ELEC_SALARY_MAX_SECOND = []\n",
    "ELEC_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELEC_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELEC_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELEC_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELEC_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELEC_STATUS_SECOND, ELEC_SALARY_SECOND, ELEC_SALARY_MIN_SECOND, \n",
    "              ELEC_SALARY_MAX_SECOND, ELEC_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electrical (JOBLUM) \n",
    "\n",
    "ELEC_TITLE_LIST = np.concatenate((ELEC_TITLE_FIRST, ELEC_TITLE_SECOND))\n",
    "ELEC_COMPANY_LIST = np.concatenate((ELEC_COMPANY_FIRST, ELEC_COMPANY_SECOND))\n",
    "ELEC_DATE_LIST = np.concatenate((ELEC_DATE_FIRST, ELEC_DATE_SECOND))\n",
    "ELEC_LOCATION_LIST = np.concatenate((ELEC_LOCATION_FIRST, ELEC_LOCATION_SECOND))\n",
    "ELEC_STATUS_LIST = np.concatenate((ELEC_STATUS_FIRST, ELEC_STATUS_SECOND))\n",
    "ELEC_SALARY_LIST = np.concatenate((ELEC_SALARY_FIRST, ELEC_SALARY_SECOND))\n",
    "ELEC_SALARY_MIN_LIST = np.concatenate((ELEC_SALARY_MIN_FIRST, ELEC_SALARY_MIN_SECOND))\n",
    "ELEC_SALARY_MAX_LIST = np.concatenate((ELEC_SALARY_MAX_FIRST, ELEC_SALARY_MAX_SECOND))\n",
    "ELEC_DESCRIPTION_LIST = np.concatenate((ELEC_DESCRIPTION_FIRST, ELEC_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electrical (JOBLUM) \n",
    "ELEC={'Website': \"Joblum\",\n",
    "      'Job Title': ELEC_TITLE_LIST, \n",
    "      'Category': \"Electrical\", \n",
    "      'Company': ELEC_COMPANY_LIST, \n",
    "      'Date Posted': ELEC_DATE_LIST, \n",
    "      'Location': ELEC_LOCATION_LIST, \n",
    "      'Status': ELEC_STATUS_LIST, \n",
    "      'Salary': ELEC_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELEC_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELEC_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELEC_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELEC_df = pd.DataFrame(data=ELEC)\n",
    "ELEC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEC_df.to_csv ('Joblum Data\\JOBLUM-ELEC.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-integration",
   "metadata": {},
   "source": [
    "### CATEGORY - Electrical Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECENG_TITLE_FIRST = []\n",
    "ELECENG_COMPANY_FIRST = []\n",
    "ELECENG_DATE_FIRST = []\n",
    "ELECENG_LOCATION_FIRST = []\n",
    "ELECENG_STATUS_FIRST = []\n",
    "ELECENG_SALARY_FIRST = []\n",
    "ELECENG_SALARY_MIN_FIRST = []\n",
    "ELECENG_SALARY_MAX_FIRST = []\n",
    "ELECENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electrical-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECENG_STATUS_FIRST, ELECENG_SALARY_FIRST, ELECENG_SALARY_MIN_FIRST, \n",
    "              ELECENG_SALARY_MAX_FIRST, ELECENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECENG_TITLE_SECOND = []\n",
    "ELECENG_COMPANY_SECOND = []\n",
    "ELECENG_DATE_SECOND = []\n",
    "ELECENG_LOCATION_SECOND = []\n",
    "ELECENG_STATUS_SECOND = []\n",
    "ELECENG_SALARY_SECOND = []\n",
    "ELECENG_SALARY_MIN_SECOND = []\n",
    "ELECENG_SALARY_MAX_SECOND = []\n",
    "ELECENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECENG_STATUS_SECOND, ELECENG_SALARY_SECOND, ELECENG_SALARY_MIN_SECOND, \n",
    "              ELECENG_SALARY_MAX_SECOND, ELECENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electrical Engineering (JOBLUM) \n",
    "\n",
    "ELECENG_TITLE_LIST = np.concatenate((ELECENG_TITLE_FIRST, ELECENG_TITLE_SECOND))\n",
    "ELECENG_COMPANY_LIST = np.concatenate((ELECENG_COMPANY_FIRST, ELECENG_COMPANY_SECOND))\n",
    "ELECENG_DATE_LIST = np.concatenate((ELECENG_DATE_FIRST, ELECENG_DATE_SECOND))\n",
    "ELECENG_LOCATION_LIST = np.concatenate((ELECENG_LOCATION_FIRST, ELECENG_LOCATION_SECOND))\n",
    "ELECENG_STATUS_LIST = np.concatenate((ELECENG_STATUS_FIRST, ELECENG_STATUS_SECOND))\n",
    "ELECENG_SALARY_LIST = np.concatenate((ELECENG_SALARY_FIRST, ELECENG_SALARY_SECOND))\n",
    "ELECENG_SALARY_MIN_LIST = np.concatenate((ELECENG_SALARY_MIN_FIRST, ELECENG_SALARY_MIN_SECOND))\n",
    "ELECENG_SALARY_MAX_LIST = np.concatenate((ELECENG_SALARY_MAX_FIRST, ELECENG_SALARY_MAX_SECOND))\n",
    "ELECENG_DESCRIPTION_LIST = np.concatenate((ELECENG_DESCRIPTION_FIRST, ELECENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electrical Engineering (JOBLUM) \n",
    "ELECENG={'Website': \"Joblum\",\n",
    "      'Job Title': ELECENG_TITLE_LIST, \n",
    "      'Category': \"Electrical Engineering\", \n",
    "      'Company': ELECENG_COMPANY_LIST, \n",
    "      'Date Posted': ELECENG_DATE_LIST, \n",
    "      'Location': ELECENG_LOCATION_LIST, \n",
    "      'Status': ELECENG_STATUS_LIST, \n",
    "      'Salary': ELECENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECENG_df = pd.DataFrame(data=ELECENG)\n",
    "ELECENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELECENG_df.to_csv ('Joblum Data\\JOBLUM-ELECENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-rental",
   "metadata": {},
   "source": [
    "### CATEGORY - Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECTRO_TITLE_FIRST = []\n",
    "ELECTRO_COMPANY_FIRST = []\n",
    "ELECTRO_DATE_FIRST = []\n",
    "ELECTRO_LOCATION_FIRST = []\n",
    "ELECTRO_STATUS_FIRST = []\n",
    "ELECTRO_SALARY_FIRST = []\n",
    "ELECTRO_SALARY_MIN_FIRST = []\n",
    "ELECTRO_SALARY_MAX_FIRST = []\n",
    "ELECTRO_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electronics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTRO_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTRO_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTRO_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTRO_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTRO_STATUS_FIRST, ELECTRO_SALARY_FIRST, ELECTRO_SALARY_MIN_FIRST, \n",
    "              ELECTRO_SALARY_MAX_FIRST, ELECTRO_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECTRO_TITLE_SECOND = []\n",
    "ELECTRO_COMPANY_SECOND = []\n",
    "ELECTRO_DATE_SECOND = []\n",
    "ELECTRO_LOCATION_SECOND = []\n",
    "ELECTRO_STATUS_SECOND = []\n",
    "ELECTRO_SALARY_SECOND = []\n",
    "ELECTRO_SALARY_MIN_SECOND = []\n",
    "ELECTRO_SALARY_MAX_SECOND = []\n",
    "ELECTRO_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTRO_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTRO_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTRO_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTRO_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTRO_STATUS_SECOND, ELECTRO_SALARY_SECOND, ELECTRO_SALARY_MIN_SECOND, \n",
    "              ELECTRO_SALARY_MAX_SECOND, ELECTRO_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electronics (JOBLUM) \n",
    "\n",
    "ELECTRO_TITLE_LIST = np.concatenate((ELECTRO_TITLE_FIRST, ELECTRO_TITLE_SECOND))\n",
    "ELECTRO_COMPANY_LIST = np.concatenate((ELECTRO_COMPANY_FIRST, ELECTRO_COMPANY_SECOND))\n",
    "ELECTRO_DATE_LIST = np.concatenate((ELECTRO_DATE_FIRST, ELECTRO_DATE_SECOND))\n",
    "ELECTRO_LOCATION_LIST = np.concatenate((ELECTRO_LOCATION_FIRST, ELECTRO_LOCATION_SECOND))\n",
    "ELECTRO_STATUS_LIST = np.concatenate((ELECTRO_STATUS_FIRST, ELECTRO_STATUS_SECOND))\n",
    "ELECTRO_SALARY_LIST = np.concatenate((ELECTRO_SALARY_FIRST, ELECTRO_SALARY_SECOND))\n",
    "ELECTRO_SALARY_MIN_LIST = np.concatenate((ELECTRO_SALARY_MIN_FIRST, ELECTRO_SALARY_MIN_SECOND))\n",
    "ELECTRO_SALARY_MAX_LIST = np.concatenate((ELECTRO_SALARY_MAX_FIRST, ELECTRO_SALARY_MAX_SECOND))\n",
    "ELECTRO_DESCRIPTION_LIST = np.concatenate((ELECTRO_DESCRIPTION_FIRST, ELECTRO_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electronics (JOBLUM) \n",
    "ELECTRO={'Website': \"Joblum\",\n",
    "      'Job Title': ELECTRO_TITLE_LIST, \n",
    "      'Category': \"Electronics\", \n",
    "      'Company': ELECTRO_COMPANY_LIST, \n",
    "      'Date Posted': ELECTRO_DATE_LIST, \n",
    "      'Location': ELECTRO_LOCATION_LIST, \n",
    "      'Status': ELECTRO_STATUS_LIST, \n",
    "      'Salary': ELECTRO_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECTRO_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECTRO_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECTRO_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECTRO_df = pd.DataFrame(data=ELECTRO)\n",
    "ELECTRO_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-region",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ELECTRO_df.to_csv ('Joblum Data\\JOBLUM-ELECTRO.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-profit",
   "metadata": {},
   "source": [
    "### CATEGORY - Electronics Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECTROENG_TITLE_FIRST = []\n",
    "ELECTROENG_COMPANY_FIRST = []\n",
    "ELECTROENG_DATE_FIRST = []\n",
    "ELECTROENG_LOCATION_FIRST = []\n",
    "ELECTROENG_STATUS_FIRST = []\n",
    "ELECTROENG_SALARY_FIRST = []\n",
    "ELECTROENG_SALARY_MIN_FIRST = []\n",
    "ELECTROENG_SALARY_MAX_FIRST = []\n",
    "ELECTROENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electronics-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTROENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTROENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTROENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTROENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTROENG_STATUS_FIRST, ELECTROENG_SALARY_FIRST, ELECTROENG_SALARY_MIN_FIRST, \n",
    "              ELECTROENG_SALARY_MAX_FIRST, ELECTROENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECTROENG_TITLE_SECOND = []\n",
    "ELECTROENG_COMPANY_SECOND = []\n",
    "ELECTROENG_DATE_SECOND = []\n",
    "ELECTROENG_LOCATION_SECOND = []\n",
    "ELECTROENG_STATUS_SECOND = []\n",
    "ELECTROENG_SALARY_SECOND = []\n",
    "ELECTROENG_SALARY_MIN_SECOND = []\n",
    "ELECTROENG_SALARY_MAX_SECOND = []\n",
    "ELECTROENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTROENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTROENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTROENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTROENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTROENG_STATUS_SECOND, ELECTROENG_SALARY_SECOND, ELECTROENG_SALARY_MIN_SECOND, \n",
    "              ELECTROENG_SALARY_MAX_SECOND, ELECTROENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electronics Engineering (JOBLUM) \n",
    "\n",
    "ELECTROENG_TITLE_LIST = np.concatenate((ELECTROENG_TITLE_FIRST, ELECTROENG_TITLE_SECOND))\n",
    "ELECTROENG_COMPANY_LIST = np.concatenate((ELECTROENG_COMPANY_FIRST, ELECTROENG_COMPANY_SECOND))\n",
    "ELECTROENG_DATE_LIST = np.concatenate((ELECTROENG_DATE_FIRST, ELECTROENG_DATE_SECOND))\n",
    "ELECTROENG_LOCATION_LIST = np.concatenate((ELECTROENG_LOCATION_FIRST, ELECTROENG_LOCATION_SECOND))\n",
    "ELECTROENG_STATUS_LIST = np.concatenate((ELECTROENG_STATUS_FIRST, ELECTROENG_STATUS_SECOND))\n",
    "ELECTROENG_SALARY_LIST = np.concatenate((ELECTROENG_SALARY_FIRST, ELECTROENG_SALARY_SECOND))\n",
    "ELECTROENG_SALARY_MIN_LIST = np.concatenate((ELECTROENG_SALARY_MIN_FIRST, ELECTROENG_SALARY_MIN_SECOND))\n",
    "ELECTROENG_SALARY_MAX_LIST = np.concatenate((ELECTROENG_SALARY_MAX_FIRST, ELECTROENG_SALARY_MAX_SECOND))\n",
    "ELECTROENG_DESCRIPTION_LIST = np.concatenate((ELECTROENG_DESCRIPTION_FIRST, ELECTROENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electronics Engineering (JOBLUM) \n",
    "ELECTROENG={'Website': \"Joblum\",\n",
    "      'Job Title': ELECTROENG_TITLE_LIST, \n",
    "      'Category': \"Electronics Engineering\", \n",
    "      'Company': ELECTROENG_COMPANY_LIST, \n",
    "      'Date Posted': ELECTROENG_DATE_LIST, \n",
    "      'Location': ELECTROENG_LOCATION_LIST, \n",
    "      'Status': ELECTROENG_STATUS_LIST, \n",
    "      'Salary': ELECTROENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECTROENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECTROENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECTROENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECTROENG_df = pd.DataFrame(data=ELECTROENG)\n",
    "ELECTROENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELECTROENG_df.to_csv ('Joblum Data\\JOBLUM-ELECTROENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-spring",
   "metadata": {},
   "source": [
    "### CATEGORY - Environmental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENVI_TITLE_FIRST = []\n",
    "ENVI_COMPANY_FIRST = []\n",
    "ENVI_DATE_FIRST = []\n",
    "ENVI_LOCATION_FIRST = []\n",
    "ENVI_STATUS_FIRST = []\n",
    "ENVI_SALARY_FIRST = []\n",
    "ENVI_SALARY_MIN_FIRST = []\n",
    "ENVI_SALARY_MAX_FIRST = []\n",
    "ENVI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-environmental?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVI_STATUS_FIRST, ENVI_SALARY_FIRST, ENVI_SALARY_MIN_FIRST, \n",
    "              ENVI_SALARY_MAX_FIRST, ENVI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENVI_TITLE_SECOND = []\n",
    "ENVI_COMPANY_SECOND = []\n",
    "ENVI_DATE_SECOND = []\n",
    "ENVI_LOCATION_SECOND = []\n",
    "ENVI_STATUS_SECOND = []\n",
    "ENVI_SALARY_SECOND = []\n",
    "ENVI_SALARY_MIN_SECOND = []\n",
    "ENVI_SALARY_MAX_SECOND = []\n",
    "ENVI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVI_STATUS_SECOND, ENVI_SALARY_SECOND, ENVI_SALARY_MIN_SECOND, \n",
    "              ENVI_SALARY_MAX_SECOND, ENVI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Environmental (JOBLUM) \n",
    "\n",
    "ENVI_TITLE_LIST = np.concatenate((ENVI_TITLE_FIRST, ENVI_TITLE_SECOND))\n",
    "ENVI_COMPANY_LIST = np.concatenate((ENVI_COMPANY_FIRST, ENVI_COMPANY_SECOND))\n",
    "ENVI_DATE_LIST = np.concatenate((ENVI_DATE_FIRST, ENVI_DATE_SECOND))\n",
    "ENVI_LOCATION_LIST = np.concatenate((ENVI_LOCATION_FIRST, ENVI_LOCATION_SECOND))\n",
    "ENVI_STATUS_LIST = np.concatenate((ENVI_STATUS_FIRST, ENVI_STATUS_SECOND))\n",
    "ENVI_SALARY_LIST = np.concatenate((ENVI_SALARY_FIRST, ENVI_SALARY_SECOND))\n",
    "ENVI_SALARY_MIN_LIST = np.concatenate((ENVI_SALARY_MIN_FIRST, ENVI_SALARY_MIN_SECOND))\n",
    "ENVI_SALARY_MAX_LIST = np.concatenate((ENVI_SALARY_MAX_FIRST, ENVI_SALARY_MAX_SECOND))\n",
    "ENVI_DESCRIPTION_LIST = np.concatenate((ENVI_DESCRIPTION_FIRST, ENVI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Environmental (JOBLUM) \n",
    "ENVI={'Website': \"Joblum\",\n",
    "      'Job Title': ENVI_TITLE_LIST, \n",
    "      'Category': \"Environmental\", \n",
    "      'Company': ENVI_COMPANY_LIST, \n",
    "      'Date Posted': ENVI_DATE_LIST, \n",
    "      'Location': ENVI_LOCATION_LIST, \n",
    "      'Status': ENVI_STATUS_LIST, \n",
    "      'Salary': ENVI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENVI_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENVI_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENVI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "ENVI_df = pd.DataFrame(data=ENVI)\n",
    "ENVI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVI_df.to_csv ('Joblum Data\\JOBLUM-ENVI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-contemporary",
   "metadata": {},
   "source": [
    "### CATEGORY - Environmental Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENVIENG_TITLE_FIRST = []\n",
    "ENVIENG_COMPANY_FIRST = []\n",
    "ENVIENG_DATE_FIRST = []\n",
    "ENVIENG_LOCATION_FIRST = []\n",
    "ENVIENG_STATUS_FIRST = []\n",
    "ENVIENG_SALARY_FIRST = []\n",
    "ENVIENG_SALARY_MIN_FIRST = []\n",
    "ENVIENG_SALARY_MAX_FIRST = []\n",
    "ENVIENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-environmental-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVIENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVIENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVIENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVIENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVIENG_STATUS_FIRST, ENVIENG_SALARY_FIRST, ENVIENG_SALARY_MIN_FIRST, \n",
    "              ENVIENG_SALARY_MAX_FIRST, ENVIENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENVIENG_TITLE_SECOND = []\n",
    "ENVIENG_COMPANY_SECOND = []\n",
    "ENVIENG_DATE_SECOND = []\n",
    "ENVIENG_LOCATION_SECOND = []\n",
    "ENVIENG_STATUS_SECOND = []\n",
    "ENVIENG_SALARY_SECOND = []\n",
    "ENVIENG_SALARY_MIN_SECOND = []\n",
    "ENVIENG_SALARY_MAX_SECOND = []\n",
    "ENVIENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVIENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVIENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVIENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVIENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVIENG_STATUS_SECOND, ENVIENG_SALARY_SECOND, ENVIENG_SALARY_MIN_SECOND, \n",
    "              ENVIENG_SALARY_MAX_SECOND, ENVIENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Environmental Engineering (JOBLUM) \n",
    "\n",
    "ENVIENG_TITLE_LIST = np.concatenate((ENVIENG_TITLE_FIRST, ENVIENG_TITLE_SECOND))\n",
    "ENVIENG_COMPANY_LIST = np.concatenate((ENVIENG_COMPANY_FIRST, ENVIENG_COMPANY_SECOND))\n",
    "ENVIENG_DATE_LIST = np.concatenate((ENVIENG_DATE_FIRST, ENVIENG_DATE_SECOND))\n",
    "ENVIENG_LOCATION_LIST = np.concatenate((ENVIENG_LOCATION_FIRST, ENVIENG_LOCATION_SECOND))\n",
    "ENVIENG_STATUS_LIST = np.concatenate((ENVIENG_STATUS_FIRST, ENVIENG_STATUS_SECOND))\n",
    "ENVIENG_SALARY_LIST = np.concatenate((ENVIENG_SALARY_FIRST, ENVIENG_SALARY_SECOND))\n",
    "ENVIENG_SALARY_MIN_LIST = np.concatenate((ENVIENG_SALARY_MIN_FIRST, ENVIENG_SALARY_MIN_SECOND))\n",
    "ENVIENG_SALARY_MAX_LIST = np.concatenate((ENVIENG_SALARY_MAX_FIRST, ENVIENG_SALARY_MAX_SECOND))\n",
    "ENVIENG_DESCRIPTION_LIST = np.concatenate((ENVIENG_DESCRIPTION_FIRST, ENVIENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Environmental Engineering (JOBLUM) \n",
    "ENVIENG={'Website': \"Joblum\",\n",
    "      'Job Title': ENVIENG_TITLE_LIST, \n",
    "      'Category': \"Environmental Engineering\", \n",
    "      'Company': ENVIENG_COMPANY_LIST, \n",
    "      'Date Posted': ENVIENG_DATE_LIST, \n",
    "      'Location': ENVIENG_LOCATION_LIST, \n",
    "      'Status': ENVIENG_STATUS_LIST, \n",
    "      'Salary': ENVIENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENVIENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENVIENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENVIENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ENVIENG_df = pd.DataFrame(data=ENVIENG)\n",
    "ENVIENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIENG_df.to_csv ('Joblum Data\\JOBLUM-ENVIENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-toyota",
   "metadata": {},
   "source": [
    "### CATEGORY - Food Tech/Nutritionist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Food Tech/Nutritionist (JOBLUM) - FIRST HALF\n",
    "\n",
    "NUTRI_TITLE_FIRST = []\n",
    "NUTRI_COMPANY_FIRST = []\n",
    "NUTRI_DATE_FIRST = []\n",
    "NUTRI_LOCATION_FIRST = []\n",
    "NUTRI_STATUS_FIRST = []\n",
    "NUTRI_SALARY_FIRST = []\n",
    "NUTRI_SALARY_MIN_FIRST = []\n",
    "NUTRI_SALARY_MAX_FIRST = []\n",
    "NUTRI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-food-tech-nutritionist?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NUTRI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NUTRI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NUTRI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NUTRI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NUTRI_STATUS_FIRST, NUTRI_SALARY_FIRST, NUTRI_SALARY_MIN_FIRST, \n",
    "              NUTRI_SALARY_MAX_FIRST, NUTRI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Food Tech/Nutritionist (JOBLUM) - SECOND HALF\n",
    "\n",
    "NUTRI_TITLE_SECOND = []\n",
    "NUTRI_COMPANY_SECOND = []\n",
    "NUTRI_DATE_SECOND = []\n",
    "NUTRI_LOCATION_SECOND = []\n",
    "NUTRI_STATUS_SECOND = []\n",
    "NUTRI_SALARY_SECOND = []\n",
    "NUTRI_SALARY_MIN_SECOND = []\n",
    "NUTRI_SALARY_MAX_SECOND = []\n",
    "NUTRI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NUTRI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NUTRI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NUTRI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NUTRI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NUTRI_STATUS_SECOND, NUTRI_SALARY_SECOND, NUTRI_SALARY_MIN_SECOND, \n",
    "              NUTRI_SALARY_MAX_SECOND, NUTRI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Food Tech/Nutritionist (JOBLUM) \n",
    "\n",
    "NUTRI_TITLE_LIST = np.concatenate((NUTRI_TITLE_FIRST, NUTRI_TITLE_SECOND))\n",
    "NUTRI_COMPANY_LIST = np.concatenate((NUTRI_COMPANY_FIRST, NUTRI_COMPANY_SECOND))\n",
    "NUTRI_DATE_LIST = np.concatenate((NUTRI_DATE_FIRST, NUTRI_DATE_SECOND))\n",
    "NUTRI_LOCATION_LIST = np.concatenate((NUTRI_LOCATION_FIRST, NUTRI_LOCATION_SECOND))\n",
    "NUTRI_STATUS_LIST = np.concatenate((NUTRI_STATUS_FIRST, NUTRI_STATUS_SECOND))\n",
    "NUTRI_SALARY_LIST = np.concatenate((NUTRI_SALARY_FIRST, NUTRI_SALARY_SECOND))\n",
    "NUTRI_SALARY_MIN_LIST = np.concatenate((NUTRI_SALARY_MIN_FIRST, NUTRI_SALARY_MIN_SECOND))\n",
    "NUTRI_SALARY_MAX_LIST = np.concatenate((NUTRI_SALARY_MAX_FIRST, NUTRI_SALARY_MAX_SECOND))\n",
    "NUTRI_DESCRIPTION_LIST = np.concatenate((NUTRI_DESCRIPTION_FIRST, NUTRI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Food Tech/Nutritionist (JOBLUM) \n",
    "NUTRI={'Website': \"Joblum\",\n",
    "      'Job Title': NUTRI_TITLE_LIST, \n",
    "      'Category': \"Food Tech/Nutritionist\", \n",
    "      'Company': NUTRI_COMPANY_LIST, \n",
    "      'Date Posted': NUTRI_DATE_LIST, \n",
    "      'Location': NUTRI_LOCATION_LIST, \n",
    "      'Status': NUTRI_STATUS_LIST, \n",
    "      'Salary': NUTRI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': NUTRI_DESCRIPTION_LIST,\n",
    "      'Min Salary': NUTRI_SALARY_MIN_LIST,\n",
    "      'Max Salary': NUTRI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "NUTRI_df = pd.DataFrame(data=NUTRI)\n",
    "NUTRI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRI_df.to_csv ('Joblum Data\\JOBLUM-NUTRI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-creature",
   "metadata": {},
   "source": [
    "### CATEGORY - Geology/Geophysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Geology/Geophysics (JOBLUM) - FIRST HALF\n",
    "\n",
    "GEO_TITLE_FIRST = []\n",
    "GEO_COMPANY_FIRST = []\n",
    "GEO_DATE_FIRST = []\n",
    "GEO_LOCATION_FIRST = []\n",
    "GEO_STATUS_FIRST = []\n",
    "GEO_SALARY_FIRST = []\n",
    "GEO_SALARY_MIN_FIRST = []\n",
    "GEO_SALARY_MAX_FIRST = []\n",
    "GEO_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-geology-geophysics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        GEO_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        GEO_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        GEO_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        GEO_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, GEO_STATUS_FIRST, GEO_SALARY_FIRST, GEO_SALARY_MIN_FIRST, \n",
    "              GEO_SALARY_MAX_FIRST, GEO_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Geology/Geophysics (JOBLUM) - SECOND HALF\n",
    "\n",
    "GEO_TITLE_SECOND = []\n",
    "GEO_COMPANY_SECOND = []\n",
    "GEO_DATE_SECOND = []\n",
    "GEO_LOCATION_SECOND = []\n",
    "GEO_STATUS_SECOND = []\n",
    "GEO_SALARY_SECOND = []\n",
    "GEO_SALARY_MIN_SECOND = []\n",
    "GEO_SALARY_MAX_SECOND = []\n",
    "GEO_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        GEO_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        GEO_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        GEO_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        GEO_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, GEO_STATUS_SECOND, GEO_SALARY_SECOND, GEO_SALARY_MIN_SECOND, \n",
    "              GEO_SALARY_MAX_SECOND, GEO_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Geology/Geophysics (JOBLUM) \n",
    "\n",
    "GEO_TITLE_LIST = np.concatenate((GEO_TITLE_FIRST, GEO_TITLE_SECOND))\n",
    "GEO_COMPANY_LIST = np.concatenate((GEO_COMPANY_FIRST, GEO_COMPANY_SECOND))\n",
    "GEO_DATE_LIST = np.concatenate((GEO_DATE_FIRST, GEO_DATE_SECOND))\n",
    "GEO_LOCATION_LIST = np.concatenate((GEO_LOCATION_FIRST, GEO_LOCATION_SECOND))\n",
    "GEO_STATUS_LIST = np.concatenate((GEO_STATUS_FIRST, GEO_STATUS_SECOND))\n",
    "GEO_SALARY_LIST = np.concatenate((GEO_SALARY_FIRST, GEO_SALARY_SECOND))\n",
    "GEO_SALARY_MIN_LIST = np.concatenate((GEO_SALARY_MIN_FIRST, GEO_SALARY_MIN_SECOND))\n",
    "GEO_SALARY_MAX_LIST = np.concatenate((GEO_SALARY_MAX_FIRST, GEO_SALARY_MAX_SECOND))\n",
    "GEO_DESCRIPTION_LIST = np.concatenate((GEO_DESCRIPTION_FIRST, GEO_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Geology/Geophysics (JOBLUM) \n",
    "GEO={'Website': \"Joblum\",\n",
    "      'Job Title': GEO_TITLE_LIST, \n",
    "      'Category': \"Geology/Geophysics\", \n",
    "      'Company': GEO_COMPANY_LIST, \n",
    "      'Date Posted': GEO_DATE_LIST, \n",
    "      'Location': GEO_LOCATION_LIST, \n",
    "      'Status': GEO_STATUS_LIST, \n",
    "      'Salary': GEO_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': GEO_DESCRIPTION_LIST,\n",
    "      'Min Salary': GEO_SALARY_MIN_LIST,\n",
    "      'Max Salary': GEO_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "GEO_df = pd.DataFrame(data=GEO)\n",
    "GEO_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_df.to_csv ('Joblum Data\\JOBLUM-GEO.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-stomach",
   "metadata": {},
   "source": [
    "### CATEGORY - Industrial Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Industrial Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "INDUSENG_TITLE_FIRST = []\n",
    "INDUSENG_COMPANY_FIRST = []\n",
    "INDUSENG_DATE_FIRST = []\n",
    "INDUSENG_LOCATION_FIRST = []\n",
    "INDUSENG_STATUS_FIRST = []\n",
    "INDUSENG_SALARY_FIRST = []\n",
    "INDUSENG_SALARY_MIN_FIRST = []\n",
    "INDUSENG_SALARY_MAX_FIRST = []\n",
    "INDUSENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-industrial-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        INDUSENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        INDUSENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        INDUSENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        INDUSENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, INDUSENG_STATUS_FIRST, INDUSENG_SALARY_FIRST, INDUSENG_SALARY_MIN_FIRST, \n",
    "              INDUSENG_SALARY_MAX_FIRST, INDUSENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Industrial Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "INDUSENG_TITLE_SECOND = []\n",
    "INDUSENG_COMPANY_SECOND = []\n",
    "INDUSENG_DATE_SECOND = []\n",
    "INDUSENG_LOCATION_SECOND = []\n",
    "INDUSENG_STATUS_SECOND = []\n",
    "INDUSENG_SALARY_SECOND = []\n",
    "INDUSENG_SALARY_MIN_SECOND = []\n",
    "INDUSENG_SALARY_MAX_SECOND = []\n",
    "INDUSENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        INDUSENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        INDUSENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        INDUSENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        INDUSENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, INDUSENG_STATUS_SECOND, INDUSENG_SALARY_SECOND, INDUSENG_SALARY_MIN_SECOND, \n",
    "              INDUSENG_SALARY_MAX_SECOND, INDUSENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Industrial Engineering (JOBLUM) \n",
    "\n",
    "INDUSENG_TITLE_LIST = np.concatenate((INDUSENG_TITLE_FIRST, INDUSENG_TITLE_SECOND))\n",
    "INDUSENG_COMPANY_LIST = np.concatenate((INDUSENG_COMPANY_FIRST, INDUSENG_COMPANY_SECOND))\n",
    "INDUSENG_DATE_LIST = np.concatenate((INDUSENG_DATE_FIRST, INDUSENG_DATE_SECOND))\n",
    "INDUSENG_LOCATION_LIST = np.concatenate((INDUSENG_LOCATION_FIRST, INDUSENG_LOCATION_SECOND))\n",
    "INDUSENG_STATUS_LIST = np.concatenate((INDUSENG_STATUS_FIRST, INDUSENG_STATUS_SECOND))\n",
    "INDUSENG_SALARY_LIST = np.concatenate((INDUSENG_SALARY_FIRST, INDUSENG_SALARY_SECOND))\n",
    "INDUSENG_SALARY_MIN_LIST = np.concatenate((INDUSENG_SALARY_MIN_FIRST, INDUSENG_SALARY_MIN_SECOND))\n",
    "INDUSENG_SALARY_MAX_LIST = np.concatenate((INDUSENG_SALARY_MAX_FIRST, INDUSENG_SALARY_MAX_SECOND))\n",
    "INDUSENG_DESCRIPTION_LIST = np.concatenate((INDUSENG_DESCRIPTION_FIRST, INDUSENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Industrial Engineering (JOBLUM) \n",
    "INDUSENG={'Website': \"Joblum\",\n",
    "      'Job Title': INDUSENG_TITLE_LIST, \n",
    "      'Category': \"Industrial Engineering\", \n",
    "      'Company': INDUSENG_COMPANY_LIST, \n",
    "      'Date Posted': INDUSENG_DATE_LIST, \n",
    "      'Location': INDUSENG_LOCATION_LIST, \n",
    "      'Status': INDUSENG_STATUS_LIST, \n",
    "      'Salary': INDUSENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': INDUSENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': INDUSENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': INDUSENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "INDUSENG_df = pd.DataFrame(data=INDUSENG)\n",
    "INDUSENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDUSENG_df.to_csv ('Joblum Data\\JOBLUM-INDUSENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-option",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Hardware (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_HARDWARE_TITLE_FIRST = []\n",
    "IT_HARDWARE_COMPANY_FIRST = []\n",
    "IT_HARDWARE_DATE_FIRST = []\n",
    "IT_HARDWARE_LOCATION_FIRST = []\n",
    "IT_HARDWARE_STATUS_FIRST = []\n",
    "IT_HARDWARE_SALARY_FIRST = []\n",
    "IT_HARDWARE_SALARY_MIN_FIRST = []\n",
    "IT_HARDWARE_SALARY_MAX_FIRST = []\n",
    "IT_HARDWARE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-hardware?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_HARDWARE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_HARDWARE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_HARDWARE_STATUS_FIRST, IT_HARDWARE_SALARY_FIRST, IT_HARDWARE_SALARY_MIN_FIRST, \n",
    "              IT_HARDWARE_SALARY_MAX_FIRST, IT_HARDWARE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Hardware (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_HARDWARE_TITLE_SECOND = []\n",
    "IT_HARDWARE_COMPANY_SECOND = []\n",
    "IT_HARDWARE_DATE_SECOND = []\n",
    "IT_HARDWARE_LOCATION_SECOND = []\n",
    "IT_HARDWARE_STATUS_SECOND = []\n",
    "IT_HARDWARE_SALARY_SECOND = []\n",
    "IT_HARDWARE_SALARY_MIN_SECOND = []\n",
    "IT_HARDWARE_SALARY_MAX_SECOND = []\n",
    "IT_HARDWARE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_HARDWARE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_HARDWARE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_HARDWARE_STATUS_SECOND, IT_HARDWARE_SALARY_SECOND, IT_HARDWARE_SALARY_MIN_SECOND, \n",
    "              IT_HARDWARE_SALARY_MAX_SECOND, IT_HARDWARE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Hardware (JOBLUM) \n",
    "\n",
    "IT_HARDWARE_TITLE_LIST = np.concatenate((IT_HARDWARE_TITLE_FIRST, IT_HARDWARE_TITLE_SECOND))\n",
    "IT_HARDWARE_COMPANY_LIST = np.concatenate((IT_HARDWARE_COMPANY_FIRST, IT_HARDWARE_COMPANY_SECOND))\n",
    "IT_HARDWARE_DATE_LIST = np.concatenate((IT_HARDWARE_DATE_FIRST, IT_HARDWARE_DATE_SECOND))\n",
    "IT_HARDWARE_LOCATION_LIST = np.concatenate((IT_HARDWARE_LOCATION_FIRST, IT_HARDWARE_LOCATION_SECOND))\n",
    "IT_HARDWARE_STATUS_LIST = np.concatenate((IT_HARDWARE_STATUS_FIRST, IT_HARDWARE_STATUS_SECOND))\n",
    "IT_HARDWARE_SALARY_LIST = np.concatenate((IT_HARDWARE_SALARY_FIRST, IT_HARDWARE_SALARY_SECOND))\n",
    "IT_HARDWARE_SALARY_MIN_LIST = np.concatenate((IT_HARDWARE_SALARY_MIN_FIRST, IT_HARDWARE_SALARY_MIN_SECOND))\n",
    "IT_HARDWARE_SALARY_MAX_LIST = np.concatenate((IT_HARDWARE_SALARY_MAX_FIRST, IT_HARDWARE_SALARY_MAX_SECOND))\n",
    "IT_HARDWARE_DESCRIPTION_LIST = np.concatenate((IT_HARDWARE_DESCRIPTION_FIRST, IT_HARDWARE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Hardware (JOBLUM) \n",
    "IT_HARDWARE={'Website': \"Joblum\",\n",
    "      'Job Title': IT_HARDWARE_TITLE_LIST, \n",
    "      'Category': \"IT - Hardware\", \n",
    "      'Company': IT_HARDWARE_COMPANY_LIST, \n",
    "      'Date Posted': IT_HARDWARE_DATE_LIST, \n",
    "      'Location': IT_HARDWARE_LOCATION_LIST, \n",
    "      'Status': IT_HARDWARE_STATUS_LIST, \n",
    "      'Salary': IT_HARDWARE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_HARDWARE_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_HARDWARE_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_HARDWARE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_HARDWARE_df = pd.DataFrame(data=IT_HARDWARE)\n",
    "IT_HARDWARE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_HARDWARE_df.to_csv ('Joblum Data\\JOBLUM-IT_HARDWARE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-macedonia",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Network/Sys/DB Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Network/Sys/DB Admin (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_SYS_TITLE_FIRST = []\n",
    "IT_SYS_COMPANY_FIRST = []\n",
    "IT_SYS_DATE_FIRST = []\n",
    "IT_SYS_LOCATION_FIRST = []\n",
    "IT_SYS_STATUS_FIRST = []\n",
    "IT_SYS_SALARY_FIRST = []\n",
    "IT_SYS_SALARY_MIN_FIRST = []\n",
    "IT_SYS_SALARY_MAX_FIRST = []\n",
    "IT_SYS_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-network-sys-db-admin?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SYS_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SYS_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SYS_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SYS_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SYS_STATUS_FIRST, IT_SYS_SALARY_FIRST, IT_SYS_SALARY_MIN_FIRST, \n",
    "              IT_SYS_SALARY_MAX_FIRST, IT_SYS_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Network/Sys/DB Admin (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_SYS_TITLE_SECOND = []\n",
    "IT_SYS_COMPANY_SECOND = []\n",
    "IT_SYS_DATE_SECOND = []\n",
    "IT_SYS_LOCATION_SECOND = []\n",
    "IT_SYS_STATUS_SECOND = []\n",
    "IT_SYS_SALARY_SECOND = []\n",
    "IT_SYS_SALARY_MIN_SECOND = []\n",
    "IT_SYS_SALARY_MAX_SECOND = []\n",
    "IT_SYS_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SYS_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SYS_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SYS_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SYS_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SYS_STATUS_SECOND, IT_SYS_SALARY_SECOND, IT_SYS_SALARY_MIN_SECOND, \n",
    "              IT_SYS_SALARY_MAX_SECOND, IT_SYS_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Network/Sys/DB Admin (JOBLUM) \n",
    "\n",
    "IT_SYS_TITLE_LIST = np.concatenate((IT_SYS_TITLE_FIRST, IT_SYS_TITLE_SECOND))\n",
    "IT_SYS_COMPANY_LIST = np.concatenate((IT_SYS_COMPANY_FIRST, IT_SYS_COMPANY_SECOND))\n",
    "IT_SYS_DATE_LIST = np.concatenate((IT_SYS_DATE_FIRST, IT_SYS_DATE_SECOND))\n",
    "IT_SYS_LOCATION_LIST = np.concatenate((IT_SYS_LOCATION_FIRST, IT_SYS_LOCATION_SECOND))\n",
    "IT_SYS_STATUS_LIST = np.concatenate((IT_SYS_STATUS_FIRST, IT_SYS_STATUS_SECOND))\n",
    "IT_SYS_SALARY_LIST = np.concatenate((IT_SYS_SALARY_FIRST, IT_SYS_SALARY_SECOND))\n",
    "IT_SYS_SALARY_MIN_LIST = np.concatenate((IT_SYS_SALARY_MIN_FIRST, IT_SYS_SALARY_MIN_SECOND))\n",
    "IT_SYS_SALARY_MAX_LIST = np.concatenate((IT_SYS_SALARY_MAX_FIRST, IT_SYS_SALARY_MAX_SECOND))\n",
    "IT_SYS_DESCRIPTION_LIST = np.concatenate((IT_SYS_DESCRIPTION_FIRST, IT_SYS_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Network/Sys/DB Admin (JOBLUM) \n",
    "IT_SYS={'Website': \"Joblum\",\n",
    "      'Job Title': IT_SYS_TITLE_LIST, \n",
    "      'Category': \"IT - Network/Sys/DB Admin\", \n",
    "      'Company': IT_SYS_COMPANY_LIST, \n",
    "      'Date Posted': IT_SYS_DATE_LIST, \n",
    "      'Location': IT_SYS_LOCATION_LIST, \n",
    "      'Status': IT_SYS_STATUS_LIST, \n",
    "      'Salary': IT_SYS_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_SYS_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_SYS_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_SYS_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_SYS_df = pd.DataFrame(data=IT_SYS)\n",
    "IT_SYS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_SYS_df.to_csv ('Joblum Data\\JOBLUM-IT_SYS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-syracuse",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Software (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_SOFTWARE_TITLE_FIRST = []\n",
    "IT_SOFTWARE_COMPANY_FIRST = []\n",
    "IT_SOFTWARE_DATE_FIRST = []\n",
    "IT_SOFTWARE_LOCATION_FIRST = []\n",
    "IT_SOFTWARE_STATUS_FIRST = []\n",
    "IT_SOFTWARE_SALARY_FIRST = []\n",
    "IT_SOFTWARE_SALARY_MIN_FIRST = []\n",
    "IT_SOFTWARE_SALARY_MAX_FIRST = []\n",
    "IT_SOFTWARE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-software?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SOFTWARE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SOFTWARE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SOFTWARE_STATUS_FIRST, IT_SOFTWARE_SALARY_FIRST, IT_SOFTWARE_SALARY_MIN_FIRST, \n",
    "              IT_SOFTWARE_SALARY_MAX_FIRST, IT_SOFTWARE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Software (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_SOFTWARE_TITLE_SECOND = []\n",
    "IT_SOFTWARE_COMPANY_SECOND = []\n",
    "IT_SOFTWARE_DATE_SECOND = []\n",
    "IT_SOFTWARE_LOCATION_SECOND = []\n",
    "IT_SOFTWARE_STATUS_SECOND = []\n",
    "IT_SOFTWARE_SALARY_SECOND = []\n",
    "IT_SOFTWARE_SALARY_MIN_SECOND = []\n",
    "IT_SOFTWARE_SALARY_MAX_SECOND = []\n",
    "IT_SOFTWARE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SOFTWARE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SOFTWARE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SOFTWARE_STATUS_SECOND, IT_SOFTWARE_SALARY_SECOND, IT_SOFTWARE_SALARY_MIN_SECOND, \n",
    "              IT_SOFTWARE_SALARY_MAX_SECOND, IT_SOFTWARE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Software (JOBLUM) \n",
    "\n",
    "IT_SOFTWARE_TITLE_LIST = np.concatenate((IT_SOFTWARE_TITLE_FIRST, IT_SOFTWARE_TITLE_SECOND))\n",
    "IT_SOFTWARE_COMPANY_LIST = np.concatenate((IT_SOFTWARE_COMPANY_FIRST, IT_SOFTWARE_COMPANY_SECOND))\n",
    "IT_SOFTWARE_DATE_LIST = np.concatenate((IT_SOFTWARE_DATE_FIRST, IT_SOFTWARE_DATE_SECOND))\n",
    "IT_SOFTWARE_LOCATION_LIST = np.concatenate((IT_SOFTWARE_LOCATION_FIRST, IT_SOFTWARE_LOCATION_SECOND))\n",
    "IT_SOFTWARE_STATUS_LIST = np.concatenate((IT_SOFTWARE_STATUS_FIRST, IT_SOFTWARE_STATUS_SECOND))\n",
    "IT_SOFTWARE_SALARY_LIST = np.concatenate((IT_SOFTWARE_SALARY_FIRST, IT_SOFTWARE_SALARY_SECOND))\n",
    "IT_SOFTWARE_SALARY_MIN_LIST = np.concatenate((IT_SOFTWARE_SALARY_MIN_FIRST, IT_SOFTWARE_SALARY_MIN_SECOND))\n",
    "IT_SOFTWARE_SALARY_MAX_LIST = np.concatenate((IT_SOFTWARE_SALARY_MAX_FIRST, IT_SOFTWARE_SALARY_MAX_SECOND))\n",
    "IT_SOFTWARE_DESCRIPTION_LIST = np.concatenate((IT_SOFTWARE_DESCRIPTION_FIRST, IT_SOFTWARE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Software (JOBLUM) \n",
    "IT_SOFTWARE={'Website': \"Joblum\",\n",
    "      'Job Title': IT_SOFTWARE_TITLE_LIST, \n",
    "      'Category': \"IT - Software\", \n",
    "      'Company': IT_SOFTWARE_COMPANY_LIST, \n",
    "      'Date Posted': IT_SOFTWARE_DATE_LIST, \n",
    "      'Location': IT_SOFTWARE_LOCATION_LIST, \n",
    "      'Status': IT_SOFTWARE_STATUS_LIST, \n",
    "      'Salary': IT_SOFTWARE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_SOFTWARE_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_SOFTWARE_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_SOFTWARE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_SOFTWARE_df = pd.DataFrame(data=IT_SOFTWARE)\n",
    "IT_SOFTWARE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_SOFTWARE_df.to_csv ('Joblum Data\\JOBLUM-IT_SOFTWARE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-inquiry",
   "metadata": {},
   "source": [
    "### CATEGORY - Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Maintenance (JOBLUM) - FIRST HALF\n",
    "\n",
    "MAINTENANCE_TITLE_FIRST = []\n",
    "MAINTENANCE_COMPANY_FIRST = []\n",
    "MAINTENANCE_DATE_FIRST = []\n",
    "MAINTENANCE_LOCATION_FIRST = []\n",
    "MAINTENANCE_STATUS_FIRST = []\n",
    "MAINTENANCE_SALARY_FIRST = []\n",
    "MAINTENANCE_SALARY_MIN_FIRST = []\n",
    "MAINTENANCE_SALARY_MAX_FIRST = []\n",
    "MAINTENANCE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-maintenance?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MAINTENANCE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MAINTENANCE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MAINTENANCE_STATUS_FIRST, MAINTENANCE_SALARY_FIRST, MAINTENANCE_SALARY_MIN_FIRST, \n",
    "              MAINTENANCE_SALARY_MAX_FIRST, MAINTENANCE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Maintenance (JOBLUM) - SECOND HALF\n",
    "\n",
    "MAINTENANCE_TITLE_SECOND = []\n",
    "MAINTENANCE_COMPANY_SECOND = []\n",
    "MAINTENANCE_DATE_SECOND = []\n",
    "MAINTENANCE_LOCATION_SECOND = []\n",
    "MAINTENANCE_STATUS_SECOND = []\n",
    "MAINTENANCE_SALARY_SECOND = []\n",
    "MAINTENANCE_SALARY_MIN_SECOND = []\n",
    "MAINTENANCE_SALARY_MAX_SECOND = []\n",
    "MAINTENANCE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MAINTENANCE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MAINTENANCE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MAINTENANCE_STATUS_SECOND, MAINTENANCE_SALARY_SECOND, MAINTENANCE_SALARY_MIN_SECOND, \n",
    "              MAINTENANCE_SALARY_MAX_SECOND, MAINTENANCE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Maintenance (JOBLUM) \n",
    "\n",
    "MAINTENANCE_TITLE_LIST = np.concatenate((MAINTENANCE_TITLE_FIRST, MAINTENANCE_TITLE_SECOND))\n",
    "MAINTENANCE_COMPANY_LIST = np.concatenate((MAINTENANCE_COMPANY_FIRST, MAINTENANCE_COMPANY_SECOND))\n",
    "MAINTENANCE_DATE_LIST = np.concatenate((MAINTENANCE_DATE_FIRST, MAINTENANCE_DATE_SECOND))\n",
    "MAINTENANCE_LOCATION_LIST = np.concatenate((MAINTENANCE_LOCATION_FIRST, MAINTENANCE_LOCATION_SECOND))\n",
    "MAINTENANCE_STATUS_LIST = np.concatenate((MAINTENANCE_STATUS_FIRST, MAINTENANCE_STATUS_SECOND))\n",
    "MAINTENANCE_SALARY_LIST = np.concatenate((MAINTENANCE_SALARY_FIRST, MAINTENANCE_SALARY_SECOND))\n",
    "MAINTENANCE_SALARY_MIN_LIST = np.concatenate((MAINTENANCE_SALARY_MIN_FIRST, MAINTENANCE_SALARY_MIN_SECOND))\n",
    "MAINTENANCE_SALARY_MAX_LIST = np.concatenate((MAINTENANCE_SALARY_MAX_FIRST, MAINTENANCE_SALARY_MAX_SECOND))\n",
    "MAINTENANCE_DESCRIPTION_LIST = np.concatenate((MAINTENANCE_DESCRIPTION_FIRST, MAINTENANCE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Maintenance (JOBLUM) \n",
    "MAINTENANCE={'Website': \"Joblum\",\n",
    "      'Job Title': MAINTENANCE_TITLE_LIST, \n",
    "      'Category': \"Maintenance\", \n",
    "      'Company': MAINTENANCE_COMPANY_LIST, \n",
    "      'Date Posted': MAINTENANCE_DATE_LIST, \n",
    "      'Location': MAINTENANCE_LOCATION_LIST, \n",
    "      'Status': MAINTENANCE_STATUS_LIST, \n",
    "      'Salary': MAINTENANCE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MAINTENANCE_DESCRIPTION_LIST,\n",
    "      'Min Salary': MAINTENANCE_SALARY_MIN_LIST,\n",
    "      'Max Salary': MAINTENANCE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MAINTENANCE_df = pd.DataFrame(data=MAINTENANCE)\n",
    "MAINTENANCE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAINTENANCE_df.to_csv ('Joblum Data\\JOBLUM-MAINTENANCE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-discipline",
   "metadata": {},
   "source": [
    "### CATEGORY - Mechanical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical (JOBLUM) - FIRST HALF\n",
    "\n",
    "MECH_TITLE_FIRST = []\n",
    "MECH_COMPANY_FIRST = []\n",
    "MECH_DATE_FIRST = []\n",
    "MECH_LOCATION_FIRST = []\n",
    "MECH_STATUS_FIRST = []\n",
    "MECH_SALARY_FIRST = []\n",
    "MECH_SALARY_MIN_FIRST = []\n",
    "MECH_SALARY_MAX_FIRST = []\n",
    "MECH_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-mechanical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECH_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECH_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECH_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECH_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECH_STATUS_FIRST, MECH_SALARY_FIRST, MECH_SALARY_MIN_FIRST, \n",
    "              MECH_SALARY_MAX_FIRST, MECH_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical (JOBLUM) - SECOND HALF\n",
    "\n",
    "MECH_TITLE_SECOND = []\n",
    "MECH_COMPANY_SECOND = []\n",
    "MECH_DATE_SECOND = []\n",
    "MECH_LOCATION_SECOND = []\n",
    "MECH_STATUS_SECOND = []\n",
    "MECH_SALARY_SECOND = []\n",
    "MECH_SALARY_MIN_SECOND = []\n",
    "MECH_SALARY_MAX_SECOND = []\n",
    "MECH_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECH_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECH_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECH_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECH_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECH_STATUS_SECOND, MECH_SALARY_SECOND, MECH_SALARY_MIN_SECOND, \n",
    "              MECH_SALARY_MAX_SECOND, MECH_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Mechanical (JOBLUM) \n",
    "\n",
    "MECH_TITLE_LIST = np.concatenate((MECH_TITLE_FIRST, MECH_TITLE_SECOND))\n",
    "MECH_COMPANY_LIST = np.concatenate((MECH_COMPANY_FIRST, MECH_COMPANY_SECOND))\n",
    "MECH_DATE_LIST = np.concatenate((MECH_DATE_FIRST, MECH_DATE_SECOND))\n",
    "MECH_LOCATION_LIST = np.concatenate((MECH_LOCATION_FIRST, MECH_LOCATION_SECOND))\n",
    "MECH_STATUS_LIST = np.concatenate((MECH_STATUS_FIRST, MECH_STATUS_SECOND))\n",
    "MECH_SALARY_LIST = np.concatenate((MECH_SALARY_FIRST, MECH_SALARY_SECOND))\n",
    "MECH_SALARY_MIN_LIST = np.concatenate((MECH_SALARY_MIN_FIRST, MECH_SALARY_MIN_SECOND))\n",
    "MECH_SALARY_MAX_LIST = np.concatenate((MECH_SALARY_MAX_FIRST, MECH_SALARY_MAX_SECOND))\n",
    "MECH_DESCRIPTION_LIST = np.concatenate((MECH_DESCRIPTION_FIRST, MECH_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Mechanical (JOBLUM) \n",
    "MECH={'Website': \"Joblum\",\n",
    "      'Job Title': MECH_TITLE_LIST, \n",
    "      'Category': \"Mechanical\", \n",
    "      'Company': MECH_COMPANY_LIST, \n",
    "      'Date Posted': MECH_DATE_LIST, \n",
    "      'Location': MECH_LOCATION_LIST, \n",
    "      'Status': MECH_STATUS_LIST, \n",
    "      'Salary': MECH_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MECH_DESCRIPTION_LIST,\n",
    "      'Min Salary': MECH_SALARY_MIN_LIST,\n",
    "      'Max Salary': MECH_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MECH_df = pd.DataFrame(data=MECH)\n",
    "MECH_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "MECH_df.to_csv ('Joblum Data\\JOBLUM-MECH.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-mitchell",
   "metadata": {},
   "source": [
    "### CATEGORY - Mechanical/Automotive Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical/Automotive Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "MECHENG_TITLE_FIRST = []\n",
    "MECHENG_COMPANY_FIRST = []\n",
    "MECHENG_DATE_FIRST = []\n",
    "MECHENG_LOCATION_FIRST = []\n",
    "MECHENG_STATUS_FIRST = []\n",
    "MECHENG_SALARY_FIRST = []\n",
    "MECHENG_SALARY_MIN_FIRST = []\n",
    "MECHENG_SALARY_MAX_FIRST = []\n",
    "MECHENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-mechanical-automotive-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECHENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECHENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECHENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECHENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECHENG_STATUS_FIRST, MECHENG_SALARY_FIRST, MECHENG_SALARY_MIN_FIRST, \n",
    "              MECHENG_SALARY_MAX_FIRST, MECHENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical/Automotive Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "MECHENG_TITLE_SECOND = []\n",
    "MECHENG_COMPANY_SECOND = []\n",
    "MECHENG_DATE_SECOND = []\n",
    "MECHENG_LOCATION_SECOND = []\n",
    "MECHENG_STATUS_SECOND = []\n",
    "MECHENG_SALARY_SECOND = []\n",
    "MECHENG_SALARY_MIN_SECOND = []\n",
    "MECHENG_SALARY_MAX_SECOND = []\n",
    "MECHENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECHENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECHENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECHENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECHENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECHENG_STATUS_SECOND, MECHENG_SALARY_SECOND, MECHENG_SALARY_MIN_SECOND, \n",
    "              MECHENG_SALARY_MAX_SECOND, MECHENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Mechanical/Automotive Engineering (JOBLUM) \n",
    "\n",
    "MECHENG_TITLE_LIST = np.concatenate((MECHENG_TITLE_FIRST, MECHENG_TITLE_SECOND))\n",
    "MECHENG_COMPANY_LIST = np.concatenate((MECHENG_COMPANY_FIRST, MECHENG_COMPANY_SECOND))\n",
    "MECHENG_DATE_LIST = np.concatenate((MECHENG_DATE_FIRST, MECHENG_DATE_SECOND))\n",
    "MECHENG_LOCATION_LIST = np.concatenate((MECHENG_LOCATION_FIRST, MECHENG_LOCATION_SECOND))\n",
    "MECHENG_STATUS_LIST = np.concatenate((MECHENG_STATUS_FIRST, MECHENG_STATUS_SECOND))\n",
    "MECHENG_SALARY_LIST = np.concatenate((MECHENG_SALARY_FIRST, MECHENG_SALARY_SECOND))\n",
    "MECHENG_SALARY_MIN_LIST = np.concatenate((MECHENG_SALARY_MIN_FIRST, MECHENG_SALARY_MIN_SECOND))\n",
    "MECHENG_SALARY_MAX_LIST = np.concatenate((MECHENG_SALARY_MAX_FIRST, MECHENG_SALARY_MAX_SECOND))\n",
    "MECHENG_DESCRIPTION_LIST = np.concatenate((MECHENG_DESCRIPTION_FIRST, MECHENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Mechanical/Automotive Engineering (JOBLUM) \n",
    "MECHENG={'Website': \"Joblum\",\n",
    "      'Job Title': MECHENG_TITLE_LIST, \n",
    "      'Category': \"Mechanical/Automotive Engineering\", \n",
    "      'Company': MECHENG_COMPANY_LIST, \n",
    "      'Date Posted': MECHENG_DATE_LIST, \n",
    "      'Location': MECHENG_LOCATION_LIST, \n",
    "      'Status': MECHENG_STATUS_LIST, \n",
    "      'Salary': MECHENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MECHENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': MECHENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': MECHENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MECHENG_df = pd.DataFrame(data=MECHENG)\n",
    "MECHENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "MECHENG_df.to_csv ('Joblum Data\\JOBLUM-MECHENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-agency",
   "metadata": {},
   "source": [
    "### CATEGORY - Nurse/Medical Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Nurse/Medical Support (JOBLUM) - FIRST HALF\n",
    "\n",
    "NURSE_TITLE_FIRST = []\n",
    "NURSE_COMPANY_FIRST = []\n",
    "NURSE_DATE_FIRST = []\n",
    "NURSE_LOCATION_FIRST = []\n",
    "NURSE_STATUS_FIRST = []\n",
    "NURSE_SALARY_FIRST = []\n",
    "NURSE_SALARY_MIN_FIRST = []\n",
    "NURSE_SALARY_MAX_FIRST = []\n",
    "NURSE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-nurse-medical-support?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NURSE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NURSE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NURSE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NURSE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NURSE_STATUS_FIRST, NURSE_SALARY_FIRST, NURSE_SALARY_MIN_FIRST, \n",
    "              NURSE_SALARY_MAX_FIRST, NURSE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Nurse/Medical Support (JOBLUM) - SECOND HALF\n",
    "\n",
    "NURSE_TITLE_SECOND = []\n",
    "NURSE_COMPANY_SECOND = []\n",
    "NURSE_DATE_SECOND = []\n",
    "NURSE_LOCATION_SECOND = []\n",
    "NURSE_STATUS_SECOND = []\n",
    "NURSE_SALARY_SECOND = []\n",
    "NURSE_SALARY_MIN_SECOND = []\n",
    "NURSE_SALARY_MAX_SECOND = []\n",
    "NURSE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NURSE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NURSE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NURSE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NURSE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NURSE_STATUS_SECOND, NURSE_SALARY_SECOND, NURSE_SALARY_MIN_SECOND, \n",
    "              NURSE_SALARY_MAX_SECOND, NURSE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Nurse/Medical Support (JOBLUM) \n",
    "\n",
    "NURSE_TITLE_LIST = np.concatenate((NURSE_TITLE_FIRST, NURSE_TITLE_SECOND))\n",
    "NURSE_COMPANY_LIST = np.concatenate((NURSE_COMPANY_FIRST, NURSE_COMPANY_SECOND))\n",
    "NURSE_DATE_LIST = np.concatenate((NURSE_DATE_FIRST, NURSE_DATE_SECOND))\n",
    "NURSE_LOCATION_LIST = np.concatenate((NURSE_LOCATION_FIRST, NURSE_LOCATION_SECOND))\n",
    "NURSE_STATUS_LIST = np.concatenate((NURSE_STATUS_FIRST, NURSE_STATUS_SECOND))\n",
    "NURSE_SALARY_LIST = np.concatenate((NURSE_SALARY_FIRST, NURSE_SALARY_SECOND))\n",
    "NURSE_SALARY_MIN_LIST = np.concatenate((NURSE_SALARY_MIN_FIRST, NURSE_SALARY_MIN_SECOND))\n",
    "NURSE_SALARY_MAX_LIST = np.concatenate((NURSE_SALARY_MAX_FIRST, NURSE_SALARY_MAX_SECOND))\n",
    "NURSE_DESCRIPTION_LIST = np.concatenate((NURSE_DESCRIPTION_FIRST, NURSE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Nurse/Medical Support (JOBLUM) \n",
    "NURSE={'Website': \"Joblum\",\n",
    "      'Job Title': NURSE_TITLE_LIST, \n",
    "      'Category': \"Nurse/Medical Support\", \n",
    "      'Company': NURSE_COMPANY_LIST, \n",
    "      'Date Posted': NURSE_DATE_LIST, \n",
    "      'Location': NURSE_LOCATION_LIST, \n",
    "      'Status': NURSE_STATUS_LIST, \n",
    "      'Salary': NURSE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': NURSE_DESCRIPTION_LIST,\n",
    "      'Min Salary': NURSE_SALARY_MIN_LIST,\n",
    "      'Max Salary': NURSE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "NURSE_df = pd.DataFrame(data=NURSE)\n",
    "NURSE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "NURSE_df.to_csv ('Joblum Data\\JOBLUM-NURSE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-columbia",
   "metadata": {},
   "source": [
    "### CATEGORY - Oil/Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas (JOBLUM) - FIRST HALF\n",
    "\n",
    "OIL_TITLE_FIRST = []\n",
    "OIL_COMPANY_FIRST = []\n",
    "OIL_DATE_FIRST = []\n",
    "OIL_LOCATION_FIRST = []\n",
    "OIL_STATUS_FIRST = []\n",
    "OIL_SALARY_FIRST = []\n",
    "OIL_SALARY_MIN_FIRST = []\n",
    "OIL_SALARY_MAX_FIRST = []\n",
    "OIL_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-oil-gas?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OIL_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OIL_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OIL_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OIL_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OIL_STATUS_FIRST, OIL_SALARY_FIRST, OIL_SALARY_MIN_FIRST, \n",
    "              OIL_SALARY_MAX_FIRST, OIL_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas (JOBLUM) - SECOND HALF\n",
    "\n",
    "OIL_TITLE_SECOND = []\n",
    "OIL_COMPANY_SECOND = []\n",
    "OIL_DATE_SECOND = []\n",
    "OIL_LOCATION_SECOND = []\n",
    "OIL_STATUS_SECOND = []\n",
    "OIL_SALARY_SECOND = []\n",
    "OIL_SALARY_MIN_SECOND = []\n",
    "OIL_SALARY_MAX_SECOND = []\n",
    "OIL_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OIL_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OIL_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OIL_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OIL_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OIL_STATUS_SECOND, OIL_SALARY_SECOND, OIL_SALARY_MIN_SECOND, \n",
    "              OIL_SALARY_MAX_SECOND, OIL_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Oil/Gas (JOBLUM) \n",
    "\n",
    "OIL_TITLE_LIST = np.concatenate((OIL_TITLE_FIRST, OIL_TITLE_SECOND))\n",
    "OIL_COMPANY_LIST = np.concatenate((OIL_COMPANY_FIRST, OIL_COMPANY_SECOND))\n",
    "OIL_DATE_LIST = np.concatenate((OIL_DATE_FIRST, OIL_DATE_SECOND))\n",
    "OIL_LOCATION_LIST = np.concatenate((OIL_LOCATION_FIRST, OIL_LOCATION_SECOND))\n",
    "OIL_STATUS_LIST = np.concatenate((OIL_STATUS_FIRST, OIL_STATUS_SECOND))\n",
    "OIL_SALARY_LIST = np.concatenate((OIL_SALARY_FIRST, OIL_SALARY_SECOND))\n",
    "OIL_SALARY_MIN_LIST = np.concatenate((OIL_SALARY_MIN_FIRST, OIL_SALARY_MIN_SECOND))\n",
    "OIL_SALARY_MAX_LIST = np.concatenate((OIL_SALARY_MAX_FIRST, OIL_SALARY_MAX_SECOND))\n",
    "OIL_DESCRIPTION_LIST = np.concatenate((OIL_DESCRIPTION_FIRST, OIL_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Oil/Gas (JOBLUM) \n",
    "OIL={'Website': \"Joblum\",\n",
    "      'Job Title': OIL_TITLE_LIST, \n",
    "      'Category': \"Oil/Gas\", \n",
    "      'Company': OIL_COMPANY_LIST, \n",
    "      'Date Posted': OIL_DATE_LIST, \n",
    "      'Location': OIL_LOCATION_LIST, \n",
    "      'Status': OIL_STATUS_LIST, \n",
    "      'Salary': OIL_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': OIL_DESCRIPTION_LIST,\n",
    "      'Min Salary': OIL_SALARY_MIN_LIST,\n",
    "      'Max Salary': OIL_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "OIL_df = pd.DataFrame(data=OIL)\n",
    "OIL_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "OIL_df.to_csv ('Joblum Data\\JOBLUM-OIL.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-southwest",
   "metadata": {},
   "source": [
    "### CATEGORY - Oil/Gas Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "OILENG_TITLE_FIRST = []\n",
    "OILENG_COMPANY_FIRST = []\n",
    "OILENG_DATE_FIRST = []\n",
    "OILENG_LOCATION_FIRST = []\n",
    "OILENG_STATUS_FIRST = []\n",
    "OILENG_SALARY_FIRST = []\n",
    "OILENG_SALARY_MIN_FIRST = []\n",
    "OILENG_SALARY_MAX_FIRST = []\n",
    "OILENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-oil-gas-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OILENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OILENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OILENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OILENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OILENG_STATUS_FIRST, OILENG_SALARY_FIRST, OILENG_SALARY_MIN_FIRST, \n",
    "              OILENG_SALARY_MAX_FIRST, OILENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "OILENG_TITLE_SECOND = []\n",
    "OILENG_COMPANY_SECOND = []\n",
    "OILENG_DATE_SECOND = []\n",
    "OILENG_LOCATION_SECOND = []\n",
    "OILENG_STATUS_SECOND = []\n",
    "OILENG_SALARY_SECOND = []\n",
    "OILENG_SALARY_MIN_SECOND = []\n",
    "OILENG_SALARY_MAX_SECOND = []\n",
    "OILENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OILENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OILENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OILENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OILENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OILENG_STATUS_SECOND, OILENG_SALARY_SECOND, OILENG_SALARY_MIN_SECOND, \n",
    "              OILENG_SALARY_MAX_SECOND, OILENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Oil/Gas Engineering (JOBLUM) \n",
    "\n",
    "OILENG_TITLE_LIST = np.concatenate((OILENG_TITLE_FIRST, OILENG_TITLE_SECOND))\n",
    "OILENG_COMPANY_LIST = np.concatenate((OILENG_COMPANY_FIRST, OILENG_COMPANY_SECOND))\n",
    "OILENG_DATE_LIST = np.concatenate((OILENG_DATE_FIRST, OILENG_DATE_SECOND))\n",
    "OILENG_LOCATION_LIST = np.concatenate((OILENG_LOCATION_FIRST, OILENG_LOCATION_SECOND))\n",
    "OILENG_STATUS_LIST = np.concatenate((OILENG_STATUS_FIRST, OILENG_STATUS_SECOND))\n",
    "OILENG_SALARY_LIST = np.concatenate((OILENG_SALARY_FIRST, OILENG_SALARY_SECOND))\n",
    "OILENG_SALARY_MIN_LIST = np.concatenate((OILENG_SALARY_MIN_FIRST, OILENG_SALARY_MIN_SECOND))\n",
    "OILENG_SALARY_MAX_LIST = np.concatenate((OILENG_SALARY_MAX_FIRST, OILENG_SALARY_MAX_SECOND))\n",
    "OILENG_DESCRIPTION_LIST = np.concatenate((OILENG_DESCRIPTION_FIRST, OILENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Oil/Gas Engineering (JOBLUM) \n",
    "OILENG={'Website': \"Joblum\",\n",
    "      'Job Title': OILENG_TITLE_LIST, \n",
    "      'Category': \"Oil/Gas Engineering\", \n",
    "      'Company': OILENG_COMPANY_LIST, \n",
    "      'Date Posted': OILENG_DATE_LIST, \n",
    "      'Location': OILENG_LOCATION_LIST, \n",
    "      'Status': OILENG_STATUS_LIST, \n",
    "      'Salary': OILENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': OILENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': OILENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': OILENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "OILENG_df = pd.DataFrame(data=OILENG)\n",
    "OILENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "OILENG_df.to_csv ('Joblum Data\\JOBLUM-OILENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-cooper",
   "metadata": {},
   "source": [
    "### CATEGORY - Other Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Other Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENG_TITLE_FIRST = []\n",
    "ENG_COMPANY_FIRST = []\n",
    "ENG_DATE_FIRST = []\n",
    "ENG_LOCATION_FIRST = []\n",
    "ENG_STATUS_FIRST = []\n",
    "ENG_SALARY_FIRST = []\n",
    "ENG_SALARY_MIN_FIRST = []\n",
    "ENG_SALARY_MAX_FIRST = []\n",
    "ENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-other-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENG_STATUS_FIRST, ENG_SALARY_FIRST, ENG_SALARY_MIN_FIRST, \n",
    "              ENG_SALARY_MAX_FIRST, ENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Other Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENG_TITLE_SECOND = []\n",
    "ENG_COMPANY_SECOND = []\n",
    "ENG_DATE_SECOND = []\n",
    "ENG_LOCATION_SECOND = []\n",
    "ENG_STATUS_SECOND = []\n",
    "ENG_SALARY_SECOND = []\n",
    "ENG_SALARY_MIN_SECOND = []\n",
    "ENG_SALARY_MAX_SECOND = []\n",
    "ENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENG_STATUS_SECOND, ENG_SALARY_SECOND, ENG_SALARY_MIN_SECOND, \n",
    "              ENG_SALARY_MAX_SECOND, ENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Other Engineering (JOBLUM) \n",
    "\n",
    "ENG_TITLE_LIST = np.concatenate((ENG_TITLE_FIRST, ENG_TITLE_SECOND))\n",
    "ENG_COMPANY_LIST = np.concatenate((ENG_COMPANY_FIRST, ENG_COMPANY_SECOND))\n",
    "ENG_DATE_LIST = np.concatenate((ENG_DATE_FIRST, ENG_DATE_SECOND))\n",
    "ENG_LOCATION_LIST = np.concatenate((ENG_LOCATION_FIRST, ENG_LOCATION_SECOND))\n",
    "ENG_STATUS_LIST = np.concatenate((ENG_STATUS_FIRST, ENG_STATUS_SECOND))\n",
    "ENG_SALARY_LIST = np.concatenate((ENG_SALARY_FIRST, ENG_SALARY_SECOND))\n",
    "ENG_SALARY_MIN_LIST = np.concatenate((ENG_SALARY_MIN_FIRST, ENG_SALARY_MIN_SECOND))\n",
    "ENG_SALARY_MAX_LIST = np.concatenate((ENG_SALARY_MAX_FIRST, ENG_SALARY_MAX_SECOND))\n",
    "ENG_DESCRIPTION_LIST = np.concatenate((ENG_DESCRIPTION_FIRST, ENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Other Engineering (JOBLUM) \n",
    "ENG={'Website': \"Joblum\",\n",
    "      'Job Title': ENG_TITLE_LIST, \n",
    "      'Category': \"Other Engineering\", \n",
    "      'Company': ENG_COMPANY_LIST, \n",
    "      'Date Posted': ENG_DATE_LIST, \n",
    "      'Location': ENG_LOCATION_LIST, \n",
    "      'Status': ENG_STATUS_LIST, \n",
    "      'Salary': ENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ENG_df = pd.DataFrame(data=ENG)\n",
    "ENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_df.to_csv ('Joblum Data\\JOBLUM-ENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-prompt",
   "metadata": {},
   "source": [
    "### CATEGORY - Pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Pharmacy (JOBLUM) - FIRST HALF\n",
    "\n",
    "PHARMA_TITLE_FIRST = []\n",
    "PHARMA_COMPANY_FIRST = []\n",
    "PHARMA_DATE_FIRST = []\n",
    "PHARMA_LOCATION_FIRST = []\n",
    "PHARMA_STATUS_FIRST = []\n",
    "PHARMA_SALARY_FIRST = []\n",
    "PHARMA_SALARY_MIN_FIRST = []\n",
    "PHARMA_SALARY_MAX_FIRST = []\n",
    "PHARMA_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-pharmacy?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PHARMA_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PHARMA_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PHARMA_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PHARMA_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PHARMA_STATUS_FIRST, PHARMA_SALARY_FIRST, PHARMA_SALARY_MIN_FIRST, \n",
    "              PHARMA_SALARY_MAX_FIRST, PHARMA_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Pharmacy (JOBLUM) - SECOND HALF\n",
    "\n",
    "PHARMA_TITLE_SECOND = []\n",
    "PHARMA_COMPANY_SECOND = []\n",
    "PHARMA_DATE_SECOND = []\n",
    "PHARMA_LOCATION_SECOND = []\n",
    "PHARMA_STATUS_SECOND = []\n",
    "PHARMA_SALARY_SECOND = []\n",
    "PHARMA_SALARY_MIN_SECOND = []\n",
    "PHARMA_SALARY_MAX_SECOND = []\n",
    "PHARMA_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PHARMA_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PHARMA_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PHARMA_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PHARMA_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PHARMA_STATUS_SECOND, PHARMA_SALARY_SECOND, PHARMA_SALARY_MIN_SECOND, \n",
    "              PHARMA_SALARY_MAX_SECOND, PHARMA_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Pharmacy (JOBLUM) \n",
    "\n",
    "PHARMA_TITLE_LIST = np.concatenate((PHARMA_TITLE_FIRST, PHARMA_TITLE_SECOND))\n",
    "PHARMA_COMPANY_LIST = np.concatenate((PHARMA_COMPANY_FIRST, PHARMA_COMPANY_SECOND))\n",
    "PHARMA_DATE_LIST = np.concatenate((PHARMA_DATE_FIRST, PHARMA_DATE_SECOND))\n",
    "PHARMA_LOCATION_LIST = np.concatenate((PHARMA_LOCATION_FIRST, PHARMA_LOCATION_SECOND))\n",
    "PHARMA_STATUS_LIST = np.concatenate((PHARMA_STATUS_FIRST, PHARMA_STATUS_SECOND))\n",
    "PHARMA_SALARY_LIST = np.concatenate((PHARMA_SALARY_FIRST, PHARMA_SALARY_SECOND))\n",
    "PHARMA_SALARY_MIN_LIST = np.concatenate((PHARMA_SALARY_MIN_FIRST, PHARMA_SALARY_MIN_SECOND))\n",
    "PHARMA_SALARY_MAX_LIST = np.concatenate((PHARMA_SALARY_MAX_FIRST, PHARMA_SALARY_MAX_SECOND))\n",
    "PHARMA_DESCRIPTION_LIST = np.concatenate((PHARMA_DESCRIPTION_FIRST, PHARMA_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Pharmacy (JOBLUM) \n",
    "PHARMA={'Website': \"Joblum\",\n",
    "      'Job Title': PHARMA_TITLE_LIST, \n",
    "      'Category': \"Pharmacy\", \n",
    "      'Company': PHARMA_COMPANY_LIST, \n",
    "      'Date Posted': PHARMA_DATE_LIST, \n",
    "      'Location': PHARMA_LOCATION_LIST, \n",
    "      'Status': PHARMA_STATUS_LIST, \n",
    "      'Salary': PHARMA_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': PHARMA_DESCRIPTION_LIST,\n",
    "      'Min Salary': PHARMA_SALARY_MIN_LIST,\n",
    "      'Max Salary': PHARMA_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "PHARMA_df = pd.DataFrame(data=PHARMA)\n",
    "PHARMA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHARMA_df.to_csv ('Joblum Data\\JOBLUM-PHARMA.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-characterization",
   "metadata": {},
   "source": [
    "### CATEGORY - Quality Assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quality Assurance (JOBLUM) - FIRST HALF\n",
    "\n",
    "QUALITY_TITLE_FIRST = []\n",
    "QUALITY_COMPANY_FIRST = []\n",
    "QUALITY_DATE_FIRST = []\n",
    "QUALITY_LOCATION_FIRST = []\n",
    "QUALITY_STATUS_FIRST = []\n",
    "QUALITY_SALARY_FIRST = []\n",
    "QUALITY_SALARY_MIN_FIRST = []\n",
    "QUALITY_SALARY_MAX_FIRST = []\n",
    "QUALITY_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-quality-assurance?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUALITY_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUALITY_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUALITY_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUALITY_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUALITY_STATUS_FIRST, QUALITY_SALARY_FIRST, QUALITY_SALARY_MIN_FIRST, \n",
    "              QUALITY_SALARY_MAX_FIRST, QUALITY_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quality Assurance (JOBLUM) - SECOND HALF\n",
    "\n",
    "QUALITY_TITLE_SECOND = []\n",
    "QUALITY_COMPANY_SECOND = []\n",
    "QUALITY_DATE_SECOND = []\n",
    "QUALITY_LOCATION_SECOND = []\n",
    "QUALITY_STATUS_SECOND = []\n",
    "QUALITY_SALARY_SECOND = []\n",
    "QUALITY_SALARY_MIN_SECOND = []\n",
    "QUALITY_SALARY_MAX_SECOND = []\n",
    "QUALITY_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUALITY_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUALITY_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUALITY_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUALITY_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUALITY_STATUS_SECOND, QUALITY_SALARY_SECOND, QUALITY_SALARY_MIN_SECOND, \n",
    "              QUALITY_SALARY_MAX_SECOND, QUALITY_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Quality Assurance (JOBLUM) \n",
    "\n",
    "QUALITY_TITLE_LIST = np.concatenate((QUALITY_TITLE_FIRST, QUALITY_TITLE_SECOND))\n",
    "QUALITY_COMPANY_LIST = np.concatenate((QUALITY_COMPANY_FIRST, QUALITY_COMPANY_SECOND))\n",
    "QUALITY_DATE_LIST = np.concatenate((QUALITY_DATE_FIRST, QUALITY_DATE_SECOND))\n",
    "QUALITY_LOCATION_LIST = np.concatenate((QUALITY_LOCATION_FIRST, QUALITY_LOCATION_SECOND))\n",
    "QUALITY_STATUS_LIST = np.concatenate((QUALITY_STATUS_FIRST, QUALITY_STATUS_SECOND))\n",
    "QUALITY_SALARY_LIST = np.concatenate((QUALITY_SALARY_FIRST, QUALITY_SALARY_SECOND))\n",
    "QUALITY_SALARY_MIN_LIST = np.concatenate((QUALITY_SALARY_MIN_FIRST, QUALITY_SALARY_MIN_SECOND))\n",
    "QUALITY_SALARY_MAX_LIST = np.concatenate((QUALITY_SALARY_MAX_FIRST, QUALITY_SALARY_MAX_SECOND))\n",
    "QUALITY_DESCRIPTION_LIST = np.concatenate((QUALITY_DESCRIPTION_FIRST, QUALITY_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Quality Assurance (JOBLUM) \n",
    "QUALITY={'Website': \"Joblum\",\n",
    "      'Job Title': QUALITY_TITLE_LIST, \n",
    "      'Category': \"Quality Assurance\", \n",
    "      'Company': QUALITY_COMPANY_LIST, \n",
    "      'Date Posted': QUALITY_DATE_LIST, \n",
    "      'Location': QUALITY_LOCATION_LIST, \n",
    "      'Status': QUALITY_STATUS_LIST, \n",
    "      'Salary': QUALITY_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': QUALITY_DESCRIPTION_LIST,\n",
    "      'Min Salary': QUALITY_SALARY_MIN_LIST,\n",
    "      'Max Salary': QUALITY_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "QUALITY_df = pd.DataFrame(data=QUALITY)\n",
    "QUALITY_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUALITY_df.to_csv ('Joblum Data\\JOBLUM-QUALITY.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-sector",
   "metadata": {},
   "source": [
    "### CATEGORY - Quantity Surveying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quantity Surveying (JOBLUM) - FIRST HALF\n",
    "\n",
    "QUANTITY_TITLE_FIRST = []\n",
    "QUANTITY_COMPANY_FIRST = []\n",
    "QUANTITY_DATE_FIRST = []\n",
    "QUANTITY_LOCATION_FIRST = []\n",
    "QUANTITY_STATUS_FIRST = []\n",
    "QUANTITY_SALARY_FIRST = []\n",
    "QUANTITY_SALARY_MIN_FIRST = []\n",
    "QUANTITY_SALARY_MAX_FIRST = []\n",
    "QUANTITY_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-quantity-surveying?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUANTITY_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUANTITY_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUANTITY_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUANTITY_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUANTITY_STATUS_FIRST, QUANTITY_SALARY_FIRST, QUANTITY_SALARY_MIN_FIRST, \n",
    "              QUANTITY_SALARY_MAX_FIRST, QUANTITY_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quantity Surveying (JOBLUM) - SECOND HALF\n",
    "\n",
    "QUANTITY_TITLE_SECOND = []\n",
    "QUANTITY_COMPANY_SECOND = []\n",
    "QUANTITY_DATE_SECOND = []\n",
    "QUANTITY_LOCATION_SECOND = []\n",
    "QUANTITY_STATUS_SECOND = []\n",
    "QUANTITY_SALARY_SECOND = []\n",
    "QUANTITY_SALARY_MIN_SECOND = []\n",
    "QUANTITY_SALARY_MAX_SECOND = []\n",
    "QUANTITY_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUANTITY_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUANTITY_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUANTITY_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUANTITY_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUANTITY_STATUS_SECOND, QUANTITY_SALARY_SECOND, QUANTITY_SALARY_MIN_SECOND, \n",
    "              QUANTITY_SALARY_MAX_SECOND, QUANTITY_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Quantity Surveying (JOBLUM) \n",
    "\n",
    "QUANTITY_TITLE_LIST = np.concatenate((QUANTITY_TITLE_FIRST, QUANTITY_TITLE_SECOND))\n",
    "QUANTITY_COMPANY_LIST = np.concatenate((QUANTITY_COMPANY_FIRST, QUANTITY_COMPANY_SECOND))\n",
    "QUANTITY_DATE_LIST = np.concatenate((QUANTITY_DATE_FIRST, QUANTITY_DATE_SECOND))\n",
    "QUANTITY_LOCATION_LIST = np.concatenate((QUANTITY_LOCATION_FIRST, QUANTITY_LOCATION_SECOND))\n",
    "QUANTITY_STATUS_LIST = np.concatenate((QUANTITY_STATUS_FIRST, QUANTITY_STATUS_SECOND))\n",
    "QUANTITY_SALARY_LIST = np.concatenate((QUANTITY_SALARY_FIRST, QUANTITY_SALARY_SECOND))\n",
    "QUANTITY_SALARY_MIN_LIST = np.concatenate((QUANTITY_SALARY_MIN_FIRST, QUANTITY_SALARY_MIN_SECOND))\n",
    "QUANTITY_SALARY_MAX_LIST = np.concatenate((QUANTITY_SALARY_MAX_FIRST, QUANTITY_SALARY_MAX_SECOND))\n",
    "QUANTITY_DESCRIPTION_LIST = np.concatenate((QUANTITY_DESCRIPTION_FIRST, QUANTITY_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Quantity Surveying (JOBLUM) \n",
    "QUANTITY={'Website': \"Joblum\",\n",
    "      'Job Title': QUANTITY_TITLE_LIST, \n",
    "      'Category': \"Quantity Surveying\", \n",
    "      'Company': QUANTITY_COMPANY_LIST, \n",
    "      'Date Posted': QUANTITY_DATE_LIST, \n",
    "      'Location': QUANTITY_LOCATION_LIST, \n",
    "      'Status': QUANTITY_STATUS_LIST, \n",
    "      'Salary': QUANTITY_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': QUANTITY_DESCRIPTION_LIST,\n",
    "      'Min Salary': QUANTITY_SALARY_MIN_LIST,\n",
    "      'Max Salary': QUANTITY_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "QUANTITY_df = pd.DataFrame(data=QUANTITY)\n",
    "QUANTITY_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTITY_df.to_csv ('Joblum Data\\JOBLUM-QUANTITY.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-correspondence",
   "metadata": {},
   "source": [
    "### CATEGORY - Science & Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Science & Technology (JOBLUM) - FIRST HALF\n",
    "\n",
    "SNT_TITLE_FIRST = []\n",
    "SNT_COMPANY_FIRST = []\n",
    "SNT_DATE_FIRST = []\n",
    "SNT_LOCATION_FIRST = []\n",
    "SNT_STATUS_FIRST = []\n",
    "SNT_SALARY_FIRST = []\n",
    "SNT_SALARY_MIN_FIRST = []\n",
    "SNT_SALARY_MAX_FIRST = []\n",
    "SNT_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-science-amp-technology?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        SNT_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        SNT_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        SNT_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        SNT_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, SNT_STATUS_FIRST, SNT_SALARY_FIRST, SNT_SALARY_MIN_FIRST, \n",
    "              SNT_SALARY_MAX_FIRST, SNT_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Science & Technology (JOBLUM) - SECOND HALF\n",
    "\n",
    "SNT_TITLE_SECOND = []\n",
    "SNT_COMPANY_SECOND = []\n",
    "SNT_DATE_SECOND = []\n",
    "SNT_LOCATION_SECOND = []\n",
    "SNT_STATUS_SECOND = []\n",
    "SNT_SALARY_SECOND = []\n",
    "SNT_SALARY_MIN_SECOND = []\n",
    "SNT_SALARY_MAX_SECOND = []\n",
    "SNT_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        SNT_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        SNT_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        SNT_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        SNT_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, SNT_STATUS_SECOND, SNT_SALARY_SECOND, SNT_SALARY_MIN_SECOND, \n",
    "              SNT_SALARY_MAX_SECOND, SNT_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Science & Technology (JOBLUM) \n",
    "\n",
    "SNT_TITLE_LIST = np.concatenate((SNT_TITLE_FIRST, SNT_TITLE_SECOND))\n",
    "SNT_COMPANY_LIST = np.concatenate((SNT_COMPANY_FIRST, SNT_COMPANY_SECOND))\n",
    "SNT_DATE_LIST = np.concatenate((SNT_DATE_FIRST, SNT_DATE_SECOND))\n",
    "SNT_LOCATION_LIST = np.concatenate((SNT_LOCATION_FIRST, SNT_LOCATION_SECOND))\n",
    "SNT_STATUS_LIST = np.concatenate((SNT_STATUS_FIRST, SNT_STATUS_SECOND))\n",
    "SNT_SALARY_LIST = np.concatenate((SNT_SALARY_FIRST, SNT_SALARY_SECOND))\n",
    "SNT_SALARY_MIN_LIST = np.concatenate((SNT_SALARY_MIN_FIRST, SNT_SALARY_MIN_SECOND))\n",
    "SNT_SALARY_MAX_LIST = np.concatenate((SNT_SALARY_MAX_FIRST, SNT_SALARY_MAX_SECOND))\n",
    "SNT_DESCRIPTION_LIST = np.concatenate((SNT_DESCRIPTION_FIRST, SNT_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Science & Technology (JOBLUM) \n",
    "SNT={'Website': \"Joblum\",\n",
    "      'Job Title': SNT_TITLE_LIST, \n",
    "      'Category': \"Science & Technology\", \n",
    "      'Company': SNT_COMPANY_LIST, \n",
    "      'Date Posted': SNT_DATE_LIST, \n",
    "      'Location': SNT_LOCATION_LIST, \n",
    "      'Status': SNT_STATUS_LIST, \n",
    "      'Salary': SNT_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': SNT_DESCRIPTION_LIST,\n",
    "      'Min Salary': SNT_SALARY_MIN_LIST,\n",
    "      'Max Salary': SNT_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "SNT_df = pd.DataFrame(data=SNT)\n",
    "SNT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNT_df.to_csv ('Joblum Data\\JOBLUM-SNT.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-robert",
   "metadata": {},
   "source": [
    "### CATEGORY - Practitioner/Medical Asst Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Practitioner/Medical Asst Jobs (JOBLUM) - FIRST HALF\n",
    "\n",
    "PRAC_TITLE_FIRST = []\n",
    "PRAC_COMPANY_FIRST = []\n",
    "PRAC_DATE_FIRST = []\n",
    "PRAC_LOCATION_FIRST = []\n",
    "PRAC_STATUS_FIRST = []\n",
    "PRAC_SALARY_FIRST = []\n",
    "PRAC_SALARY_MIN_FIRST = []\n",
    "PRAC_SALARY_MAX_FIRST = []\n",
    "PRAC_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-practitioner-medical-asst?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PRAC_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PRAC_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PRAC_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PRAC_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PRAC_STATUS_FIRST, PRAC_SALARY_FIRST, PRAC_SALARY_MIN_FIRST, \n",
    "              PRAC_SALARY_MAX_FIRST, PRAC_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Practitioner/Medical Asst Jobs (JOBLUM) - SECOND HALF\n",
    "\n",
    "PRAC_TITLE_SECOND = []\n",
    "PRAC_COMPANY_SECOND = []\n",
    "PRAC_DATE_SECOND = []\n",
    "PRAC_LOCATION_SECOND = []\n",
    "PRAC_STATUS_SECOND = []\n",
    "PRAC_SALARY_SECOND = []\n",
    "PRAC_SALARY_MIN_SECOND = []\n",
    "PRAC_SALARY_MAX_SECOND = []\n",
    "PRAC_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PRAC_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PRAC_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PRAC_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PRAC_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PRAC_STATUS_SECOND, PRAC_SALARY_SECOND, PRAC_SALARY_MIN_SECOND, \n",
    "              PRAC_SALARY_MAX_SECOND, PRAC_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Practitioner/Medical Asst Jobs (JOBLUM) \n",
    "\n",
    "PRAC_TITLE_LIST = np.concatenate((PRAC_TITLE_FIRST, PRAC_TITLE_SECOND))\n",
    "PRAC_COMPANY_LIST = np.concatenate((PRAC_COMPANY_FIRST, PRAC_COMPANY_SECOND))\n",
    "PRAC_DATE_LIST = np.concatenate((PRAC_DATE_FIRST, PRAC_DATE_SECOND))\n",
    "PRAC_LOCATION_LIST = np.concatenate((PRAC_LOCATION_FIRST, PRAC_LOCATION_SECOND))\n",
    "PRAC_STATUS_LIST = np.concatenate((PRAC_STATUS_FIRST, PRAC_STATUS_SECOND))\n",
    "PRAC_SALARY_LIST = np.concatenate((PRAC_SALARY_FIRST, PRAC_SALARY_SECOND))\n",
    "PRAC_SALARY_MIN_LIST = np.concatenate((PRAC_SALARY_MIN_FIRST, PRAC_SALARY_MIN_SECOND))\n",
    "PRAC_SALARY_MAX_LIST = np.concatenate((PRAC_SALARY_MAX_FIRST, PRAC_SALARY_MAX_SECOND))\n",
    "PRAC_DESCRIPTION_LIST = np.concatenate((PRAC_DESCRIPTION_FIRST, PRAC_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Practitioner/Medical Asst Jobs (JOBLUM) \n",
    "PRAC={'Website': \"Joblum\",\n",
    "      'Job Title': PRAC_TITLE_LIST, \n",
    "      'Category': \"Practitioner/Medical Asst Jobs\", \n",
    "      'Company': PRAC_COMPANY_LIST, \n",
    "      'Date Posted': PRAC_DATE_LIST, \n",
    "      'Location': PRAC_LOCATION_LIST, \n",
    "      'Status': PRAC_STATUS_LIST, \n",
    "      'Salary': PRAC_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': PRAC_DESCRIPTION_LIST,\n",
    "      'Min Salary': PRAC_SALARY_MIN_LIST,\n",
    "      'Max Salary': PRAC_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "PRAC_df = pd.DataFrame(data=PRAC)\n",
    "PRAC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRAC_df.to_csv ('Joblum Data\\JOBLUM-PRAC.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "vulnerable-position",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STAT_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-ef29aae8b4f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#concatenate all df (JOBLUM)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#The usage of this code is only applicable if the user was able to run the whole notbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m JOBLUM_df = pd.concat([STAT_df, AGRI_df, ARCHI_df, AVI_df, BIOMED_df, BIOTECH_df, \n\u001b[0m\u001b[0;32m      4\u001b[0m                        \u001b[0mCHEMENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCHEM_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIVILENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONSTRUCTION_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIAGNOSIS_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        \u001b[0mDOCTOR_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEC_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECTRO_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECTROENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENVI_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STAT_df' is not defined"
     ]
    }
   ],
   "source": [
    "#concatenate all df (JOBLUM) \n",
    "#The usage of this code is only applicable if the user was able to run the whole notbook\n",
    "JOBLUM_df = pd.concat([STAT_df, AGRI_df, ARCHI_df, AVI_df, BIOMED_df, BIOTECH_df, \n",
    "                       CHEMENG_df, CHEM_df, CIVILENG_df, CONSTRUCTION_df, DIAGNOSIS_df, \n",
    "                       DOCTOR_df, ELEC_df, ELECENG_df, ELECTRO_df, ELECTROENG_df, ENVI_df, \n",
    "                       ENVIENG_df, NUTRI_df, GEO_df, INDUSENG_df, IT_HARDWARE_df, IT_SYS_df, \n",
    "                       IT_SOFTWARE_df, MAINTENANCE_df, MECH_df, MECHENG_df, NURSE_df, OIL_df, \n",
    "                       OILENG_df, ENG_df, PHARMA_df, QUALITY_df, QUANTITY_df, SNT_df, PRAC_df], \n",
    "                      ignore_index=True, sort=False)\n",
    "JOBLUM_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
