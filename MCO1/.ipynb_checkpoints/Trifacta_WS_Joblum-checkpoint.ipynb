{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d574d393-a2e0-48fd-ae9a-1b7ec28df28b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Job Portal - JOBLUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f48b75-8129-488c-8193-018d20cd474b",
   "metadata": {},
   "source": [
    "## Imports used (to be described)\n",
    "\n",
    "* `os` - a module that provides functions to interact with the operating system.\n",
    "* `pandas` - is a tool that helps analyze data.\n",
    "* `numpy` - Library that contains multiple functions that help ease the work with arrays, matrices, and alike to better reassemble data.\n",
    "* `json` - enables import and export from and to JSON files\n",
    "* `re` - Short for Regular Expressions, help recognize patterns on strings of data and is used to orderly reassemble them.\n",
    "* `gensim` - Library that efficiently handles large, unmanaged text collections of data.\n",
    "* `nltk` - Short for Natural Language Toolkit. It helps the program to apply human language data to statistical natural language.\n",
    "* `requests` - Requests allows the program to send HTTP requests easily.\n",
    "* `Seaborn` - A library in python that is used to better visualize data through drawing informative graphs.\n",
    "* `math` - Imported library that allows quick computations of mathematical tasks\n",
    "* `calplot` - \n",
    "* `matplotlib.pyplot` -\n",
    "* `gensim.utils` `simple_preprocess` - used to preprocess text by making them lower-cased, and transforming the words to their original form (de-tokenizing)\n",
    "* `gensim.parsing.preprocessing` `STOPWORDS` - stop words common words that do not have value and are often removed in pre-processing\n",
    "* `gensim` `corpora` - used to work with corpus and words\n",
    "* `gensim` `models` - used for topic modelling and model training\n",
    "* `nltk.stem` `WordNetLemmatizer` - used for grouping similar strings together\n",
    "* `bs4` `BeautifulSoup` - library used to web scrape HTML from websites\n",
    "* `datetime` `datetime` - An imported module in python to create an object that properly resembles date and time. Used for converting string of time into datetime format to month, day, and year.\n",
    "* `datetime` `timedelta` - used for finding delta of time ago with time scraped if date has minutes, hours, days, or weeks ago\n",
    "* `dateutil.relativedelta` `relativedelta` - used for finding delta of time ago with time scraped if date has months and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3e0ce9-0c80-4cfb-b2b7-6f2552f262bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import requests\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import calplot\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a1868-c962-48ab-9bcc-f3dc2a8f41aa",
   "metadata": {},
   "source": [
    "Joblum.com is an online career portal that features relevant and up-to-the-minute job listings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e74d9-8b44-426f-a99f-b66010b21446",
   "metadata": {},
   "source": [
    "### Check number of jobs in soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef9ecb-cc39-40de-a408-2383215625d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get number of jobs per category (JOBLUM)\n",
    "def getNumJobs(soup):\n",
    "    NUM_JOBS = soup.find_all('p',{'class':'no-of-jobs'})\n",
    "    NUM_JOBS =  re.findall(r'(?s)(?<= of ).*?(?= jobs)',str(NUM_JOBS[1]))\n",
    "    if NUM_JOBS:\n",
    "        return int(NUM_JOBS[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083010da-09c5-4541-bdc2-0d2bd3cb259b",
   "metadata": {},
   "source": [
    "### Souptest\n",
    "\n",
    "Getting the html of the URL of the Information Communications Technology job openings, it can be observed that it contains the list of jobs that we interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b79258-2662-4d14-aa47-a2b844d2de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the page and parsing HTML data (JOBLUM)\n",
    "def getSoup(JOBLUM_JOB_URL):\n",
    "    page = requests.get(JOBLUM_JOB_URL)\n",
    "    return BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c6ed9-1bd3-4a53-9036-533d434dc2e5",
   "metadata": {},
   "source": [
    "### Check number of pages in soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca65220-35f3-42f2-a288-3db4ffdcf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number of pages (JOBLUM)\n",
    "def getNumPages(NUM_JOBS):\n",
    "    return math.ceil(NUM_JOBS/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e5625-f93c-43e0-86e3-76949eac6b60",
   "metadata": {},
   "source": [
    "### Get links for each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e8d6f-d921-436c-a23a-32e313913e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the links of each page (JOBLUM)\n",
    "def getLinks(Num_Pages, JOBLUM_URLs):\n",
    "    JOB_LINKS = []\n",
    "    for j in range(1,NUM_PAGES+1):\n",
    "        JOB_LINKS.append(JOBLUM_URLs + str(j))\n",
    "    return JOB_LINKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d485d1d-8a5e-40c7-bb94-3d770b298509",
   "metadata": {},
   "source": [
    "### Get url for each job in each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6539b-cf57-49ad-8a94-6abbd64b66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting the URLs of each job posting (JOBLUM)\n",
    "def getJobURL(JOBLUM_SOUP):\n",
    "    JOBLUM_JOBS_URL = JOBLUM_SOUP.find_all('div',{'class':'mobile-company-logo hidden-md hidden-lg'})\n",
    "    return re.findall(r'(?s)(?<= href=\").*?(?=\" )',str(JOBLUM_JOBS_URL))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f60e8-bc0d-410b-8f06-844ab1d4127c",
   "metadata": {},
   "source": [
    "### Get the job description of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98351962-5e23-41c1-bac1-11ca6eabfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Description (JOBLUM)\n",
    "def getJobDescription(JOB_SOUP):\n",
    "    try:\n",
    "        JOB_INFO = JOB_SOUP.find('span',{'itemprop':'description'})\n",
    "        JOB_INFO_ARRAY = []\n",
    "        for n in range(len(JOB_INFO.contents[1].contents[0])):\n",
    "            if(isinstance(JOB_INFO.contents[1].contents[0].contents[n],NavigableString)):\n",
    "                JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n])\n",
    "            else:\n",
    "                for s in range(len(JOB_INFO.contents[1].contents[0].contents[n])):\n",
    "                    if(isinstance(JOB_INFO.contents[1].contents[0].contents[n].contents[s], Tag)):\n",
    "                        JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n].contents[s].text)\n",
    "                    else:\n",
    "                        JOB_INFO_ARRAY.append(JOB_INFO.contents[1].contents[0].contents[n].contents[s])\n",
    "        JOB_DESCRIPTION = ' '.join(JOB_INFO_ARRAY)\n",
    "        JOB_DESCRIPTION = JOB_DESCRIPTION.replace(\"\\xa0\",\" \")\n",
    "        JOB_DESCRIPTION = re.sub(' +', ' ', JOB_DESCRIPTION) \n",
    "        return JOB_DESCRIPTION\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd4fb4-6141-4a0c-bc97-b628dcaae96c",
   "metadata": {},
   "source": [
    "### Get the job salary of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee952c-081a-447a-ae9f-786beeb4166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Salary (JOBLUM)\n",
    "def getJobSalary(JOB_SOUP):\n",
    "    JOB_INFO = JOB_SOUP.find('span',{'itemprop':'description'})\n",
    "    JOB_SALARY_FINDER = JOB_SOUP.find('p',{'class':'job-subinfo'})\n",
    "    JOB_SALARY_ARRAY = re.findall('[0-9]+,[0-9]+',str(JOB_SALARY_FINDER))\n",
    "    JOB_SALARY = \"\"\n",
    "    JOB_SALARY_MIN = \"Not Specified\"\n",
    "    JOB_SALARY_MAX = \"Not Specified\"\n",
    "    JOB_SALARY_CHECKER = re.findall('(?i)Salary',str(JOB_SALARY_FINDER))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)Salary',str(JOB_INFO))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)PHP',str(JOB_INFO))\n",
    "    if not JOB_SALARY_CHECKER:\n",
    "        JOB_SALARY_CHECKER = re.findall('(?i)pesos',str(JOB_INFO))\n",
    "    if not JOB_SALARY_ARRAY:\n",
    "        JOB_SALARY_ARRAY = re.findall('[0-9]+,[0-9]+',str(JOB_INFO))\n",
    "    if JOB_SALARY_CHECKER:\n",
    "        if (len(JOB_SALARY_ARRAY)==2):\n",
    "            if (int(JOB_SALARY_ARRAY[1].replace(\",\",\"\")) > int(JOB_SALARY_ARRAY[0].replace(\",\",\"\"))):\n",
    "                if (int(JOB_SALARY_ARRAY[1].replace(\",\",\"\")) > 1000):\n",
    "                    JOB_SALARY = '-'.join(JOB_SALARY_ARRAY)\n",
    "                    JOB_SALARY_MIN = JOB_SALARY_ARRAY[0]\n",
    "                    JOB_SALARY_MAX = JOB_SALARY_ARRAY[1]                \n",
    "        elif (len(JOB_SALARY_ARRAY)==1):\n",
    "            JOB_SALARY = JOB_SALARY_ARRAY[0]\n",
    "            JOB_SALARY_MIN = JOB_SALARY_ARRAY[0]\n",
    "            JOB_SALARY_MAX = JOB_SALARY_ARRAY[0]\n",
    "    return JOB_SALARY, JOB_SALARY_MIN, JOB_SALARY_MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e0c14e-8405-449c-bd3b-701221094a9d",
   "metadata": {},
   "source": [
    "### Get the job type of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9ef18-f24c-4ae5-9190-34b0cdf4caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geting Job Type (JOBLUM)\n",
    "def getJobType(JOB_SOUP):\n",
    "    JOB_INFO = JOB_SOUP.find('div',{'class':'col-sm-8 job-main-col'})\n",
    "    if (len(re.findall('(?i)Full-time',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Fulltime',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Full time',str(JOB_INFO)))>0):\n",
    "        return \"Full Time\"\n",
    "    elif (len(re.findall('(?i)Part-time',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    elif (len(re.findall('(?i)Parttime',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    elif (len(re.findall('(?i)Part time',str(JOB_INFO)))>0):\n",
    "        return \"Part Time\"\n",
    "    else:\n",
    "        return \"Not Specified\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940a8ad9-b44d-4940-acea-858812fefa6e",
   "metadata": {},
   "source": [
    "### Web Scraping function based on selected category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc41c5e-ce54-49d1-8a4c-440c7b6c8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of each job posting (JOBLUM)\n",
    "def scrapeJob(URL, JOB_STATUS_LIST, JOB_SALARY_LIST, JOB_SALARY_MIN_LIST, \n",
    "              JOB_SALARY_MAX_LIST, JOB_DESCRIPTION_LIST):\n",
    "    \n",
    "    for m in range(len(URL)):\n",
    "        JOB_SOUP = getSoup('https://ph.joblum.com' + URL[m])\n",
    "        JOB_DESCRIPTION = getJobDescription(JOB_SOUP)\n",
    "        JOB_SALARY, JOB_SALARY_MIN, JOB_SALARY_MAX = getJobSalary(JOB_SOUP)\n",
    "        JOB_STATUS = getJobType(JOB_SOUP)\n",
    "        JOB_STATUS_LIST.append(JOB_STATUS)\n",
    "        JOB_SALARY_LIST.append(JOB_SALARY)\n",
    "        JOB_SALARY_MIN_LIST.append(JOB_SALARY_MIN)\n",
    "        JOB_SALARY_MAX_LIST.append(JOB_SALARY_MAX)\n",
    "        JOB_DESCRIPTION_LIST.append(JOB_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c58a3-ab38-4a5b-93e3-332a57f7b08c",
   "metadata": {},
   "source": [
    "### For Categorizing\n",
    "\n",
    "<a href=\"https://www.onetonline.org/find/stem?t=0\">\n",
    "    onetonline.org\n",
    "</a> and <a href=\"https://www.istemnetwork.org/parents-students/stem-career-opportunities/\">\n",
    "    careerwise.minnstate.edu\n",
    "</a>\n",
    "\n",
    "- Basis for careers from AGRI were classified as Agriculture\n",
    "\n",
    "<a href=\"https://www.bestcolleges.com/careers/stem/\">\n",
    "    bestcolleges.com\n",
    "</a> \n",
    "\n",
    "- Basis for careers from T_HARDWARE, IT_SYS, IT_SOFTWARE  were classified as IT\n",
    "- Basis for careers from ARCHI, AVI, CHEMENG, CIVILENG, CONSTRUCTION, ELEC, ELECENG, ELECTRO, ELECTROENG, ENVIENG, INDUSENG, MAINTENANCE, MECH, MECHENG, NURSE, OIL, OILENG, ENG, QUALITY, QUANTITY were classified as Engineering\n",
    "- Basis for careers from STAT were classified as Mathematics\n",
    "- Basis for careers from BIOMED, BIOTECH, DIAGNOSIS, DOCTOR, PHARMA, and PRAC were classified as Medicine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796dc9e4-3be5-4cf9-8c26-4124bde9b7dc",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic First Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c9664-09d3-4605-a6db-d3b59d4c967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Actuarial/Statistics (JOBLUM) - FIRST HALF\n",
    "\n",
    "STAT_TITLE_FIRST = []\n",
    "STAT_COMPANY_FIRST = []\n",
    "STAT_DATE_FIRST = []\n",
    "STAT_LOCATION_FIRST = []\n",
    "STAT_STATUS_FIRST = []\n",
    "STAT_SALARY_FIRST = []\n",
    "STAT_SALARY_MIN_FIRST = []\n",
    "STAT_SALARY_MAX_FIRST = []\n",
    "STAT_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-actuarial-statistics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        STAT_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        STAT_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        STAT_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        STAT_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, STAT_STATUS_FIRST, STAT_SALARY_FIRST, STAT_SALARY_MIN_FIRST, \n",
    "              STAT_SALARY_MAX_FIRST, STAT_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de96cc5-0280-4846-9bcc-da7379762bf8",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic Second Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43527a9-bd98-48a1-8be5-bbc0ff611c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Actuarial/Statistics (JOBLUM) - SECOND HALF\n",
    "\n",
    "STAT_TITLE_SECOND = []\n",
    "STAT_COMPANY_SECOND = []\n",
    "STAT_DATE_SECOND = []\n",
    "STAT_LOCATION_SECOND = []\n",
    "STAT_STATUS_SECOND = []\n",
    "STAT_SALARY_SECOND = []\n",
    "STAT_SALARY_MIN_SECOND = []\n",
    "STAT_SALARY_MAX_SECOND = []\n",
    "STAT_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        STAT_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        STAT_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        STAT_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        STAT_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, STAT_STATUS_SECOND, STAT_SALARY_SECOND, STAT_SALARY_MIN_SECOND, \n",
    "              STAT_SALARY_MAX_SECOND, STAT_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16e496-c3d5-4c3f-a5ab-4208c6dd0fe7",
   "metadata": {},
   "source": [
    "### CATEGORY - Joined Actuarial/Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7cf3d-b5fa-48d9-933f-463d9ad9873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Actuarial/Statistics (JOBLUM) \n",
    "\n",
    "STAT_TITLE_LIST = np.concatenate((STAT_TITLE_FIRST, STAT_TITLE_SECOND))\n",
    "STAT_COMPANY_LIST = np.concatenate((STAT_COMPANY_FIRST, STAT_COMPANY_SECOND))\n",
    "STAT_DATE_LIST = np.concatenate((STAT_DATE_FIRST, STAT_DATE_SECOND))\n",
    "STAT_LOCATION_LIST = np.concatenate((STAT_LOCATION_FIRST, STAT_LOCATION_SECOND))\n",
    "STAT_STATUS_LIST = np.concatenate((STAT_STATUS_FIRST, STAT_STATUS_SECOND))\n",
    "STAT_SALARY_LIST = np.concatenate((STAT_SALARY_FIRST, STAT_SALARY_SECOND))\n",
    "STAT_SALARY_MIN_LIST = np.concatenate((STAT_SALARY_MIN_FIRST, STAT_SALARY_MIN_SECOND))\n",
    "STAT_SALARY_MAX_LIST = np.concatenate((STAT_SALARY_MAX_FIRST, STAT_SALARY_MAX_SECOND))\n",
    "STAT_DESCRIPTION_LIST = np.concatenate((STAT_DESCRIPTION_FIRST, STAT_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a0672-2f7d-47f2-a7ad-c273048f7097",
   "metadata": {},
   "source": [
    "### CATEGORY - Actuarial/Statistic Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5de792-f618-4a2d-ba77-c24f778ce62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Actuarial/Statistics (JOBLUM) \n",
    "STAT={'Website': \"Joblum\",\n",
    "      'Job Title': STAT_TITLE_LIST, \n",
    "      'Category': \"Actuarial/Statistics\", \n",
    "      'Company': STAT_COMPANY_LIST, \n",
    "      'Date Posted': STAT_DATE_LIST, \n",
    "      'Location': STAT_LOCATION_LIST, \n",
    "      'Status': STAT_STATUS_LIST, \n",
    "      'Salary': STAT_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': STAT_DESCRIPTION_LIST,\n",
    "      'Min Salary': STAT_SALARY_MIN_LIST,\n",
    "      'Max Salary': STAT_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Mathematics\"}\n",
    "STAT_df = pd.DataFrame(data=STAT)\n",
    "STAT_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73b70b-ee0c-4dcf-b2ff-711ab15ddf5b",
   "metadata": {},
   "source": [
    "### Parse data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2450118-746f-4e83-b2f2-657695ff13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAT_df.to_csv ('Joblum Data\\JOBLUM-STAT.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb99d1-f936-4d4b-9877-080ab7577ab2",
   "metadata": {},
   "source": [
    "### CATEGORY - Agriculture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3415a23-6db4-496a-a596-21237972813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Agriculture (JOBLUM) - FIRST HALF\n",
    "\n",
    "AGRI_TITLE_FIRST = []\n",
    "AGRI_COMPANY_FIRST = []\n",
    "AGRI_DATE_FIRST = []\n",
    "AGRI_LOCATION_FIRST = []\n",
    "AGRI_STATUS_FIRST = []\n",
    "AGRI_SALARY_FIRST = []\n",
    "AGRI_SALARY_MIN_FIRST = []\n",
    "AGRI_SALARY_MAX_FIRST = []\n",
    "AGRI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-agriculture?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AGRI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AGRI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AGRI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AGRI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AGRI_STATUS_FIRST, AGRI_SALARY_FIRST, AGRI_SALARY_MIN_FIRST, \n",
    "              AGRI_SALARY_MAX_FIRST, AGRI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd85c5-6859-475b-a0ad-71a586ff7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Agriculture (JOBLUM) - SECOND HALF\n",
    "\n",
    "AGRI_TITLE_SECOND = []\n",
    "AGRI_COMPANY_SECOND = []\n",
    "AGRI_DATE_SECOND = []\n",
    "AGRI_LOCATION_SECOND = []\n",
    "AGRI_STATUS_SECOND = []\n",
    "AGRI_SALARY_SECOND = []\n",
    "AGRI_SALARY_MIN_SECOND = []\n",
    "AGRI_SALARY_MAX_SECOND = []\n",
    "AGRI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AGRI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AGRI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AGRI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AGRI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AGRI_STATUS_SECOND, AGRI_SALARY_SECOND, AGRI_SALARY_MIN_SECOND, \n",
    "              AGRI_SALARY_MAX_SECOND, AGRI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d94a0-d805-443c-b821-19734df8f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Agriculture (JOBLUM) \n",
    "\n",
    "AGRI_TITLE_LIST = np.concatenate((AGRI_TITLE_FIRST, AGRI_TITLE_SECOND))\n",
    "AGRI_COMPANY_LIST = np.concatenate((AGRI_COMPANY_FIRST, AGRI_COMPANY_SECOND))\n",
    "AGRI_DATE_LIST = np.concatenate((AGRI_DATE_FIRST, AGRI_DATE_SECOND))\n",
    "AGRI_LOCATION_LIST = np.concatenate((AGRI_LOCATION_FIRST, AGRI_LOCATION_SECOND))\n",
    "AGRI_STATUS_LIST = np.concatenate((AGRI_STATUS_FIRST, AGRI_STATUS_SECOND))\n",
    "AGRI_SALARY_LIST = np.concatenate((AGRI_SALARY_FIRST, AGRI_SALARY_SECOND))\n",
    "AGRI_SALARY_MIN_LIST = np.concatenate((AGRI_SALARY_MIN_FIRST, AGRI_SALARY_MIN_SECOND))\n",
    "AGRI_SALARY_MAX_LIST = np.concatenate((AGRI_SALARY_MAX_FIRST, AGRI_SALARY_MAX_SECOND))\n",
    "AGRI_DESCRIPTION_LIST = np.concatenate((AGRI_DESCRIPTION_FIRST, AGRI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83516b7b-2692-47db-b33b-6a650522218a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Agriculture (JOBLUM) \n",
    "AGRI={'Website': \"Joblum\",\n",
    "      'Job Title': AGRI_TITLE_LIST, \n",
    "      'Category': \"Agriculture\", \n",
    "      'Company': AGRI_COMPANY_LIST, \n",
    "      'Date Posted': AGRI_DATE_LIST, \n",
    "      'Location': AGRI_LOCATION_LIST, \n",
    "      'Status': AGRI_STATUS_LIST, \n",
    "      'Salary': AGRI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': AGRI_DESCRIPTION_LIST,\n",
    "      'Min Salary': AGRI_SALARY_MIN_LIST,\n",
    "      'Max Salary': AGRI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Agriculture\"}\n",
    "AGRI_df = pd.DataFrame(data=AGRI)\n",
    "AGRI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e1996-e499-4248-b833-003beee78548",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGRI_df.to_csv ('Joblum Data\\JOBLUM-AGRI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2dea81-c136-43e3-9e14-4c3aa305b290",
   "metadata": {},
   "source": [
    "### CATEGORY - Architect/Interior Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4fdbe-553d-4a42-afb0-95da006fb12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Architect/Interior Design (JOBLUM) - FIRST HALF\n",
    "\n",
    "ARCHI_TITLE_FIRST = []\n",
    "ARCHI_COMPANY_FIRST = []\n",
    "ARCHI_DATE_FIRST = []\n",
    "ARCHI_LOCATION_FIRST = []\n",
    "ARCHI_STATUS_FIRST = []\n",
    "ARCHI_SALARY_FIRST = []\n",
    "ARCHI_SALARY_MIN_FIRST = []\n",
    "ARCHI_SALARY_MAX_FIRST = []\n",
    "ARCHI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-architect-interior-design?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ARCHI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ARCHI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ARCHI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ARCHI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ARCHI_STATUS_FIRST, ARCHI_SALARY_FIRST, ARCHI_SALARY_MIN_FIRST, \n",
    "              ARCHI_SALARY_MAX_FIRST, ARCHI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c53ac79-bc42-4de9-92cf-a850d54ac942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Architect/Interior Design (JOBLUM) - SECOND HALF\n",
    "\n",
    "ARCHI_TITLE_SECOND = []\n",
    "ARCHI_COMPANY_SECOND = []\n",
    "ARCHI_DATE_SECOND = []\n",
    "ARCHI_LOCATION_SECOND = []\n",
    "ARCHI_STATUS_SECOND = []\n",
    "ARCHI_SALARY_SECOND = []\n",
    "ARCHI_SALARY_MIN_SECOND = []\n",
    "ARCHI_SALARY_MAX_SECOND = []\n",
    "ARCHI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ARCHI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ARCHI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ARCHI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ARCHI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ARCHI_STATUS_SECOND, ARCHI_SALARY_SECOND, ARCHI_SALARY_MIN_SECOND, \n",
    "              ARCHI_SALARY_MAX_SECOND, ARCHI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431b431-41e8-4af7-9791-5ff267834b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Architect/Interior Design (JOBLUM) \n",
    "\n",
    "ARCHI_TITLE_LIST = np.concatenate((ARCHI_TITLE_FIRST, ARCHI_TITLE_SECOND))\n",
    "ARCHI_COMPANY_LIST = np.concatenate((ARCHI_COMPANY_FIRST, ARCHI_COMPANY_SECOND))\n",
    "ARCHI_DATE_LIST = np.concatenate((ARCHI_DATE_FIRST, ARCHI_DATE_SECOND))\n",
    "ARCHI_LOCATION_LIST = np.concatenate((ARCHI_LOCATION_FIRST, ARCHI_LOCATION_SECOND))\n",
    "ARCHI_STATUS_LIST = np.concatenate((ARCHI_STATUS_FIRST, ARCHI_STATUS_SECOND))\n",
    "ARCHI_SALARY_LIST = np.concatenate((ARCHI_SALARY_FIRST, ARCHI_SALARY_SECOND))\n",
    "ARCHI_SALARY_MIN_LIST = np.concatenate((ARCHI_SALARY_MIN_FIRST, ARCHI_SALARY_MIN_SECOND))\n",
    "ARCHI_SALARY_MAX_LIST = np.concatenate((ARCHI_SALARY_MAX_FIRST, ARCHI_SALARY_MAX_SECOND))\n",
    "ARCHI_DESCRIPTION_LIST = np.concatenate((ARCHI_DESCRIPTION_FIRST, ARCHI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428bcf6-305a-4bfd-9417-64c2f58b88ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Architect/Interior Design (JOBLUM) \n",
    "ARCHI={'Website': \"Joblum\",\n",
    "      'Job Title': ARCHI_TITLE_LIST, \n",
    "      'Category': \"Architect/Interior Design\", \n",
    "      'Company': ARCHI_COMPANY_LIST, \n",
    "      'Date Posted': ARCHI_DATE_LIST, \n",
    "      'Location': ARCHI_LOCATION_LIST, \n",
    "      'Status': ARCHI_STATUS_LIST, \n",
    "      'Salary': ARCHI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ARCHI_DESCRIPTION_LIST,\n",
    "      'Min Salary': ARCHI_SALARY_MIN_LIST,\n",
    "      'Max Salary': ARCHI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ARCHI_df = pd.DataFrame(data=ARCHI)\n",
    "ARCHI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba05f4-98f3-4917-af5c-9e0d7d993f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHI_df.to_csv ('Joblum Data\\JOBLUM-ARCHI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576b6cb-0add-439f-9901-9af9eb0b6f2c",
   "metadata": {},
   "source": [
    "### CATEGORY - Aviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ecc525-3627-49dc-9a96-80c28d9db91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Aviation (JOBLUM) - FIRST HALF\n",
    "\n",
    "AVI_TITLE_FIRST = []\n",
    "AVI_COMPANY_FIRST = []\n",
    "AVI_DATE_FIRST = []\n",
    "AVI_LOCATION_FIRST = []\n",
    "AVI_STATUS_FIRST = []\n",
    "AVI_SALARY_FIRST = []\n",
    "AVI_SALARY_MIN_FIRST = []\n",
    "AVI_SALARY_MAX_FIRST = []\n",
    "AVI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-aviation?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AVI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AVI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AVI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AVI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AVI_STATUS_FIRST, AVI_SALARY_FIRST, AVI_SALARY_MIN_FIRST, \n",
    "              AVI_SALARY_MAX_FIRST, AVI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5933e88-ecc6-4586-9fec-3866cdc5443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Aviation (JOBLUM) - SECOND HALF\n",
    "\n",
    "AVI_TITLE_SECOND = []\n",
    "AVI_COMPANY_SECOND = []\n",
    "AVI_DATE_SECOND = []\n",
    "AVI_LOCATION_SECOND = []\n",
    "AVI_STATUS_SECOND = []\n",
    "AVI_SALARY_SECOND = []\n",
    "AVI_SALARY_MIN_SECOND = []\n",
    "AVI_SALARY_MAX_SECOND = []\n",
    "AVI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        AVI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        AVI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        AVI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        AVI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, AVI_STATUS_SECOND, AVI_SALARY_SECOND, AVI_SALARY_MIN_SECOND, \n",
    "              AVI_SALARY_MAX_SECOND, AVI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15194bf9-bfd1-4953-821f-f60d5b1309e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Aviation (JOBLUM) \n",
    "\n",
    "AVI_TITLE_LIST = np.concatenate((AVI_TITLE_FIRST, AVI_TITLE_SECOND))\n",
    "AVI_COMPANY_LIST = np.concatenate((AVI_COMPANY_FIRST, AVI_COMPANY_SECOND))\n",
    "AVI_DATE_LIST = np.concatenate((AVI_DATE_FIRST, AVI_DATE_SECOND))\n",
    "AVI_LOCATION_LIST = np.concatenate((AVI_LOCATION_FIRST, AVI_LOCATION_SECOND))\n",
    "AVI_STATUS_LIST = np.concatenate((AVI_STATUS_FIRST, AVI_STATUS_SECOND))\n",
    "AVI_SALARY_LIST = np.concatenate((AVI_SALARY_FIRST, AVI_SALARY_SECOND))\n",
    "AVI_SALARY_MIN_LIST = np.concatenate((AVI_SALARY_MIN_FIRST, AVI_SALARY_MIN_SECOND))\n",
    "AVI_SALARY_MAX_LIST = np.concatenate((AVI_SALARY_MAX_FIRST, AVI_SALARY_MAX_SECOND))\n",
    "AVI_DESCRIPTION_LIST = np.concatenate((AVI_DESCRIPTION_FIRST, AVI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c58ad-25f9-451b-b797-de08dab077b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Aviation (JOBLUM) \n",
    "AVI={'Website': \"Joblum\",\n",
    "      'Job Title': AVI_TITLE_LIST, \n",
    "      'Category': \"Aviation\", \n",
    "      'Company': AVI_COMPANY_LIST, \n",
    "      'Date Posted': AVI_DATE_LIST, \n",
    "      'Location': AVI_LOCATION_LIST, \n",
    "      'Status': AVI_STATUS_LIST, \n",
    "      'Salary': AVI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': AVI_DESCRIPTION_LIST,\n",
    "      'Min Salary': AVI_SALARY_MIN_LIST,\n",
    "      'Max Salary': AVI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "AVI_df = pd.DataFrame(data=AVI)\n",
    "AVI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c60c023-6e95-4e84-bbde-d38aeccd567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVI_df.to_csv ('Joblum Data\\JOBLUM-AVI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42168cb5-7de6-46c0-9c3d-5c689af1ae36",
   "metadata": {},
   "source": [
    "### CATEGORY - Biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78c01f-e478-4db7-95db-74d1f654ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biomedical (JOBLUM) - FIRST HALF\n",
    "\n",
    "BIOMED_TITLE_FIRST = []\n",
    "BIOMED_COMPANY_FIRST = []\n",
    "BIOMED_DATE_FIRST = []\n",
    "BIOMED_LOCATION_FIRST = []\n",
    "BIOMED_STATUS_FIRST = []\n",
    "BIOMED_SALARY_FIRST = []\n",
    "BIOMED_SALARY_MIN_FIRST = []\n",
    "BIOMED_SALARY_MAX_FIRST = []\n",
    "BIOMED_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-biomedical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOMED_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOMED_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOMED_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOMED_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOMED_STATUS_FIRST, BIOMED_SALARY_FIRST, BIOMED_SALARY_MIN_FIRST, \n",
    "              BIOMED_SALARY_MAX_FIRST, BIOMED_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a596d6-adfa-492a-bd9a-5a3136425df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biomedical (JOBLUM) - SECOND HALF\n",
    "\n",
    "BIOMED_TITLE_SECOND = []\n",
    "BIOMED_COMPANY_SECOND = []\n",
    "BIOMED_DATE_SECOND = []\n",
    "BIOMED_LOCATION_SECOND = []\n",
    "BIOMED_STATUS_SECOND = []\n",
    "BIOMED_SALARY_SECOND = []\n",
    "BIOMED_SALARY_MIN_SECOND = []\n",
    "BIOMED_SALARY_MAX_SECOND = []\n",
    "BIOMED_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOMED_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOMED_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOMED_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOMED_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOMED_STATUS_SECOND, BIOMED_SALARY_SECOND, BIOMED_SALARY_MIN_SECOND, \n",
    "              BIOMED_SALARY_MAX_SECOND, BIOMED_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959b9a6-5775-41da-b101-8fc31d130115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Biomedical (JOBLUM) \n",
    "\n",
    "BIOMED_TITLE_LIST = np.concatenate((BIOMED_TITLE_FIRST, BIOMED_TITLE_SECOND))\n",
    "BIOMED_COMPANY_LIST = np.concatenate((BIOMED_COMPANY_FIRST, BIOMED_COMPANY_SECOND))\n",
    "BIOMED_DATE_LIST = np.concatenate((BIOMED_DATE_FIRST, BIOMED_DATE_SECOND))\n",
    "BIOMED_LOCATION_LIST = np.concatenate((BIOMED_LOCATION_FIRST, BIOMED_LOCATION_SECOND))\n",
    "BIOMED_STATUS_LIST = np.concatenate((BIOMED_STATUS_FIRST, BIOMED_STATUS_SECOND))\n",
    "BIOMED_SALARY_LIST = np.concatenate((BIOMED_SALARY_FIRST, BIOMED_SALARY_SECOND))\n",
    "BIOMED_SALARY_MIN_LIST = np.concatenate((BIOMED_SALARY_MIN_FIRST, BIOMED_SALARY_MIN_SECOND))\n",
    "BIOMED_SALARY_MAX_LIST = np.concatenate((BIOMED_SALARY_MAX_FIRST, BIOMED_SALARY_MAX_SECOND))\n",
    "BIOMED_DESCRIPTION_LIST = np.concatenate((BIOMED_DESCRIPTION_FIRST, BIOMED_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0131b5-8f47-4afb-9cd8-81483e417c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Biomedical (JOBLUM) \n",
    "BIOMED={'Website': \"Joblum\",\n",
    "      'Job Title': BIOMED_TITLE_LIST, \n",
    "      'Category': \"Biomedical\", \n",
    "      'Company': BIOMED_COMPANY_LIST, \n",
    "      'Date Posted': BIOMED_DATE_LIST, \n",
    "      'Location': BIOMED_LOCATION_LIST, \n",
    "      'Status': BIOMED_STATUS_LIST, \n",
    "      'Salary': BIOMED_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': BIOMED_DESCRIPTION_LIST,\n",
    "      'Min Salary': BIOMED_SALARY_MIN_LIST,\n",
    "      'Max Salary': BIOMED_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "BIOMED_df = pd.DataFrame(data=BIOMED)\n",
    "BIOMED_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eade0b-5884-44f2-b050-b066ee4aeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOMED_df.to_csv ('Joblum Data\\JOBLUM-BIOMED.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f64318-073e-4f10-9e3d-c80d01397222",
   "metadata": {},
   "source": [
    "### CATEGORY - Biotechnology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c519e98-3e70-460b-b708-639f24276964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biotechnology (JOBLUM) - FIRST HALF\n",
    "\n",
    "BIOTECH_TITLE_FIRST = []\n",
    "BIOTECH_COMPANY_FIRST = []\n",
    "BIOTECH_DATE_FIRST = []\n",
    "BIOTECH_LOCATION_FIRST = []\n",
    "BIOTECH_STATUS_FIRST = []\n",
    "BIOTECH_SALARY_FIRST = []\n",
    "BIOTECH_SALARY_MIN_FIRST = []\n",
    "BIOTECH_SALARY_MAX_FIRST = []\n",
    "BIOTECH_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-biotechnology?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOTECH_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOTECH_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOTECH_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOTECH_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOTECH_STATUS_FIRST, BIOTECH_SALARY_FIRST, BIOTECH_SALARY_MIN_FIRST, \n",
    "              BIOTECH_SALARY_MAX_FIRST, BIOTECH_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb183a3-865c-4f8a-ae71-566f24985fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Biotechnology (JOBLUM) - SECOND HALF\n",
    "\n",
    "BIOTECH_TITLE_SECOND = []\n",
    "BIOTECH_COMPANY_SECOND = []\n",
    "BIOTECH_DATE_SECOND = []\n",
    "BIOTECH_LOCATION_SECOND = []\n",
    "BIOTECH_STATUS_SECOND = []\n",
    "BIOTECH_SALARY_SECOND = []\n",
    "BIOTECH_SALARY_MIN_SECOND = []\n",
    "BIOTECH_SALARY_MAX_SECOND = []\n",
    "BIOTECH_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        BIOTECH_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        BIOTECH_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        BIOTECH_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        BIOTECH_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, BIOTECH_STATUS_SECOND, BIOTECH_SALARY_SECOND, BIOTECH_SALARY_MIN_SECOND, \n",
    "              BIOTECH_SALARY_MAX_SECOND, BIOTECH_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128dcd5d-1377-4c50-b2ff-81b3dfc521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Biotechnology (JOBLUM) \n",
    "\n",
    "BIOTECH_TITLE_LIST = np.concatenate((BIOTECH_TITLE_FIRST, BIOTECH_TITLE_SECOND))\n",
    "BIOTECH_COMPANY_LIST = np.concatenate((BIOTECH_COMPANY_FIRST, BIOTECH_COMPANY_SECOND))\n",
    "BIOTECH_DATE_LIST = np.concatenate((BIOTECH_DATE_FIRST, BIOTECH_DATE_SECOND))\n",
    "BIOTECH_LOCATION_LIST = np.concatenate((BIOTECH_LOCATION_FIRST, BIOTECH_LOCATION_SECOND))\n",
    "BIOTECH_STATUS_LIST = np.concatenate((BIOTECH_STATUS_FIRST, BIOTECH_STATUS_SECOND))\n",
    "BIOTECH_SALARY_LIST = np.concatenate((BIOTECH_SALARY_FIRST, BIOTECH_SALARY_SECOND))\n",
    "BIOTECH_SALARY_MIN_LIST = np.concatenate((BIOTECH_SALARY_MIN_FIRST, BIOTECH_SALARY_MIN_SECOND))\n",
    "BIOTECH_SALARY_MAX_LIST = np.concatenate((BIOTECH_SALARY_MAX_FIRST, BIOTECH_SALARY_MAX_SECOND))\n",
    "BIOTECH_DESCRIPTION_LIST = np.concatenate((BIOTECH_DESCRIPTION_FIRST, BIOTECH_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524d2ad-9b38-445a-a219-dc998ac48e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Biotechnology (JOBLUM) \n",
    "BIOTECH={'Website': \"Joblum\",\n",
    "      'Job Title': BIOTECH_TITLE_LIST, \n",
    "      'Category': \"Biotechnology\", \n",
    "      'Company': BIOTECH_COMPANY_LIST, \n",
    "      'Date Posted': BIOTECH_DATE_LIST, \n",
    "      'Location': BIOTECH_LOCATION_LIST, \n",
    "      'Status': BIOTECH_STATUS_LIST, \n",
    "      'Salary': BIOTECH_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': BIOTECH_DESCRIPTION_LIST,\n",
    "      'Min Salary': BIOTECH_SALARY_MIN_LIST,\n",
    "      'Max Salary': BIOTECH_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "BIOTECH_df = pd.DataFrame(data=BIOTECH)\n",
    "BIOTECH_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6feb6-14a1-43a0-a860-3392b8d23ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOTECH_df.to_csv ('Joblum Data\\JOBLUM-BIOTECH.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f16824-e474-4edd-964d-256b126d5ad3",
   "metadata": {},
   "source": [
    "### CATEGORY - Chemical Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2c7b6-f04a-4af1-b432-65e88b5bdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemical Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "CHEMENG_TITLE_FIRST = []\n",
    "CHEMENG_COMPANY_FIRST = []\n",
    "CHEMENG_DATE_FIRST = []\n",
    "CHEMENG_LOCATION_FIRST = []\n",
    "CHEMENG_STATUS_FIRST = []\n",
    "CHEMENG_SALARY_FIRST = []\n",
    "CHEMENG_SALARY_MIN_FIRST = []\n",
    "CHEMENG_SALARY_MAX_FIRST = []\n",
    "CHEMENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-chemical-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEMENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEMENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEMENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEMENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEMENG_STATUS_FIRST, CHEMENG_SALARY_FIRST, CHEMENG_SALARY_MIN_FIRST, \n",
    "              CHEMENG_SALARY_MAX_FIRST, CHEMENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84cd3b5-98bf-4f1b-a437-d8d9ae108d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemical Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "CHEMENG_TITLE_SECOND = []\n",
    "CHEMENG_COMPANY_SECOND = []\n",
    "CHEMENG_DATE_SECOND = []\n",
    "CHEMENG_LOCATION_SECOND = []\n",
    "CHEMENG_STATUS_SECOND = []\n",
    "CHEMENG_SALARY_SECOND = []\n",
    "CHEMENG_SALARY_MIN_SECOND = []\n",
    "CHEMENG_SALARY_MAX_SECOND = []\n",
    "CHEMENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEMENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEMENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEMENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEMENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEMENG_STATUS_SECOND, CHEMENG_SALARY_SECOND, CHEMENG_SALARY_MIN_SECOND, \n",
    "              CHEMENG_SALARY_MAX_SECOND, CHEMENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f215f65-2817-4d20-86f5-362d53439464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Chemical Engineering (JOBLUM) \n",
    "\n",
    "CHEMENG_TITLE_LIST = np.concatenate((CHEMENG_TITLE_FIRST, CHEMENG_TITLE_SECOND))\n",
    "CHEMENG_COMPANY_LIST = np.concatenate((CHEMENG_COMPANY_FIRST, CHEMENG_COMPANY_SECOND))\n",
    "CHEMENG_DATE_LIST = np.concatenate((CHEMENG_DATE_FIRST, CHEMENG_DATE_SECOND))\n",
    "CHEMENG_LOCATION_LIST = np.concatenate((CHEMENG_LOCATION_FIRST, CHEMENG_LOCATION_SECOND))\n",
    "CHEMENG_STATUS_LIST = np.concatenate((CHEMENG_STATUS_FIRST, CHEMENG_STATUS_SECOND))\n",
    "CHEMENG_SALARY_LIST = np.concatenate((CHEMENG_SALARY_FIRST, CHEMENG_SALARY_SECOND))\n",
    "CHEMENG_SALARY_MIN_LIST = np.concatenate((CHEMENG_SALARY_MIN_FIRST, CHEMENG_SALARY_MIN_SECOND))\n",
    "CHEMENG_SALARY_MAX_LIST = np.concatenate((CHEMENG_SALARY_MAX_FIRST, CHEMENG_SALARY_MAX_SECOND))\n",
    "CHEMENG_DESCRIPTION_LIST = np.concatenate((CHEMENG_DESCRIPTION_FIRST, CHEMENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3efb96-9121-4c1f-ad0e-8e79a6fa2888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Chemical Engineering (JOBLUM) \n",
    "CHEMENG={'Website': \"Joblum\",\n",
    "      'Job Title': CHEMENG_TITLE_LIST, \n",
    "      'Category': \"Chemical Engineering\", \n",
    "      'Company': CHEMENG_COMPANY_LIST, \n",
    "      'Date Posted': CHEMENG_DATE_LIST, \n",
    "      'Location': CHEMENG_LOCATION_LIST, \n",
    "      'Status': CHEMENG_STATUS_LIST, \n",
    "      'Salary': CHEMENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CHEMENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': CHEMENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': CHEMENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CHEMENG_df = pd.DataFrame(data=CHEMENG)\n",
    "CHEMENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd0160-2095-4582-ad94-e531ecf5a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEMENG_df.to_csv ('Joblum Data\\JOBLUM-CHEMENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4617f3b-fd38-49b9-a6e3-bb5146e11c7c",
   "metadata": {},
   "source": [
    "### CATEGORY - Chemistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba193e9-a9e1-453a-9eef-51aeb3fa4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemistry (JOBLUM) - FIRST HALF\n",
    "\n",
    "CHEM_TITLE_FIRST = []\n",
    "CHEM_COMPANY_FIRST = []\n",
    "CHEM_DATE_FIRST = []\n",
    "CHEM_LOCATION_FIRST = []\n",
    "CHEM_STATUS_FIRST = []\n",
    "CHEM_SALARY_FIRST = []\n",
    "CHEM_SALARY_MIN_FIRST = []\n",
    "CHEM_SALARY_MAX_FIRST = []\n",
    "CHEM_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-chemistry?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEM_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEM_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEM_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEM_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEM_STATUS_FIRST, CHEM_SALARY_FIRST, CHEM_SALARY_MIN_FIRST, \n",
    "              CHEM_SALARY_MAX_FIRST, CHEM_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f46ae-1a18-4c28-b68c-103c094004dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Chemistry (JOBLUM) - SECOND HALF\n",
    "\n",
    "CHEM_TITLE_SECOND = []\n",
    "CHEM_COMPANY_SECOND = []\n",
    "CHEM_DATE_SECOND = []\n",
    "CHEM_LOCATION_SECOND = []\n",
    "CHEM_STATUS_SECOND = []\n",
    "CHEM_SALARY_SECOND = []\n",
    "CHEM_SALARY_MIN_SECOND = []\n",
    "CHEM_SALARY_MAX_SECOND = []\n",
    "CHEM_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CHEM_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CHEM_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CHEM_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CHEM_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CHEM_STATUS_SECOND, CHEM_SALARY_SECOND, CHEM_SALARY_MIN_SECOND, \n",
    "              CHEM_SALARY_MAX_SECOND, CHEM_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475393e-3e64-4f02-87f2-09fd46c4acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Chemistry (JOBLUM) \n",
    "\n",
    "CHEM_TITLE_LIST = np.concatenate((CHEM_TITLE_FIRST, CHEM_TITLE_SECOND))\n",
    "CHEM_COMPANY_LIST = np.concatenate((CHEM_COMPANY_FIRST, CHEM_COMPANY_SECOND))\n",
    "CHEM_DATE_LIST = np.concatenate((CHEM_DATE_FIRST, CHEM_DATE_SECOND))\n",
    "CHEM_LOCATION_LIST = np.concatenate((CHEM_LOCATION_FIRST, CHEM_LOCATION_SECOND))\n",
    "CHEM_STATUS_LIST = np.concatenate((CHEM_STATUS_FIRST, CHEM_STATUS_SECOND))\n",
    "CHEM_SALARY_LIST = np.concatenate((CHEM_SALARY_FIRST, CHEM_SALARY_SECOND))\n",
    "CHEM_SALARY_MIN_LIST = np.concatenate((CHEM_SALARY_MIN_FIRST, CHEM_SALARY_MIN_SECOND))\n",
    "CHEM_SALARY_MAX_LIST = np.concatenate((CHEM_SALARY_MAX_FIRST, CHEM_SALARY_MAX_SECOND))\n",
    "CHEM_DESCRIPTION_LIST = np.concatenate((CHEM_DESCRIPTION_FIRST, CHEM_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ee0d0-2dc7-4e16-9b1f-7bdb8637a673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Chemistry (JOBLUM) \n",
    "CHEM={'Website': \"Joblum\",\n",
    "      'Job Title': CHEM_TITLE_LIST, \n",
    "      'Category': \"Chemistry\", \n",
    "      'Company': CHEM_COMPANY_LIST, \n",
    "      'Date Posted': CHEM_DATE_LIST, \n",
    "      'Location': CHEM_LOCATION_LIST, \n",
    "      'Status': CHEM_STATUS_LIST, \n",
    "      'Salary': CHEM_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CHEM_DESCRIPTION_LIST,\n",
    "      'Min Salary': CHEM_SALARY_MIN_LIST,\n",
    "      'Max Salary': CHEM_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "CHEM_df = pd.DataFrame(data=CHEM)\n",
    "CHEM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96ed7-de37-4053-bf04-014a7ad074c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHEM_df.to_csv ('Joblum Data\\JOBLUM-CHEM.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450eb09-5d77-4ae9-9b3c-3bdca6e558a9",
   "metadata": {},
   "source": [
    "### CATEGORY - Civil Engineering/Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e64d14-18e5-4eb4-8b4e-81e42579e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil Engineering/Construction (JOBLUM) - FIRST HALF\n",
    "\n",
    "CIVILENG_TITLE_FIRST = []\n",
    "CIVILENG_COMPANY_FIRST = []\n",
    "CIVILENG_DATE_FIRST = []\n",
    "CIVILENG_LOCATION_FIRST = []\n",
    "CIVILENG_STATUS_FIRST = []\n",
    "CIVILENG_SALARY_FIRST = []\n",
    "CIVILENG_SALARY_MIN_FIRST = []\n",
    "CIVILENG_SALARY_MAX_FIRST = []\n",
    "CIVILENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-civil-engineering-construction?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CIVILENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CIVILENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CIVILENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CIVILENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CIVILENG_STATUS_FIRST, CIVILENG_SALARY_FIRST, CIVILENG_SALARY_MIN_FIRST, \n",
    "              CIVILENG_SALARY_MAX_FIRST, CIVILENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f35de-fe43-43a3-a915-c6d95435702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil Engineering/Construction (JOBLUM) - SECOND HALF\n",
    "\n",
    "CIVILENG_TITLE_SECOND = []\n",
    "CIVILENG_COMPANY_SECOND = []\n",
    "CIVILENG_DATE_SECOND = []\n",
    "CIVILENG_LOCATION_SECOND = []\n",
    "CIVILENG_STATUS_SECOND = []\n",
    "CIVILENG_SALARY_SECOND = []\n",
    "CIVILENG_SALARY_MIN_SECOND = []\n",
    "CIVILENG_SALARY_MAX_SECOND = []\n",
    "CIVILENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CIVILENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CIVILENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CIVILENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CIVILENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CIVILENG_STATUS_SECOND, CIVILENG_SALARY_SECOND, CIVILENG_SALARY_MIN_SECOND, \n",
    "              CIVILENG_SALARY_MAX_SECOND, CIVILENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa610f2b-c32d-42a9-8cc0-dbcfa7d0c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Civil Engineering/Construction (JOBLUM) \n",
    "\n",
    "CIVILENG_TITLE_LIST = np.concatenate((CIVILENG_TITLE_FIRST, CIVILENG_TITLE_SECOND))\n",
    "CIVILENG_COMPANY_LIST = np.concatenate((CIVILENG_COMPANY_FIRST, CIVILENG_COMPANY_SECOND))\n",
    "CIVILENG_DATE_LIST = np.concatenate((CIVILENG_DATE_FIRST, CIVILENG_DATE_SECOND))\n",
    "CIVILENG_LOCATION_LIST = np.concatenate((CIVILENG_LOCATION_FIRST, CIVILENG_LOCATION_SECOND))\n",
    "CIVILENG_STATUS_LIST = np.concatenate((CIVILENG_STATUS_FIRST, CIVILENG_STATUS_SECOND))\n",
    "CIVILENG_SALARY_LIST = np.concatenate((CIVILENG_SALARY_FIRST, CIVILENG_SALARY_SECOND))\n",
    "CIVILENG_SALARY_MIN_LIST = np.concatenate((CIVILENG_SALARY_MIN_FIRST, CIVILENG_SALARY_MIN_SECOND))\n",
    "CIVILENG_SALARY_MAX_LIST = np.concatenate((CIVILENG_SALARY_MAX_FIRST, CIVILENG_SALARY_MAX_SECOND))\n",
    "CIVILENG_DESCRIPTION_LIST = np.concatenate((CIVILENG_DESCRIPTION_FIRST, CIVILENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85e5b7-35f4-4ec8-8422-089fdedc236c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Civil Engineering/Construction (JOBLUM) \n",
    "CIVILENG={'Website': \"Joblum\",\n",
    "      'Job Title': CIVILENG_TITLE_LIST, \n",
    "      'Category': \"Civil Engineering/Construction\", \n",
    "      'Company': CIVILENG_COMPANY_LIST, \n",
    "      'Date Posted': CIVILENG_DATE_LIST, \n",
    "      'Location': CIVILENG_LOCATION_LIST, \n",
    "      'Status': CIVILENG_STATUS_LIST, \n",
    "      'Salary': CIVILENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CIVILENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': CIVILENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': CIVILENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CIVILENG_df = pd.DataFrame(data=CIVILENG)\n",
    "CIVILENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cae44-cea4-4b11-b0a5-0a9c57d57ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIVILENG_df.to_csv ('Joblum Data\\JOBLUM-CIVILENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da0735-2305-4ae4-8e50-98fb1daa7334",
   "metadata": {},
   "source": [
    "### CATEGORY - Civil/Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ad726-3e56-4fb1-9cb7-e399132b4562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Scraping data of Civil/Construction (JOBLUM) - FIRST HALF\n",
    "\n",
    "CONSTRUCTION_TITLE_FIRST = []\n",
    "CONSTRUCTION_COMPANY_FIRST = []\n",
    "CONSTRUCTION_DATE_FIRST = []\n",
    "CONSTRUCTION_LOCATION_FIRST = []\n",
    "CONSTRUCTION_STATUS_FIRST = []\n",
    "CONSTRUCTION_SALARY_FIRST = []\n",
    "CONSTRUCTION_SALARY_MIN_FIRST = []\n",
    "CONSTRUCTION_SALARY_MAX_FIRST = []\n",
    "CONSTRUCTION_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-civil-construction?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CONSTRUCTION_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CONSTRUCTION_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CONSTRUCTION_STATUS_FIRST, CONSTRUCTION_SALARY_FIRST, CONSTRUCTION_SALARY_MIN_FIRST, \n",
    "              CONSTRUCTION_SALARY_MAX_FIRST, CONSTRUCTION_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3817602-1eaa-41df-9063-8bdcbaadc02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Civil/Construction (JOBLUM) - SECOND HALF\n",
    "\n",
    "CONSTRUCTION_TITLE_SECOND = []\n",
    "CONSTRUCTION_COMPANY_SECOND = []\n",
    "CONSTRUCTION_DATE_SECOND = []\n",
    "CONSTRUCTION_LOCATION_SECOND = []\n",
    "CONSTRUCTION_STATUS_SECOND = []\n",
    "CONSTRUCTION_SALARY_SECOND = []\n",
    "CONSTRUCTION_SALARY_MIN_SECOND = []\n",
    "CONSTRUCTION_SALARY_MAX_SECOND = []\n",
    "CONSTRUCTION_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        CONSTRUCTION_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        CONSTRUCTION_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        CONSTRUCTION_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, CONSTRUCTION_STATUS_SECOND, CONSTRUCTION_SALARY_SECOND, CONSTRUCTION_SALARY_MIN_SECOND, \n",
    "              CONSTRUCTION_SALARY_MAX_SECOND, CONSTRUCTION_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c58d93-3467-496a-a236-0df7ba9d59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Civil/Construction (JOBLUM) \n",
    "\n",
    "CONSTRUCTION_TITLE_LIST = np.concatenate((CONSTRUCTION_TITLE_FIRST, CONSTRUCTION_TITLE_SECOND))\n",
    "CONSTRUCTION_COMPANY_LIST = np.concatenate((CONSTRUCTION_COMPANY_FIRST, CONSTRUCTION_COMPANY_SECOND))\n",
    "CONSTRUCTION_DATE_LIST = np.concatenate((CONSTRUCTION_DATE_FIRST, CONSTRUCTION_DATE_SECOND))\n",
    "CONSTRUCTION_LOCATION_LIST = np.concatenate((CONSTRUCTION_LOCATION_FIRST, CONSTRUCTION_LOCATION_SECOND))\n",
    "CONSTRUCTION_STATUS_LIST = np.concatenate((CONSTRUCTION_STATUS_FIRST, CONSTRUCTION_STATUS_SECOND))\n",
    "CONSTRUCTION_SALARY_LIST = np.concatenate((CONSTRUCTION_SALARY_FIRST, CONSTRUCTION_SALARY_SECOND))\n",
    "CONSTRUCTION_SALARY_MIN_LIST = np.concatenate((CONSTRUCTION_SALARY_MIN_FIRST, CONSTRUCTION_SALARY_MIN_SECOND))\n",
    "CONSTRUCTION_SALARY_MAX_LIST = np.concatenate((CONSTRUCTION_SALARY_MAX_FIRST, CONSTRUCTION_SALARY_MAX_SECOND))\n",
    "CONSTRUCTION_DESCRIPTION_LIST = np.concatenate((CONSTRUCTION_DESCRIPTION_FIRST, CONSTRUCTION_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f6d90-3e12-4543-b45b-087d09226080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Civil/Construction (JOBLUM) \n",
    "CONSTRUCTION={'Website': \"Joblum\",\n",
    "      'Job Title': CONSTRUCTION_TITLE_LIST, \n",
    "      'Category': \"Civil/Construction\", \n",
    "      'Company': CONSTRUCTION_COMPANY_LIST, \n",
    "      'Date Posted': CONSTRUCTION_DATE_LIST, \n",
    "      'Location': CONSTRUCTION_LOCATION_LIST, \n",
    "      'Status': CONSTRUCTION_STATUS_LIST, \n",
    "      'Salary': CONSTRUCTION_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': CONSTRUCTION_DESCRIPTION_LIST,\n",
    "      'Min Salary': CONSTRUCTION_SALARY_MIN_LIST,\n",
    "      'Max Salary': CONSTRUCTION_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "CONSTRUCTION_df = pd.DataFrame(data=CONSTRUCTION)\n",
    "CONSTRUCTION_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4dac2-e61a-4a97-932e-ca5651292455",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSTRUCTION_df.to_csv ('Joblum Data\\JOBLUM-CONSTRUCTION.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828e3ee-29ae-4df6-9b1b-ea592053102e",
   "metadata": {},
   "source": [
    "### CATEGORY - Diagnosis/Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5888520-8007-4fec-bdc7-6dd803829953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Diagnosis/Others (JOBLUM) - FIRST HALF\n",
    "\n",
    "DIAGNOSIS_TITLE_FIRST = []\n",
    "DIAGNOSIS_COMPANY_FIRST = []\n",
    "DIAGNOSIS_DATE_FIRST = []\n",
    "DIAGNOSIS_LOCATION_FIRST = []\n",
    "DIAGNOSIS_STATUS_FIRST = []\n",
    "DIAGNOSIS_SALARY_FIRST = []\n",
    "DIAGNOSIS_SALARY_MIN_FIRST = []\n",
    "DIAGNOSIS_SALARY_MAX_FIRST = []\n",
    "DIAGNOSIS_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-diagnosis-others?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DIAGNOSIS_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DIAGNOSIS_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DIAGNOSIS_STATUS_FIRST, DIAGNOSIS_SALARY_FIRST, DIAGNOSIS_SALARY_MIN_FIRST, \n",
    "              DIAGNOSIS_SALARY_MAX_FIRST, DIAGNOSIS_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd158bd-c85f-4f24-9810-508296c33733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Diagnosis/Others (JOBLUM) - SECOND HALF\n",
    "\n",
    "DIAGNOSIS_TITLE_SECOND = []\n",
    "DIAGNOSIS_COMPANY_SECOND = []\n",
    "DIAGNOSIS_DATE_SECOND = []\n",
    "DIAGNOSIS_LOCATION_SECOND = []\n",
    "DIAGNOSIS_STATUS_SECOND = []\n",
    "DIAGNOSIS_SALARY_SECOND = []\n",
    "DIAGNOSIS_SALARY_MIN_SECOND = []\n",
    "DIAGNOSIS_SALARY_MAX_SECOND = []\n",
    "DIAGNOSIS_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DIAGNOSIS_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DIAGNOSIS_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DIAGNOSIS_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DIAGNOSIS_STATUS_SECOND, DIAGNOSIS_SALARY_SECOND, DIAGNOSIS_SALARY_MIN_SECOND, \n",
    "              DIAGNOSIS_SALARY_MAX_SECOND, DIAGNOSIS_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1294f5a-3629-4a95-ac8e-1d53307f1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Diagnosis/Others (JOBLUM) \n",
    "\n",
    "DIAGNOSIS_TITLE_LIST = np.concatenate((DIAGNOSIS_TITLE_FIRST, DIAGNOSIS_TITLE_SECOND))\n",
    "DIAGNOSIS_COMPANY_LIST = np.concatenate((DIAGNOSIS_COMPANY_FIRST, DIAGNOSIS_COMPANY_SECOND))\n",
    "DIAGNOSIS_DATE_LIST = np.concatenate((DIAGNOSIS_DATE_FIRST, DIAGNOSIS_DATE_SECOND))\n",
    "DIAGNOSIS_LOCATION_LIST = np.concatenate((DIAGNOSIS_LOCATION_FIRST, DIAGNOSIS_LOCATION_SECOND))\n",
    "DIAGNOSIS_STATUS_LIST = np.concatenate((DIAGNOSIS_STATUS_FIRST, DIAGNOSIS_STATUS_SECOND))\n",
    "DIAGNOSIS_SALARY_LIST = np.concatenate((DIAGNOSIS_SALARY_FIRST, DIAGNOSIS_SALARY_SECOND))\n",
    "DIAGNOSIS_SALARY_MIN_LIST = np.concatenate((DIAGNOSIS_SALARY_MIN_FIRST, DIAGNOSIS_SALARY_MIN_SECOND))\n",
    "DIAGNOSIS_SALARY_MAX_LIST = np.concatenate((DIAGNOSIS_SALARY_MAX_FIRST, DIAGNOSIS_SALARY_MAX_SECOND))\n",
    "DIAGNOSIS_DESCRIPTION_LIST = np.concatenate((DIAGNOSIS_DESCRIPTION_FIRST, DIAGNOSIS_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb467f-66ac-4525-a44e-a9c070919931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating Data Frame for Diagnosis/Others (JOBLUM) \n",
    "DIAGNOSIS={'Website': \"Joblum\",\n",
    "      'Job Title': DIAGNOSIS_TITLE_LIST, \n",
    "      'Category': \"Diagnosis/Others\", \n",
    "      'Company': DIAGNOSIS_COMPANY_LIST, \n",
    "      'Date Posted': DIAGNOSIS_DATE_LIST, \n",
    "      'Location': DIAGNOSIS_LOCATION_LIST, \n",
    "      'Status': DIAGNOSIS_STATUS_LIST, \n",
    "      'Salary': DIAGNOSIS_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': DIAGNOSIS_DESCRIPTION_LIST,\n",
    "      'Min Salary': DIAGNOSIS_SALARY_MIN_LIST,\n",
    "      'Max Salary': DIAGNOSIS_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "DIAGNOSIS_df = pd.DataFrame(data=DIAGNOSIS)\n",
    "DIAGNOSIS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d508e5-410c-40f5-bae0-1b65bc5e6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAGNOSIS_df.to_csv ('Joblum Data\\JOBLUM-DIAGNOSIS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed75f2d-4c3d-44bc-a105-4780aa5e7775",
   "metadata": {},
   "source": [
    "### CATEGORY - Doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c234fc-9c9a-42a3-ae5c-3ab0a815a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Doctor/Diagnosis (JOBLUM) - FIRST HALF\n",
    "\n",
    "DOCTOR_TITLE_FIRST = []\n",
    "DOCTOR_COMPANY_FIRST = []\n",
    "DOCTOR_DATE_FIRST = []\n",
    "DOCTOR_LOCATION_FIRST = []\n",
    "DOCTOR_STATUS_FIRST = []\n",
    "DOCTOR_SALARY_FIRST = []\n",
    "DOCTOR_SALARY_MIN_FIRST = []\n",
    "DOCTOR_SALARY_MAX_FIRST = []\n",
    "DOCTOR_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-doctor-diagnosis?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DOCTOR_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DOCTOR_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DOCTOR_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DOCTOR_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DOCTOR_STATUS_FIRST, DOCTOR_SALARY_FIRST, DOCTOR_SALARY_MIN_FIRST, \n",
    "              DOCTOR_SALARY_MAX_FIRST, DOCTOR_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e2612-b715-449f-bf17-3f3cdf9790e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Doctor/Diagnosis (JOBLUM) - SECOND HALF\n",
    "\n",
    "DOCTOR_TITLE_SECOND = []\n",
    "DOCTOR_COMPANY_SECOND = []\n",
    "DOCTOR_DATE_SECOND = []\n",
    "DOCTOR_LOCATION_SECOND = []\n",
    "DOCTOR_STATUS_SECOND = []\n",
    "DOCTOR_SALARY_SECOND = []\n",
    "DOCTOR_SALARY_MIN_SECOND = []\n",
    "DOCTOR_SALARY_MAX_SECOND = []\n",
    "DOCTOR_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        DOCTOR_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        DOCTOR_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        DOCTOR_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        DOCTOR_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, DOCTOR_STATUS_SECOND, DOCTOR_SALARY_SECOND, DOCTOR_SALARY_MIN_SECOND, \n",
    "              DOCTOR_SALARY_MAX_SECOND, DOCTOR_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e53a97-a4c2-47fb-9844-d1e36fe52469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Doctor/Diagnosis (JOBLUM) \n",
    "\n",
    "DOCTOR_TITLE_LIST = np.concatenate((DOCTOR_TITLE_FIRST, DOCTOR_TITLE_SECOND))\n",
    "DOCTOR_COMPANY_LIST = np.concatenate((DOCTOR_COMPANY_FIRST, DOCTOR_COMPANY_SECOND))\n",
    "DOCTOR_DATE_LIST = np.concatenate((DOCTOR_DATE_FIRST, DOCTOR_DATE_SECOND))\n",
    "DOCTOR_LOCATION_LIST = np.concatenate((DOCTOR_LOCATION_FIRST, DOCTOR_LOCATION_SECOND))\n",
    "DOCTOR_STATUS_LIST = np.concatenate((DOCTOR_STATUS_FIRST, DOCTOR_STATUS_SECOND))\n",
    "DOCTOR_SALARY_LIST = np.concatenate((DOCTOR_SALARY_FIRST, DOCTOR_SALARY_SECOND))\n",
    "DOCTOR_SALARY_MIN_LIST = np.concatenate((DOCTOR_SALARY_MIN_FIRST, DOCTOR_SALARY_MIN_SECOND))\n",
    "DOCTOR_SALARY_MAX_LIST = np.concatenate((DOCTOR_SALARY_MAX_FIRST, DOCTOR_SALARY_MAX_SECOND))\n",
    "DOCTOR_DESCRIPTION_LIST = np.concatenate((DOCTOR_DESCRIPTION_FIRST, DOCTOR_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc19e8d-acfc-46a2-badb-5e5fad2fe835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Doctor/Diagnosis (JOBLUM) \n",
    "DOCTOR={'Website': \"Joblum\",\n",
    "      'Job Title': DOCTOR_TITLE_LIST, \n",
    "      'Category': \"Doctor/DOCTOR\", \n",
    "      'Company': DOCTOR_COMPANY_LIST, \n",
    "      'Date Posted': DOCTOR_DATE_LIST, \n",
    "      'Location': DOCTOR_LOCATION_LIST, \n",
    "      'Status': DOCTOR_STATUS_LIST, \n",
    "      'Salary': DOCTOR_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': DOCTOR_DESCRIPTION_LIST,\n",
    "      'Min Salary': DOCTOR_SALARY_MIN_LIST,\n",
    "      'Max Salary': DOCTOR_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "DOCTOR_df = pd.DataFrame(data=DOCTOR)\n",
    "DOCTOR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2021a-d8e8-46de-a71e-5cd8d6c10808",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCTOR_df.to_csv ('Joblum Data\\JOBLUM-DOCTOR.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ec1db-1369-41da-be83-ca964f198565",
   "metadata": {},
   "source": [
    "### CATEGORY - Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736eec1a-3b30-48d1-ad50-5c6a7cef235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELEC_TITLE_FIRST = []\n",
    "ELEC_COMPANY_FIRST = []\n",
    "ELEC_DATE_FIRST = []\n",
    "ELEC_LOCATION_FIRST = []\n",
    "ELEC_STATUS_FIRST = []\n",
    "ELEC_SALARY_FIRST = []\n",
    "ELEC_SALARY_MIN_FIRST = []\n",
    "ELEC_SALARY_MAX_FIRST = []\n",
    "ELEC_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electrical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELEC_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELEC_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELEC_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELEC_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELEC_STATUS_FIRST, ELEC_SALARY_FIRST, ELEC_SALARY_MIN_FIRST, \n",
    "              ELEC_SALARY_MAX_FIRST, ELEC_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb79a5d-1d4d-4afa-a34e-bc4bd179f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELEC_TITLE_SECOND = []\n",
    "ELEC_COMPANY_SECOND = []\n",
    "ELEC_DATE_SECOND = []\n",
    "ELEC_LOCATION_SECOND = []\n",
    "ELEC_STATUS_SECOND = []\n",
    "ELEC_SALARY_SECOND = []\n",
    "ELEC_SALARY_MIN_SECOND = []\n",
    "ELEC_SALARY_MAX_SECOND = []\n",
    "ELEC_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELEC_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELEC_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELEC_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELEC_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELEC_STATUS_SECOND, ELEC_SALARY_SECOND, ELEC_SALARY_MIN_SECOND, \n",
    "              ELEC_SALARY_MAX_SECOND, ELEC_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11e21b-cfb3-476b-8f5d-d2016b088d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electrical (JOBLUM) \n",
    "\n",
    "ELEC_TITLE_LIST = np.concatenate((ELEC_TITLE_FIRST, ELEC_TITLE_SECOND))\n",
    "ELEC_COMPANY_LIST = np.concatenate((ELEC_COMPANY_FIRST, ELEC_COMPANY_SECOND))\n",
    "ELEC_DATE_LIST = np.concatenate((ELEC_DATE_FIRST, ELEC_DATE_SECOND))\n",
    "ELEC_LOCATION_LIST = np.concatenate((ELEC_LOCATION_FIRST, ELEC_LOCATION_SECOND))\n",
    "ELEC_STATUS_LIST = np.concatenate((ELEC_STATUS_FIRST, ELEC_STATUS_SECOND))\n",
    "ELEC_SALARY_LIST = np.concatenate((ELEC_SALARY_FIRST, ELEC_SALARY_SECOND))\n",
    "ELEC_SALARY_MIN_LIST = np.concatenate((ELEC_SALARY_MIN_FIRST, ELEC_SALARY_MIN_SECOND))\n",
    "ELEC_SALARY_MAX_LIST = np.concatenate((ELEC_SALARY_MAX_FIRST, ELEC_SALARY_MAX_SECOND))\n",
    "ELEC_DESCRIPTION_LIST = np.concatenate((ELEC_DESCRIPTION_FIRST, ELEC_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fa0ce-4930-4300-869b-a955a3e36236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electrical (JOBLUM) \n",
    "ELEC={'Website': \"Joblum\",\n",
    "      'Job Title': ELEC_TITLE_LIST, \n",
    "      'Category': \"Electrical\", \n",
    "      'Company': ELEC_COMPANY_LIST, \n",
    "      'Date Posted': ELEC_DATE_LIST, \n",
    "      'Location': ELEC_LOCATION_LIST, \n",
    "      'Status': ELEC_STATUS_LIST, \n",
    "      'Salary': ELEC_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELEC_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELEC_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELEC_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELEC_df = pd.DataFrame(data=ELEC)\n",
    "ELEC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e63501-69b8-4162-921f-c8f61820b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEC_df.to_csv ('Joblum Data\\JOBLUM-ELEC.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117a441-497f-4e1f-b43b-b002014ca1b0",
   "metadata": {},
   "source": [
    "### CATEGORY - Electrical Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6597653-5360-4443-a0a6-6d7f60bfbaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECENG_TITLE_FIRST = []\n",
    "ELECENG_COMPANY_FIRST = []\n",
    "ELECENG_DATE_FIRST = []\n",
    "ELECENG_LOCATION_FIRST = []\n",
    "ELECENG_STATUS_FIRST = []\n",
    "ELECENG_SALARY_FIRST = []\n",
    "ELECENG_SALARY_MIN_FIRST = []\n",
    "ELECENG_SALARY_MAX_FIRST = []\n",
    "ELECENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electrical-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECENG_STATUS_FIRST, ELECENG_SALARY_FIRST, ELECENG_SALARY_MIN_FIRST, \n",
    "              ELECENG_SALARY_MAX_FIRST, ELECENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99f45c-4dab-480d-a11f-d2f71070e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electrical Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECENG_TITLE_SECOND = []\n",
    "ELECENG_COMPANY_SECOND = []\n",
    "ELECENG_DATE_SECOND = []\n",
    "ELECENG_LOCATION_SECOND = []\n",
    "ELECENG_STATUS_SECOND = []\n",
    "ELECENG_SALARY_SECOND = []\n",
    "ELECENG_SALARY_MIN_SECOND = []\n",
    "ELECENG_SALARY_MAX_SECOND = []\n",
    "ELECENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECENG_STATUS_SECOND, ELECENG_SALARY_SECOND, ELECENG_SALARY_MIN_SECOND, \n",
    "              ELECENG_SALARY_MAX_SECOND, ELECENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d13021-50be-44d1-9b82-ece69c50a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electrical Engineering (JOBLUM) \n",
    "\n",
    "ELECENG_TITLE_LIST = np.concatenate((ELECENG_TITLE_FIRST, ELECENG_TITLE_SECOND))\n",
    "ELECENG_COMPANY_LIST = np.concatenate((ELECENG_COMPANY_FIRST, ELECENG_COMPANY_SECOND))\n",
    "ELECENG_DATE_LIST = np.concatenate((ELECENG_DATE_FIRST, ELECENG_DATE_SECOND))\n",
    "ELECENG_LOCATION_LIST = np.concatenate((ELECENG_LOCATION_FIRST, ELECENG_LOCATION_SECOND))\n",
    "ELECENG_STATUS_LIST = np.concatenate((ELECENG_STATUS_FIRST, ELECENG_STATUS_SECOND))\n",
    "ELECENG_SALARY_LIST = np.concatenate((ELECENG_SALARY_FIRST, ELECENG_SALARY_SECOND))\n",
    "ELECENG_SALARY_MIN_LIST = np.concatenate((ELECENG_SALARY_MIN_FIRST, ELECENG_SALARY_MIN_SECOND))\n",
    "ELECENG_SALARY_MAX_LIST = np.concatenate((ELECENG_SALARY_MAX_FIRST, ELECENG_SALARY_MAX_SECOND))\n",
    "ELECENG_DESCRIPTION_LIST = np.concatenate((ELECENG_DESCRIPTION_FIRST, ELECENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fc91c-a025-4689-86dc-7ff8d162d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electrical Engineering (JOBLUM) \n",
    "ELECENG={'Website': \"Joblum\",\n",
    "      'Job Title': ELECENG_TITLE_LIST, \n",
    "      'Category': \"Electrical Engineering\", \n",
    "      'Company': ELECENG_COMPANY_LIST, \n",
    "      'Date Posted': ELECENG_DATE_LIST, \n",
    "      'Location': ELECENG_LOCATION_LIST, \n",
    "      'Status': ELECENG_STATUS_LIST, \n",
    "      'Salary': ELECENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECENG_df = pd.DataFrame(data=ELECENG)\n",
    "ELECENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00388535-e7b1-4bf3-9466-dc2f33874fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELECENG_df.to_csv ('Joblum Data\\JOBLUM-ELECENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e98fb8-0e9b-4927-84e5-e67342afa4c5",
   "metadata": {},
   "source": [
    "### CATEGORY - Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b8d4f-18db-40f7-bf46-bef10cbca28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECTRO_TITLE_FIRST = []\n",
    "ELECTRO_COMPANY_FIRST = []\n",
    "ELECTRO_DATE_FIRST = []\n",
    "ELECTRO_LOCATION_FIRST = []\n",
    "ELECTRO_STATUS_FIRST = []\n",
    "ELECTRO_SALARY_FIRST = []\n",
    "ELECTRO_SALARY_MIN_FIRST = []\n",
    "ELECTRO_SALARY_MAX_FIRST = []\n",
    "ELECTRO_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electronics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTRO_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTRO_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTRO_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTRO_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTRO_STATUS_FIRST, ELECTRO_SALARY_FIRST, ELECTRO_SALARY_MIN_FIRST, \n",
    "              ELECTRO_SALARY_MAX_FIRST, ELECTRO_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6eda43-fa05-4e5b-8e3f-065e2939f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECTRO_TITLE_SECOND = []\n",
    "ELECTRO_COMPANY_SECOND = []\n",
    "ELECTRO_DATE_SECOND = []\n",
    "ELECTRO_LOCATION_SECOND = []\n",
    "ELECTRO_STATUS_SECOND = []\n",
    "ELECTRO_SALARY_SECOND = []\n",
    "ELECTRO_SALARY_MIN_SECOND = []\n",
    "ELECTRO_SALARY_MAX_SECOND = []\n",
    "ELECTRO_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTRO_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTRO_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTRO_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTRO_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTRO_STATUS_SECOND, ELECTRO_SALARY_SECOND, ELECTRO_SALARY_MIN_SECOND, \n",
    "              ELECTRO_SALARY_MAX_SECOND, ELECTRO_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f89fce-ff1d-415d-8126-11faed2413ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electronics (JOBLUM) \n",
    "\n",
    "ELECTRO_TITLE_LIST = np.concatenate((ELECTRO_TITLE_FIRST, ELECTRO_TITLE_SECOND))\n",
    "ELECTRO_COMPANY_LIST = np.concatenate((ELECTRO_COMPANY_FIRST, ELECTRO_COMPANY_SECOND))\n",
    "ELECTRO_DATE_LIST = np.concatenate((ELECTRO_DATE_FIRST, ELECTRO_DATE_SECOND))\n",
    "ELECTRO_LOCATION_LIST = np.concatenate((ELECTRO_LOCATION_FIRST, ELECTRO_LOCATION_SECOND))\n",
    "ELECTRO_STATUS_LIST = np.concatenate((ELECTRO_STATUS_FIRST, ELECTRO_STATUS_SECOND))\n",
    "ELECTRO_SALARY_LIST = np.concatenate((ELECTRO_SALARY_FIRST, ELECTRO_SALARY_SECOND))\n",
    "ELECTRO_SALARY_MIN_LIST = np.concatenate((ELECTRO_SALARY_MIN_FIRST, ELECTRO_SALARY_MIN_SECOND))\n",
    "ELECTRO_SALARY_MAX_LIST = np.concatenate((ELECTRO_SALARY_MAX_FIRST, ELECTRO_SALARY_MAX_SECOND))\n",
    "ELECTRO_DESCRIPTION_LIST = np.concatenate((ELECTRO_DESCRIPTION_FIRST, ELECTRO_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73167e4a-ad99-411c-bc79-5417e8bcdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electronics (JOBLUM) \n",
    "ELECTRO={'Website': \"Joblum\",\n",
    "      'Job Title': ELECTRO_TITLE_LIST, \n",
    "      'Category': \"Electronics\", \n",
    "      'Company': ELECTRO_COMPANY_LIST, \n",
    "      'Date Posted': ELECTRO_DATE_LIST, \n",
    "      'Location': ELECTRO_LOCATION_LIST, \n",
    "      'Status': ELECTRO_STATUS_LIST, \n",
    "      'Salary': ELECTRO_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECTRO_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECTRO_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECTRO_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECTRO_df = pd.DataFrame(data=ELECTRO)\n",
    "ELECTRO_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabac50e-b26c-4e2b-808f-fcb5896d5161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ELECTRO_df.to_csv ('Joblum Data\\JOBLUM-ELECTRO.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40910d24-e66c-4e20-a309-362aac509c98",
   "metadata": {},
   "source": [
    "### CATEGORY - Electronics Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca7918-18f8-4aba-bbde-f58edad91afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ELECTROENG_TITLE_FIRST = []\n",
    "ELECTROENG_COMPANY_FIRST = []\n",
    "ELECTROENG_DATE_FIRST = []\n",
    "ELECTROENG_LOCATION_FIRST = []\n",
    "ELECTROENG_STATUS_FIRST = []\n",
    "ELECTROENG_SALARY_FIRST = []\n",
    "ELECTROENG_SALARY_MIN_FIRST = []\n",
    "ELECTROENG_SALARY_MAX_FIRST = []\n",
    "ELECTROENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-electronics-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTROENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTROENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTROENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTROENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTROENG_STATUS_FIRST, ELECTROENG_SALARY_FIRST, ELECTROENG_SALARY_MIN_FIRST, \n",
    "              ELECTROENG_SALARY_MAX_FIRST, ELECTROENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c45a06-accf-4c55-967f-afb8e24554a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Electronics Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ELECTROENG_TITLE_SECOND = []\n",
    "ELECTROENG_COMPANY_SECOND = []\n",
    "ELECTROENG_DATE_SECOND = []\n",
    "ELECTROENG_LOCATION_SECOND = []\n",
    "ELECTROENG_STATUS_SECOND = []\n",
    "ELECTROENG_SALARY_SECOND = []\n",
    "ELECTROENG_SALARY_MIN_SECOND = []\n",
    "ELECTROENG_SALARY_MAX_SECOND = []\n",
    "ELECTROENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ELECTROENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ELECTROENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ELECTROENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ELECTROENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ELECTROENG_STATUS_SECOND, ELECTROENG_SALARY_SECOND, ELECTROENG_SALARY_MIN_SECOND, \n",
    "              ELECTROENG_SALARY_MAX_SECOND, ELECTROENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9b402-1592-4272-a520-30d02a31e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Electronics Engineering (JOBLUM) \n",
    "\n",
    "ELECTROENG_TITLE_LIST = np.concatenate((ELECTROENG_TITLE_FIRST, ELECTROENG_TITLE_SECOND))\n",
    "ELECTROENG_COMPANY_LIST = np.concatenate((ELECTROENG_COMPANY_FIRST, ELECTROENG_COMPANY_SECOND))\n",
    "ELECTROENG_DATE_LIST = np.concatenate((ELECTROENG_DATE_FIRST, ELECTROENG_DATE_SECOND))\n",
    "ELECTROENG_LOCATION_LIST = np.concatenate((ELECTROENG_LOCATION_FIRST, ELECTROENG_LOCATION_SECOND))\n",
    "ELECTROENG_STATUS_LIST = np.concatenate((ELECTROENG_STATUS_FIRST, ELECTROENG_STATUS_SECOND))\n",
    "ELECTROENG_SALARY_LIST = np.concatenate((ELECTROENG_SALARY_FIRST, ELECTROENG_SALARY_SECOND))\n",
    "ELECTROENG_SALARY_MIN_LIST = np.concatenate((ELECTROENG_SALARY_MIN_FIRST, ELECTROENG_SALARY_MIN_SECOND))\n",
    "ELECTROENG_SALARY_MAX_LIST = np.concatenate((ELECTROENG_SALARY_MAX_FIRST, ELECTROENG_SALARY_MAX_SECOND))\n",
    "ELECTROENG_DESCRIPTION_LIST = np.concatenate((ELECTROENG_DESCRIPTION_FIRST, ELECTROENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f727b81-4260-4ad9-a702-c6d45d2215bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Electronics Engineering (JOBLUM) \n",
    "ELECTROENG={'Website': \"Joblum\",\n",
    "      'Job Title': ELECTROENG_TITLE_LIST, \n",
    "      'Category': \"Electronics Engineering\", \n",
    "      'Company': ELECTROENG_COMPANY_LIST, \n",
    "      'Date Posted': ELECTROENG_DATE_LIST, \n",
    "      'Location': ELECTROENG_LOCATION_LIST, \n",
    "      'Status': ELECTROENG_STATUS_LIST, \n",
    "      'Salary': ELECTROENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ELECTROENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ELECTROENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ELECTROENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ELECTROENG_df = pd.DataFrame(data=ELECTROENG)\n",
    "ELECTROENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35556278-a64c-4bc0-bbb0-ddc44cca93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELECTROENG_df.to_csv ('Joblum Data\\JOBLUM-ELECTROENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223752b-ca86-438c-a46c-dec61be49405",
   "metadata": {},
   "source": [
    "### CATEGORY - Environmental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3676a-2503-41bc-8679-0283e729cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENVI_TITLE_FIRST = []\n",
    "ENVI_COMPANY_FIRST = []\n",
    "ENVI_DATE_FIRST = []\n",
    "ENVI_LOCATION_FIRST = []\n",
    "ENVI_STATUS_FIRST = []\n",
    "ENVI_SALARY_FIRST = []\n",
    "ENVI_SALARY_MIN_FIRST = []\n",
    "ENVI_SALARY_MAX_FIRST = []\n",
    "ENVI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-environmental?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVI_STATUS_FIRST, ENVI_SALARY_FIRST, ENVI_SALARY_MIN_FIRST, \n",
    "              ENVI_SALARY_MAX_FIRST, ENVI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b372d19-1342-409f-bb85-1813cea62334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENVI_TITLE_SECOND = []\n",
    "ENVI_COMPANY_SECOND = []\n",
    "ENVI_DATE_SECOND = []\n",
    "ENVI_LOCATION_SECOND = []\n",
    "ENVI_STATUS_SECOND = []\n",
    "ENVI_SALARY_SECOND = []\n",
    "ENVI_SALARY_MIN_SECOND = []\n",
    "ENVI_SALARY_MAX_SECOND = []\n",
    "ENVI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVI_STATUS_SECOND, ENVI_SALARY_SECOND, ENVI_SALARY_MIN_SECOND, \n",
    "              ENVI_SALARY_MAX_SECOND, ENVI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d654e-ab9b-4830-90f9-4fb678a1c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Environmental (JOBLUM) \n",
    "\n",
    "ENVI_TITLE_LIST = np.concatenate((ENVI_TITLE_FIRST, ENVI_TITLE_SECOND))\n",
    "ENVI_COMPANY_LIST = np.concatenate((ENVI_COMPANY_FIRST, ENVI_COMPANY_SECOND))\n",
    "ENVI_DATE_LIST = np.concatenate((ENVI_DATE_FIRST, ENVI_DATE_SECOND))\n",
    "ENVI_LOCATION_LIST = np.concatenate((ENVI_LOCATION_FIRST, ENVI_LOCATION_SECOND))\n",
    "ENVI_STATUS_LIST = np.concatenate((ENVI_STATUS_FIRST, ENVI_STATUS_SECOND))\n",
    "ENVI_SALARY_LIST = np.concatenate((ENVI_SALARY_FIRST, ENVI_SALARY_SECOND))\n",
    "ENVI_SALARY_MIN_LIST = np.concatenate((ENVI_SALARY_MIN_FIRST, ENVI_SALARY_MIN_SECOND))\n",
    "ENVI_SALARY_MAX_LIST = np.concatenate((ENVI_SALARY_MAX_FIRST, ENVI_SALARY_MAX_SECOND))\n",
    "ENVI_DESCRIPTION_LIST = np.concatenate((ENVI_DESCRIPTION_FIRST, ENVI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665fe8f-0c45-4a50-9b69-2edb59b5f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Environmental (JOBLUM) \n",
    "ENVI={'Website': \"Joblum\",\n",
    "      'Job Title': ENVI_TITLE_LIST, \n",
    "      'Category': \"Environmental\", \n",
    "      'Company': ENVI_COMPANY_LIST, \n",
    "      'Date Posted': ENVI_DATE_LIST, \n",
    "      'Location': ENVI_LOCATION_LIST, \n",
    "      'Status': ENVI_STATUS_LIST, \n",
    "      'Salary': ENVI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENVI_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENVI_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENVI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "ENVI_df = pd.DataFrame(data=ENVI)\n",
    "ENVI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f019e2-6945-4aba-94a2-a63ff53597cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVI_df.to_csv ('Joblum Data\\JOBLUM-ENVI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49a668-2d59-4b17-b428-c9652b1950ae",
   "metadata": {},
   "source": [
    "### CATEGORY - Environmental Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b4415-46ed-4e70-ba7c-fcc709b0c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENVIENG_TITLE_FIRST = []\n",
    "ENVIENG_COMPANY_FIRST = []\n",
    "ENVIENG_DATE_FIRST = []\n",
    "ENVIENG_LOCATION_FIRST = []\n",
    "ENVIENG_STATUS_FIRST = []\n",
    "ENVIENG_SALARY_FIRST = []\n",
    "ENVIENG_SALARY_MIN_FIRST = []\n",
    "ENVIENG_SALARY_MAX_FIRST = []\n",
    "ENVIENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-environmental-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVIENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVIENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVIENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVIENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVIENG_STATUS_FIRST, ENVIENG_SALARY_FIRST, ENVIENG_SALARY_MIN_FIRST, \n",
    "              ENVIENG_SALARY_MAX_FIRST, ENVIENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5d63f-59c5-40f5-a925-bbbedb57efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Environmental Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENVIENG_TITLE_SECOND = []\n",
    "ENVIENG_COMPANY_SECOND = []\n",
    "ENVIENG_DATE_SECOND = []\n",
    "ENVIENG_LOCATION_SECOND = []\n",
    "ENVIENG_STATUS_SECOND = []\n",
    "ENVIENG_SALARY_SECOND = []\n",
    "ENVIENG_SALARY_MIN_SECOND = []\n",
    "ENVIENG_SALARY_MAX_SECOND = []\n",
    "ENVIENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENVIENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENVIENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENVIENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENVIENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENVIENG_STATUS_SECOND, ENVIENG_SALARY_SECOND, ENVIENG_SALARY_MIN_SECOND, \n",
    "              ENVIENG_SALARY_MAX_SECOND, ENVIENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d623d-5854-4055-bd21-ba41bc105c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Environmental Engineering (JOBLUM) \n",
    "\n",
    "ENVIENG_TITLE_LIST = np.concatenate((ENVIENG_TITLE_FIRST, ENVIENG_TITLE_SECOND))\n",
    "ENVIENG_COMPANY_LIST = np.concatenate((ENVIENG_COMPANY_FIRST, ENVIENG_COMPANY_SECOND))\n",
    "ENVIENG_DATE_LIST = np.concatenate((ENVIENG_DATE_FIRST, ENVIENG_DATE_SECOND))\n",
    "ENVIENG_LOCATION_LIST = np.concatenate((ENVIENG_LOCATION_FIRST, ENVIENG_LOCATION_SECOND))\n",
    "ENVIENG_STATUS_LIST = np.concatenate((ENVIENG_STATUS_FIRST, ENVIENG_STATUS_SECOND))\n",
    "ENVIENG_SALARY_LIST = np.concatenate((ENVIENG_SALARY_FIRST, ENVIENG_SALARY_SECOND))\n",
    "ENVIENG_SALARY_MIN_LIST = np.concatenate((ENVIENG_SALARY_MIN_FIRST, ENVIENG_SALARY_MIN_SECOND))\n",
    "ENVIENG_SALARY_MAX_LIST = np.concatenate((ENVIENG_SALARY_MAX_FIRST, ENVIENG_SALARY_MAX_SECOND))\n",
    "ENVIENG_DESCRIPTION_LIST = np.concatenate((ENVIENG_DESCRIPTION_FIRST, ENVIENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c274de3-86b3-48c3-8cec-7a912713d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Environmental Engineering (JOBLUM) \n",
    "ENVIENG={'Website': \"Joblum\",\n",
    "      'Job Title': ENVIENG_TITLE_LIST, \n",
    "      'Category': \"Environmental Engineering\", \n",
    "      'Company': ENVIENG_COMPANY_LIST, \n",
    "      'Date Posted': ENVIENG_DATE_LIST, \n",
    "      'Location': ENVIENG_LOCATION_LIST, \n",
    "      'Status': ENVIENG_STATUS_LIST, \n",
    "      'Salary': ENVIENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENVIENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENVIENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENVIENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ENVIENG_df = pd.DataFrame(data=ENVIENG)\n",
    "ENVIENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a33caf-6bf1-4213-ab5f-a9a382474839",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIENG_df.to_csv ('Joblum Data\\JOBLUM-ENVIENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb88c6d-d2e3-4ca2-a8f0-316410c66d53",
   "metadata": {},
   "source": [
    "### CATEGORY - Food Tech/Nutritionist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb8e9f-77d2-47fc-8345-4a2aef910e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Food Tech/Nutritionist (JOBLUM) - FIRST HALF\n",
    "\n",
    "NUTRI_TITLE_FIRST = []\n",
    "NUTRI_COMPANY_FIRST = []\n",
    "NUTRI_DATE_FIRST = []\n",
    "NUTRI_LOCATION_FIRST = []\n",
    "NUTRI_STATUS_FIRST = []\n",
    "NUTRI_SALARY_FIRST = []\n",
    "NUTRI_SALARY_MIN_FIRST = []\n",
    "NUTRI_SALARY_MAX_FIRST = []\n",
    "NUTRI_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-food-tech-nutritionist?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NUTRI_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NUTRI_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NUTRI_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NUTRI_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NUTRI_STATUS_FIRST, NUTRI_SALARY_FIRST, NUTRI_SALARY_MIN_FIRST, \n",
    "              NUTRI_SALARY_MAX_FIRST, NUTRI_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1ea49-0026-4a5e-b255-b9255143a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Food Tech/Nutritionist (JOBLUM) - SECOND HALF\n",
    "\n",
    "NUTRI_TITLE_SECOND = []\n",
    "NUTRI_COMPANY_SECOND = []\n",
    "NUTRI_DATE_SECOND = []\n",
    "NUTRI_LOCATION_SECOND = []\n",
    "NUTRI_STATUS_SECOND = []\n",
    "NUTRI_SALARY_SECOND = []\n",
    "NUTRI_SALARY_MIN_SECOND = []\n",
    "NUTRI_SALARY_MAX_SECOND = []\n",
    "NUTRI_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NUTRI_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NUTRI_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NUTRI_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NUTRI_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NUTRI_STATUS_SECOND, NUTRI_SALARY_SECOND, NUTRI_SALARY_MIN_SECOND, \n",
    "              NUTRI_SALARY_MAX_SECOND, NUTRI_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0677e65-adaa-4409-bcea-6db7a9902334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Food Tech/Nutritionist (JOBLUM) \n",
    "\n",
    "NUTRI_TITLE_LIST = np.concatenate((NUTRI_TITLE_FIRST, NUTRI_TITLE_SECOND))\n",
    "NUTRI_COMPANY_LIST = np.concatenate((NUTRI_COMPANY_FIRST, NUTRI_COMPANY_SECOND))\n",
    "NUTRI_DATE_LIST = np.concatenate((NUTRI_DATE_FIRST, NUTRI_DATE_SECOND))\n",
    "NUTRI_LOCATION_LIST = np.concatenate((NUTRI_LOCATION_FIRST, NUTRI_LOCATION_SECOND))\n",
    "NUTRI_STATUS_LIST = np.concatenate((NUTRI_STATUS_FIRST, NUTRI_STATUS_SECOND))\n",
    "NUTRI_SALARY_LIST = np.concatenate((NUTRI_SALARY_FIRST, NUTRI_SALARY_SECOND))\n",
    "NUTRI_SALARY_MIN_LIST = np.concatenate((NUTRI_SALARY_MIN_FIRST, NUTRI_SALARY_MIN_SECOND))\n",
    "NUTRI_SALARY_MAX_LIST = np.concatenate((NUTRI_SALARY_MAX_FIRST, NUTRI_SALARY_MAX_SECOND))\n",
    "NUTRI_DESCRIPTION_LIST = np.concatenate((NUTRI_DESCRIPTION_FIRST, NUTRI_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbf04c-3b07-4e31-92a2-49eccb20d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Food Tech/Nutritionist (JOBLUM) \n",
    "NUTRI={'Website': \"Joblum\",\n",
    "      'Job Title': NUTRI_TITLE_LIST, \n",
    "      'Category': \"Food Tech/Nutritionist\", \n",
    "      'Company': NUTRI_COMPANY_LIST, \n",
    "      'Date Posted': NUTRI_DATE_LIST, \n",
    "      'Location': NUTRI_LOCATION_LIST, \n",
    "      'Status': NUTRI_STATUS_LIST, \n",
    "      'Salary': NUTRI_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': NUTRI_DESCRIPTION_LIST,\n",
    "      'Min Salary': NUTRI_SALARY_MIN_LIST,\n",
    "      'Max Salary': NUTRI_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "NUTRI_df = pd.DataFrame(data=NUTRI)\n",
    "NUTRI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d5512-a279-4853-9fee-d09767a1e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUTRI_df.to_csv ('Joblum Data\\JOBLUM-NUTRI.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877390d4-3ad7-4cbd-a68b-38b6da308e4f",
   "metadata": {},
   "source": [
    "### CATEGORY - Geology/Geophysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75971ede-ab4b-40b4-bd2b-855b244625f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Geology/Geophysics (JOBLUM) - FIRST HALF\n",
    "\n",
    "GEO_TITLE_FIRST = []\n",
    "GEO_COMPANY_FIRST = []\n",
    "GEO_DATE_FIRST = []\n",
    "GEO_LOCATION_FIRST = []\n",
    "GEO_STATUS_FIRST = []\n",
    "GEO_SALARY_FIRST = []\n",
    "GEO_SALARY_MIN_FIRST = []\n",
    "GEO_SALARY_MAX_FIRST = []\n",
    "GEO_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-geology-geophysics?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        GEO_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        GEO_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        GEO_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        GEO_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, GEO_STATUS_FIRST, GEO_SALARY_FIRST, GEO_SALARY_MIN_FIRST, \n",
    "              GEO_SALARY_MAX_FIRST, GEO_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095d521-5a17-49cb-857f-5239d75edd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Geology/Geophysics (JOBLUM) - SECOND HALF\n",
    "\n",
    "GEO_TITLE_SECOND = []\n",
    "GEO_COMPANY_SECOND = []\n",
    "GEO_DATE_SECOND = []\n",
    "GEO_LOCATION_SECOND = []\n",
    "GEO_STATUS_SECOND = []\n",
    "GEO_SALARY_SECOND = []\n",
    "GEO_SALARY_MIN_SECOND = []\n",
    "GEO_SALARY_MAX_SECOND = []\n",
    "GEO_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        GEO_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        GEO_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        GEO_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        GEO_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, GEO_STATUS_SECOND, GEO_SALARY_SECOND, GEO_SALARY_MIN_SECOND, \n",
    "              GEO_SALARY_MAX_SECOND, GEO_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccadd6-e4b0-456a-9c76-0559c4f27882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Geology/Geophysics (JOBLUM) \n",
    "\n",
    "GEO_TITLE_LIST = np.concatenate((GEO_TITLE_FIRST, GEO_TITLE_SECOND))\n",
    "GEO_COMPANY_LIST = np.concatenate((GEO_COMPANY_FIRST, GEO_COMPANY_SECOND))\n",
    "GEO_DATE_LIST = np.concatenate((GEO_DATE_FIRST, GEO_DATE_SECOND))\n",
    "GEO_LOCATION_LIST = np.concatenate((GEO_LOCATION_FIRST, GEO_LOCATION_SECOND))\n",
    "GEO_STATUS_LIST = np.concatenate((GEO_STATUS_FIRST, GEO_STATUS_SECOND))\n",
    "GEO_SALARY_LIST = np.concatenate((GEO_SALARY_FIRST, GEO_SALARY_SECOND))\n",
    "GEO_SALARY_MIN_LIST = np.concatenate((GEO_SALARY_MIN_FIRST, GEO_SALARY_MIN_SECOND))\n",
    "GEO_SALARY_MAX_LIST = np.concatenate((GEO_SALARY_MAX_FIRST, GEO_SALARY_MAX_SECOND))\n",
    "GEO_DESCRIPTION_LIST = np.concatenate((GEO_DESCRIPTION_FIRST, GEO_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa530908-695f-47c1-9921-8688c4fbd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Geology/Geophysics (JOBLUM) \n",
    "GEO={'Website': \"Joblum\",\n",
    "      'Job Title': GEO_TITLE_LIST, \n",
    "      'Category': \"Geology/Geophysics\", \n",
    "      'Company': GEO_COMPANY_LIST, \n",
    "      'Date Posted': GEO_DATE_LIST, \n",
    "      'Location': GEO_LOCATION_LIST, \n",
    "      'Status': GEO_STATUS_LIST, \n",
    "      'Salary': GEO_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': GEO_DESCRIPTION_LIST,\n",
    "      'Min Salary': GEO_SALARY_MIN_LIST,\n",
    "      'Max Salary': GEO_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "GEO_df = pd.DataFrame(data=GEO)\n",
    "GEO_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de666c29-d21b-4106-9626-77b9b838b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEO_df.to_csv ('Joblum Data\\JOBLUM-GEO.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f78fc-114b-4bf3-9766-f3fa493feecc",
   "metadata": {},
   "source": [
    "### CATEGORY - Industrial Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad41db-a898-4021-9e2f-97672e516833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Industrial Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "INDUSENG_TITLE_FIRST = []\n",
    "INDUSENG_COMPANY_FIRST = []\n",
    "INDUSENG_DATE_FIRST = []\n",
    "INDUSENG_LOCATION_FIRST = []\n",
    "INDUSENG_STATUS_FIRST = []\n",
    "INDUSENG_SALARY_FIRST = []\n",
    "INDUSENG_SALARY_MIN_FIRST = []\n",
    "INDUSENG_SALARY_MAX_FIRST = []\n",
    "INDUSENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-industrial-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        INDUSENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        INDUSENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        INDUSENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        INDUSENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, INDUSENG_STATUS_FIRST, INDUSENG_SALARY_FIRST, INDUSENG_SALARY_MIN_FIRST, \n",
    "              INDUSENG_SALARY_MAX_FIRST, INDUSENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b96d9-668b-4f3a-9052-fc6d0054024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Industrial Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "INDUSENG_TITLE_SECOND = []\n",
    "INDUSENG_COMPANY_SECOND = []\n",
    "INDUSENG_DATE_SECOND = []\n",
    "INDUSENG_LOCATION_SECOND = []\n",
    "INDUSENG_STATUS_SECOND = []\n",
    "INDUSENG_SALARY_SECOND = []\n",
    "INDUSENG_SALARY_MIN_SECOND = []\n",
    "INDUSENG_SALARY_MAX_SECOND = []\n",
    "INDUSENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        INDUSENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        INDUSENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        INDUSENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        INDUSENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, INDUSENG_STATUS_SECOND, INDUSENG_SALARY_SECOND, INDUSENG_SALARY_MIN_SECOND, \n",
    "              INDUSENG_SALARY_MAX_SECOND, INDUSENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51f3c5-dd44-48a0-85be-69f83f2f5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Industrial Engineering (JOBLUM) \n",
    "\n",
    "INDUSENG_TITLE_LIST = np.concatenate((INDUSENG_TITLE_FIRST, INDUSENG_TITLE_SECOND))\n",
    "INDUSENG_COMPANY_LIST = np.concatenate((INDUSENG_COMPANY_FIRST, INDUSENG_COMPANY_SECOND))\n",
    "INDUSENG_DATE_LIST = np.concatenate((INDUSENG_DATE_FIRST, INDUSENG_DATE_SECOND))\n",
    "INDUSENG_LOCATION_LIST = np.concatenate((INDUSENG_LOCATION_FIRST, INDUSENG_LOCATION_SECOND))\n",
    "INDUSENG_STATUS_LIST = np.concatenate((INDUSENG_STATUS_FIRST, INDUSENG_STATUS_SECOND))\n",
    "INDUSENG_SALARY_LIST = np.concatenate((INDUSENG_SALARY_FIRST, INDUSENG_SALARY_SECOND))\n",
    "INDUSENG_SALARY_MIN_LIST = np.concatenate((INDUSENG_SALARY_MIN_FIRST, INDUSENG_SALARY_MIN_SECOND))\n",
    "INDUSENG_SALARY_MAX_LIST = np.concatenate((INDUSENG_SALARY_MAX_FIRST, INDUSENG_SALARY_MAX_SECOND))\n",
    "INDUSENG_DESCRIPTION_LIST = np.concatenate((INDUSENG_DESCRIPTION_FIRST, INDUSENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c5021-a4fa-46bc-99d7-414c65fc3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Industrial Engineering (JOBLUM) \n",
    "INDUSENG={'Website': \"Joblum\",\n",
    "      'Job Title': INDUSENG_TITLE_LIST, \n",
    "      'Category': \"Industrial Engineering\", \n",
    "      'Company': INDUSENG_COMPANY_LIST, \n",
    "      'Date Posted': INDUSENG_DATE_LIST, \n",
    "      'Location': INDUSENG_LOCATION_LIST, \n",
    "      'Status': INDUSENG_STATUS_LIST, \n",
    "      'Salary': INDUSENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': INDUSENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': INDUSENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': INDUSENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "INDUSENG_df = pd.DataFrame(data=INDUSENG)\n",
    "INDUSENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755fa3c-d549-4252-8a35-60769f547b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDUSENG_df.to_csv ('Joblum Data\\JOBLUM-INDUSENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76234f39-6a72-4f4d-854c-a60b5641dee5",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de16d7-6377-4158-b37b-b60e21563f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Hardware (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_HARDWARE_TITLE_FIRST = []\n",
    "IT_HARDWARE_COMPANY_FIRST = []\n",
    "IT_HARDWARE_DATE_FIRST = []\n",
    "IT_HARDWARE_LOCATION_FIRST = []\n",
    "IT_HARDWARE_STATUS_FIRST = []\n",
    "IT_HARDWARE_SALARY_FIRST = []\n",
    "IT_HARDWARE_SALARY_MIN_FIRST = []\n",
    "IT_HARDWARE_SALARY_MAX_FIRST = []\n",
    "IT_HARDWARE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-hardware?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_HARDWARE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_HARDWARE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_HARDWARE_STATUS_FIRST, IT_HARDWARE_SALARY_FIRST, IT_HARDWARE_SALARY_MIN_FIRST, \n",
    "              IT_HARDWARE_SALARY_MAX_FIRST, IT_HARDWARE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cb79d-b40f-4161-8fb8-bd359511df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Hardware (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_HARDWARE_TITLE_SECOND = []\n",
    "IT_HARDWARE_COMPANY_SECOND = []\n",
    "IT_HARDWARE_DATE_SECOND = []\n",
    "IT_HARDWARE_LOCATION_SECOND = []\n",
    "IT_HARDWARE_STATUS_SECOND = []\n",
    "IT_HARDWARE_SALARY_SECOND = []\n",
    "IT_HARDWARE_SALARY_MIN_SECOND = []\n",
    "IT_HARDWARE_SALARY_MAX_SECOND = []\n",
    "IT_HARDWARE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_HARDWARE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_HARDWARE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_HARDWARE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_HARDWARE_STATUS_SECOND, IT_HARDWARE_SALARY_SECOND, IT_HARDWARE_SALARY_MIN_SECOND, \n",
    "              IT_HARDWARE_SALARY_MAX_SECOND, IT_HARDWARE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb6b60-2b77-4bbc-b47e-bae0149f13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Hardware (JOBLUM) \n",
    "\n",
    "IT_HARDWARE_TITLE_LIST = np.concatenate((IT_HARDWARE_TITLE_FIRST, IT_HARDWARE_TITLE_SECOND))\n",
    "IT_HARDWARE_COMPANY_LIST = np.concatenate((IT_HARDWARE_COMPANY_FIRST, IT_HARDWARE_COMPANY_SECOND))\n",
    "IT_HARDWARE_DATE_LIST = np.concatenate((IT_HARDWARE_DATE_FIRST, IT_HARDWARE_DATE_SECOND))\n",
    "IT_HARDWARE_LOCATION_LIST = np.concatenate((IT_HARDWARE_LOCATION_FIRST, IT_HARDWARE_LOCATION_SECOND))\n",
    "IT_HARDWARE_STATUS_LIST = np.concatenate((IT_HARDWARE_STATUS_FIRST, IT_HARDWARE_STATUS_SECOND))\n",
    "IT_HARDWARE_SALARY_LIST = np.concatenate((IT_HARDWARE_SALARY_FIRST, IT_HARDWARE_SALARY_SECOND))\n",
    "IT_HARDWARE_SALARY_MIN_LIST = np.concatenate((IT_HARDWARE_SALARY_MIN_FIRST, IT_HARDWARE_SALARY_MIN_SECOND))\n",
    "IT_HARDWARE_SALARY_MAX_LIST = np.concatenate((IT_HARDWARE_SALARY_MAX_FIRST, IT_HARDWARE_SALARY_MAX_SECOND))\n",
    "IT_HARDWARE_DESCRIPTION_LIST = np.concatenate((IT_HARDWARE_DESCRIPTION_FIRST, IT_HARDWARE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cbd3d-50b7-41eb-828c-022c2103310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Hardware (JOBLUM) \n",
    "IT_HARDWARE={'Website': \"Joblum\",\n",
    "      'Job Title': IT_HARDWARE_TITLE_LIST, \n",
    "      'Category': \"IT - Hardware\", \n",
    "      'Company': IT_HARDWARE_COMPANY_LIST, \n",
    "      'Date Posted': IT_HARDWARE_DATE_LIST, \n",
    "      'Location': IT_HARDWARE_LOCATION_LIST, \n",
    "      'Status': IT_HARDWARE_STATUS_LIST, \n",
    "      'Salary': IT_HARDWARE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_HARDWARE_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_HARDWARE_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_HARDWARE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_HARDWARE_df = pd.DataFrame(data=IT_HARDWARE)\n",
    "IT_HARDWARE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c141036-9e1c-40e6-bc25-3229b94ac1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_HARDWARE_df.to_csv ('Joblum Data\\JOBLUM-IT_HARDWARE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c9734-13bc-41b5-84c8-185e2547bf66",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Network/Sys/DB Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52279f9-4751-4896-952b-b95ac460c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Network/Sys/DB Admin (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_SYS_TITLE_FIRST = []\n",
    "IT_SYS_COMPANY_FIRST = []\n",
    "IT_SYS_DATE_FIRST = []\n",
    "IT_SYS_LOCATION_FIRST = []\n",
    "IT_SYS_STATUS_FIRST = []\n",
    "IT_SYS_SALARY_FIRST = []\n",
    "IT_SYS_SALARY_MIN_FIRST = []\n",
    "IT_SYS_SALARY_MAX_FIRST = []\n",
    "IT_SYS_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-network-sys-db-admin?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SYS_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SYS_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SYS_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SYS_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SYS_STATUS_FIRST, IT_SYS_SALARY_FIRST, IT_SYS_SALARY_MIN_FIRST, \n",
    "              IT_SYS_SALARY_MAX_FIRST, IT_SYS_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab79095-5df9-44a1-b6f8-faf732e2deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Network/Sys/DB Admin (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_SYS_TITLE_SECOND = []\n",
    "IT_SYS_COMPANY_SECOND = []\n",
    "IT_SYS_DATE_SECOND = []\n",
    "IT_SYS_LOCATION_SECOND = []\n",
    "IT_SYS_STATUS_SECOND = []\n",
    "IT_SYS_SALARY_SECOND = []\n",
    "IT_SYS_SALARY_MIN_SECOND = []\n",
    "IT_SYS_SALARY_MAX_SECOND = []\n",
    "IT_SYS_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SYS_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SYS_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SYS_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SYS_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SYS_STATUS_SECOND, IT_SYS_SALARY_SECOND, IT_SYS_SALARY_MIN_SECOND, \n",
    "              IT_SYS_SALARY_MAX_SECOND, IT_SYS_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77b11c-a390-4360-bd2d-1d3333f59a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Network/Sys/DB Admin (JOBLUM) \n",
    "\n",
    "IT_SYS_TITLE_LIST = np.concatenate((IT_SYS_TITLE_FIRST, IT_SYS_TITLE_SECOND))\n",
    "IT_SYS_COMPANY_LIST = np.concatenate((IT_SYS_COMPANY_FIRST, IT_SYS_COMPANY_SECOND))\n",
    "IT_SYS_DATE_LIST = np.concatenate((IT_SYS_DATE_FIRST, IT_SYS_DATE_SECOND))\n",
    "IT_SYS_LOCATION_LIST = np.concatenate((IT_SYS_LOCATION_FIRST, IT_SYS_LOCATION_SECOND))\n",
    "IT_SYS_STATUS_LIST = np.concatenate((IT_SYS_STATUS_FIRST, IT_SYS_STATUS_SECOND))\n",
    "IT_SYS_SALARY_LIST = np.concatenate((IT_SYS_SALARY_FIRST, IT_SYS_SALARY_SECOND))\n",
    "IT_SYS_SALARY_MIN_LIST = np.concatenate((IT_SYS_SALARY_MIN_FIRST, IT_SYS_SALARY_MIN_SECOND))\n",
    "IT_SYS_SALARY_MAX_LIST = np.concatenate((IT_SYS_SALARY_MAX_FIRST, IT_SYS_SALARY_MAX_SECOND))\n",
    "IT_SYS_DESCRIPTION_LIST = np.concatenate((IT_SYS_DESCRIPTION_FIRST, IT_SYS_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5228b9-a104-44a1-897d-b8c291729fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Network/Sys/DB Admin (JOBLUM) \n",
    "IT_SYS={'Website': \"Joblum\",\n",
    "      'Job Title': IT_SYS_TITLE_LIST, \n",
    "      'Category': \"IT - Network/Sys/DB Admin\", \n",
    "      'Company': IT_SYS_COMPANY_LIST, \n",
    "      'Date Posted': IT_SYS_DATE_LIST, \n",
    "      'Location': IT_SYS_LOCATION_LIST, \n",
    "      'Status': IT_SYS_STATUS_LIST, \n",
    "      'Salary': IT_SYS_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_SYS_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_SYS_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_SYS_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_SYS_df = pd.DataFrame(data=IT_SYS)\n",
    "IT_SYS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7826843-65f3-45f1-a268-b1ddc82c204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_SYS_df.to_csv ('Joblum Data\\JOBLUM-IT_SYS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1eb41-ab78-4cb0-accf-69ce5d0dc8a3",
   "metadata": {},
   "source": [
    "### CATEGORY - IT - Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6966-2830-4a3a-8e46-0812648b577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Software (JOBLUM) - FIRST HALF\n",
    "\n",
    "IT_SOFTWARE_TITLE_FIRST = []\n",
    "IT_SOFTWARE_COMPANY_FIRST = []\n",
    "IT_SOFTWARE_DATE_FIRST = []\n",
    "IT_SOFTWARE_LOCATION_FIRST = []\n",
    "IT_SOFTWARE_STATUS_FIRST = []\n",
    "IT_SOFTWARE_SALARY_FIRST = []\n",
    "IT_SOFTWARE_SALARY_MIN_FIRST = []\n",
    "IT_SOFTWARE_SALARY_MAX_FIRST = []\n",
    "IT_SOFTWARE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-it-software?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SOFTWARE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SOFTWARE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SOFTWARE_STATUS_FIRST, IT_SOFTWARE_SALARY_FIRST, IT_SOFTWARE_SALARY_MIN_FIRST, \n",
    "              IT_SOFTWARE_SALARY_MAX_FIRST, IT_SOFTWARE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad26e2-04e5-4149-95bb-4941604a0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of IT - Software (JOBLUM) - SECOND HALF\n",
    "\n",
    "IT_SOFTWARE_TITLE_SECOND = []\n",
    "IT_SOFTWARE_COMPANY_SECOND = []\n",
    "IT_SOFTWARE_DATE_SECOND = []\n",
    "IT_SOFTWARE_LOCATION_SECOND = []\n",
    "IT_SOFTWARE_STATUS_SECOND = []\n",
    "IT_SOFTWARE_SALARY_SECOND = []\n",
    "IT_SOFTWARE_SALARY_MIN_SECOND = []\n",
    "IT_SOFTWARE_SALARY_MAX_SECOND = []\n",
    "IT_SOFTWARE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        IT_SOFTWARE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        IT_SOFTWARE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        IT_SOFTWARE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, IT_SOFTWARE_STATUS_SECOND, IT_SOFTWARE_SALARY_SECOND, IT_SOFTWARE_SALARY_MIN_SECOND, \n",
    "              IT_SOFTWARE_SALARY_MAX_SECOND, IT_SOFTWARE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27dbc3-28c9-4b1c-aebb-08e5556ecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of IT - Software (JOBLUM) \n",
    "\n",
    "IT_SOFTWARE_TITLE_LIST = np.concatenate((IT_SOFTWARE_TITLE_FIRST, IT_SOFTWARE_TITLE_SECOND))\n",
    "IT_SOFTWARE_COMPANY_LIST = np.concatenate((IT_SOFTWARE_COMPANY_FIRST, IT_SOFTWARE_COMPANY_SECOND))\n",
    "IT_SOFTWARE_DATE_LIST = np.concatenate((IT_SOFTWARE_DATE_FIRST, IT_SOFTWARE_DATE_SECOND))\n",
    "IT_SOFTWARE_LOCATION_LIST = np.concatenate((IT_SOFTWARE_LOCATION_FIRST, IT_SOFTWARE_LOCATION_SECOND))\n",
    "IT_SOFTWARE_STATUS_LIST = np.concatenate((IT_SOFTWARE_STATUS_FIRST, IT_SOFTWARE_STATUS_SECOND))\n",
    "IT_SOFTWARE_SALARY_LIST = np.concatenate((IT_SOFTWARE_SALARY_FIRST, IT_SOFTWARE_SALARY_SECOND))\n",
    "IT_SOFTWARE_SALARY_MIN_LIST = np.concatenate((IT_SOFTWARE_SALARY_MIN_FIRST, IT_SOFTWARE_SALARY_MIN_SECOND))\n",
    "IT_SOFTWARE_SALARY_MAX_LIST = np.concatenate((IT_SOFTWARE_SALARY_MAX_FIRST, IT_SOFTWARE_SALARY_MAX_SECOND))\n",
    "IT_SOFTWARE_DESCRIPTION_LIST = np.concatenate((IT_SOFTWARE_DESCRIPTION_FIRST, IT_SOFTWARE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4c322-218b-46a5-b8f7-60804a7d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for IT - Software (JOBLUM) \n",
    "IT_SOFTWARE={'Website': \"Joblum\",\n",
    "      'Job Title': IT_SOFTWARE_TITLE_LIST, \n",
    "      'Category': \"IT - Software\", \n",
    "      'Company': IT_SOFTWARE_COMPANY_LIST, \n",
    "      'Date Posted': IT_SOFTWARE_DATE_LIST, \n",
    "      'Location': IT_SOFTWARE_LOCATION_LIST, \n",
    "      'Status': IT_SOFTWARE_STATUS_LIST, \n",
    "      'Salary': IT_SOFTWARE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': IT_SOFTWARE_DESCRIPTION_LIST,\n",
    "      'Min Salary': IT_SOFTWARE_SALARY_MIN_LIST,\n",
    "      'Max Salary': IT_SOFTWARE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"IT\"}\n",
    "IT_SOFTWARE_df = pd.DataFrame(data=IT_SOFTWARE)\n",
    "IT_SOFTWARE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c453bc-99fa-42bb-8032-9f402d5c0f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_SOFTWARE_df.to_csv ('Joblum Data\\JOBLUM-IT_SOFTWARE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4a426-5215-44d0-93fc-bea78da253e2",
   "metadata": {},
   "source": [
    "### CATEGORY - Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7182e33c-0f74-4094-84ce-a6bb3f9f6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Maintenance (JOBLUM) - FIRST HALF\n",
    "\n",
    "MAINTENANCE_TITLE_FIRST = []\n",
    "MAINTENANCE_COMPANY_FIRST = []\n",
    "MAINTENANCE_DATE_FIRST = []\n",
    "MAINTENANCE_LOCATION_FIRST = []\n",
    "MAINTENANCE_STATUS_FIRST = []\n",
    "MAINTENANCE_SALARY_FIRST = []\n",
    "MAINTENANCE_SALARY_MIN_FIRST = []\n",
    "MAINTENANCE_SALARY_MAX_FIRST = []\n",
    "MAINTENANCE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-maintenance?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MAINTENANCE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MAINTENANCE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MAINTENANCE_STATUS_FIRST, MAINTENANCE_SALARY_FIRST, MAINTENANCE_SALARY_MIN_FIRST, \n",
    "              MAINTENANCE_SALARY_MAX_FIRST, MAINTENANCE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603faa9d-8e68-4c97-a55d-400f3c32374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Maintenance (JOBLUM) - SECOND HALF\n",
    "\n",
    "MAINTENANCE_TITLE_SECOND = []\n",
    "MAINTENANCE_COMPANY_SECOND = []\n",
    "MAINTENANCE_DATE_SECOND = []\n",
    "MAINTENANCE_LOCATION_SECOND = []\n",
    "MAINTENANCE_STATUS_SECOND = []\n",
    "MAINTENANCE_SALARY_SECOND = []\n",
    "MAINTENANCE_SALARY_MIN_SECOND = []\n",
    "MAINTENANCE_SALARY_MAX_SECOND = []\n",
    "MAINTENANCE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MAINTENANCE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MAINTENANCE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MAINTENANCE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MAINTENANCE_STATUS_SECOND, MAINTENANCE_SALARY_SECOND, MAINTENANCE_SALARY_MIN_SECOND, \n",
    "              MAINTENANCE_SALARY_MAX_SECOND, MAINTENANCE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d035616-b79b-44ca-97ea-db4a82dd7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Maintenance (JOBLUM) \n",
    "\n",
    "MAINTENANCE_TITLE_LIST = np.concatenate((MAINTENANCE_TITLE_FIRST, MAINTENANCE_TITLE_SECOND))\n",
    "MAINTENANCE_COMPANY_LIST = np.concatenate((MAINTENANCE_COMPANY_FIRST, MAINTENANCE_COMPANY_SECOND))\n",
    "MAINTENANCE_DATE_LIST = np.concatenate((MAINTENANCE_DATE_FIRST, MAINTENANCE_DATE_SECOND))\n",
    "MAINTENANCE_LOCATION_LIST = np.concatenate((MAINTENANCE_LOCATION_FIRST, MAINTENANCE_LOCATION_SECOND))\n",
    "MAINTENANCE_STATUS_LIST = np.concatenate((MAINTENANCE_STATUS_FIRST, MAINTENANCE_STATUS_SECOND))\n",
    "MAINTENANCE_SALARY_LIST = np.concatenate((MAINTENANCE_SALARY_FIRST, MAINTENANCE_SALARY_SECOND))\n",
    "MAINTENANCE_SALARY_MIN_LIST = np.concatenate((MAINTENANCE_SALARY_MIN_FIRST, MAINTENANCE_SALARY_MIN_SECOND))\n",
    "MAINTENANCE_SALARY_MAX_LIST = np.concatenate((MAINTENANCE_SALARY_MAX_FIRST, MAINTENANCE_SALARY_MAX_SECOND))\n",
    "MAINTENANCE_DESCRIPTION_LIST = np.concatenate((MAINTENANCE_DESCRIPTION_FIRST, MAINTENANCE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958113c9-c4b0-4b3e-a22b-bb68a9ed31d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Maintenance (JOBLUM) \n",
    "MAINTENANCE={'Website': \"Joblum\",\n",
    "      'Job Title': MAINTENANCE_TITLE_LIST, \n",
    "      'Category': \"Maintenance\", \n",
    "      'Company': MAINTENANCE_COMPANY_LIST, \n",
    "      'Date Posted': MAINTENANCE_DATE_LIST, \n",
    "      'Location': MAINTENANCE_LOCATION_LIST, \n",
    "      'Status': MAINTENANCE_STATUS_LIST, \n",
    "      'Salary': MAINTENANCE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MAINTENANCE_DESCRIPTION_LIST,\n",
    "      'Min Salary': MAINTENANCE_SALARY_MIN_LIST,\n",
    "      'Max Salary': MAINTENANCE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MAINTENANCE_df = pd.DataFrame(data=MAINTENANCE)\n",
    "MAINTENANCE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cd91d-9870-469e-b951-e076bc983edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAINTENANCE_df.to_csv ('Joblum Data\\JOBLUM-MAINTENANCE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6096c-a422-48a8-bf73-769e0ddfb541",
   "metadata": {},
   "source": [
    "### CATEGORY - Mechanical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325d693-4604-481c-bb5e-281ea0f4bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical (JOBLUM) - FIRST HALF\n",
    "\n",
    "MECH_TITLE_FIRST = []\n",
    "MECH_COMPANY_FIRST = []\n",
    "MECH_DATE_FIRST = []\n",
    "MECH_LOCATION_FIRST = []\n",
    "MECH_STATUS_FIRST = []\n",
    "MECH_SALARY_FIRST = []\n",
    "MECH_SALARY_MIN_FIRST = []\n",
    "MECH_SALARY_MAX_FIRST = []\n",
    "MECH_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-mechanical?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECH_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECH_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECH_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECH_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECH_STATUS_FIRST, MECH_SALARY_FIRST, MECH_SALARY_MIN_FIRST, \n",
    "              MECH_SALARY_MAX_FIRST, MECH_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c22ca2-cc15-43a3-90a0-e05efb566f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical (JOBLUM) - SECOND HALF\n",
    "\n",
    "MECH_TITLE_SECOND = []\n",
    "MECH_COMPANY_SECOND = []\n",
    "MECH_DATE_SECOND = []\n",
    "MECH_LOCATION_SECOND = []\n",
    "MECH_STATUS_SECOND = []\n",
    "MECH_SALARY_SECOND = []\n",
    "MECH_SALARY_MIN_SECOND = []\n",
    "MECH_SALARY_MAX_SECOND = []\n",
    "MECH_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECH_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECH_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECH_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECH_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECH_STATUS_SECOND, MECH_SALARY_SECOND, MECH_SALARY_MIN_SECOND, \n",
    "              MECH_SALARY_MAX_SECOND, MECH_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39213d2e-3ab8-4f38-a926-24af8a3cbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Mechanical (JOBLUM) \n",
    "\n",
    "MECH_TITLE_LIST = np.concatenate((MECH_TITLE_FIRST, MECH_TITLE_SECOND))\n",
    "MECH_COMPANY_LIST = np.concatenate((MECH_COMPANY_FIRST, MECH_COMPANY_SECOND))\n",
    "MECH_DATE_LIST = np.concatenate((MECH_DATE_FIRST, MECH_DATE_SECOND))\n",
    "MECH_LOCATION_LIST = np.concatenate((MECH_LOCATION_FIRST, MECH_LOCATION_SECOND))\n",
    "MECH_STATUS_LIST = np.concatenate((MECH_STATUS_FIRST, MECH_STATUS_SECOND))\n",
    "MECH_SALARY_LIST = np.concatenate((MECH_SALARY_FIRST, MECH_SALARY_SECOND))\n",
    "MECH_SALARY_MIN_LIST = np.concatenate((MECH_SALARY_MIN_FIRST, MECH_SALARY_MIN_SECOND))\n",
    "MECH_SALARY_MAX_LIST = np.concatenate((MECH_SALARY_MAX_FIRST, MECH_SALARY_MAX_SECOND))\n",
    "MECH_DESCRIPTION_LIST = np.concatenate((MECH_DESCRIPTION_FIRST, MECH_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c933bf-6256-4fdf-9558-fc3e91996512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Mechanical (JOBLUM) \n",
    "MECH={'Website': \"Joblum\",\n",
    "      'Job Title': MECH_TITLE_LIST, \n",
    "      'Category': \"Mechanical\", \n",
    "      'Company': MECH_COMPANY_LIST, \n",
    "      'Date Posted': MECH_DATE_LIST, \n",
    "      'Location': MECH_LOCATION_LIST, \n",
    "      'Status': MECH_STATUS_LIST, \n",
    "      'Salary': MECH_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MECH_DESCRIPTION_LIST,\n",
    "      'Min Salary': MECH_SALARY_MIN_LIST,\n",
    "      'Max Salary': MECH_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MECH_df = pd.DataFrame(data=MECH)\n",
    "MECH_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257216f-bc81-4324-84ef-7bfd910ceff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MECH_df.to_csv ('Joblum Data\\JOBLUM-MECH.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08513f93-0bef-48ce-a4d9-58bf9c2e0d32",
   "metadata": {},
   "source": [
    "### CATEGORY - Mechanical/Automotive Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f414c-a67e-43ca-8b1d-76f138b62ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical/Automotive Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "MECHENG_TITLE_FIRST = []\n",
    "MECHENG_COMPANY_FIRST = []\n",
    "MECHENG_DATE_FIRST = []\n",
    "MECHENG_LOCATION_FIRST = []\n",
    "MECHENG_STATUS_FIRST = []\n",
    "MECHENG_SALARY_FIRST = []\n",
    "MECHENG_SALARY_MIN_FIRST = []\n",
    "MECHENG_SALARY_MAX_FIRST = []\n",
    "MECHENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-mechanical-automotive-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECHENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECHENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECHENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECHENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECHENG_STATUS_FIRST, MECHENG_SALARY_FIRST, MECHENG_SALARY_MIN_FIRST, \n",
    "              MECHENG_SALARY_MAX_FIRST, MECHENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55772a2d-caf5-428a-a896-b4265ee9ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Mechanical/Automotive Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "MECHENG_TITLE_SECOND = []\n",
    "MECHENG_COMPANY_SECOND = []\n",
    "MECHENG_DATE_SECOND = []\n",
    "MECHENG_LOCATION_SECOND = []\n",
    "MECHENG_STATUS_SECOND = []\n",
    "MECHENG_SALARY_SECOND = []\n",
    "MECHENG_SALARY_MIN_SECOND = []\n",
    "MECHENG_SALARY_MAX_SECOND = []\n",
    "MECHENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        MECHENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        MECHENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        MECHENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        MECHENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, MECHENG_STATUS_SECOND, MECHENG_SALARY_SECOND, MECHENG_SALARY_MIN_SECOND, \n",
    "              MECHENG_SALARY_MAX_SECOND, MECHENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7342b47-debd-4832-aff6-cab6cf93f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Mechanical/Automotive Engineering (JOBLUM) \n",
    "\n",
    "MECHENG_TITLE_LIST = np.concatenate((MECHENG_TITLE_FIRST, MECHENG_TITLE_SECOND))\n",
    "MECHENG_COMPANY_LIST = np.concatenate((MECHENG_COMPANY_FIRST, MECHENG_COMPANY_SECOND))\n",
    "MECHENG_DATE_LIST = np.concatenate((MECHENG_DATE_FIRST, MECHENG_DATE_SECOND))\n",
    "MECHENG_LOCATION_LIST = np.concatenate((MECHENG_LOCATION_FIRST, MECHENG_LOCATION_SECOND))\n",
    "MECHENG_STATUS_LIST = np.concatenate((MECHENG_STATUS_FIRST, MECHENG_STATUS_SECOND))\n",
    "MECHENG_SALARY_LIST = np.concatenate((MECHENG_SALARY_FIRST, MECHENG_SALARY_SECOND))\n",
    "MECHENG_SALARY_MIN_LIST = np.concatenate((MECHENG_SALARY_MIN_FIRST, MECHENG_SALARY_MIN_SECOND))\n",
    "MECHENG_SALARY_MAX_LIST = np.concatenate((MECHENG_SALARY_MAX_FIRST, MECHENG_SALARY_MAX_SECOND))\n",
    "MECHENG_DESCRIPTION_LIST = np.concatenate((MECHENG_DESCRIPTION_FIRST, MECHENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1fdb8-2896-4e17-97d0-d7615ab8fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Mechanical/Automotive Engineering (JOBLUM) \n",
    "MECHENG={'Website': \"Joblum\",\n",
    "      'Job Title': MECHENG_TITLE_LIST, \n",
    "      'Category': \"Mechanical/Automotive Engineering\", \n",
    "      'Company': MECHENG_COMPANY_LIST, \n",
    "      'Date Posted': MECHENG_DATE_LIST, \n",
    "      'Location': MECHENG_LOCATION_LIST, \n",
    "      'Status': MECHENG_STATUS_LIST, \n",
    "      'Salary': MECHENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': MECHENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': MECHENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': MECHENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "MECHENG_df = pd.DataFrame(data=MECHENG)\n",
    "MECHENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7459dd9-9444-4dc3-a121-b46f55b76306",
   "metadata": {},
   "outputs": [],
   "source": [
    "MECHENG_df.to_csv ('Joblum Data\\JOBLUM-MECHENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c5c92-755a-44f4-89fe-62cc72e20d06",
   "metadata": {},
   "source": [
    "### CATEGORY - Nurse/Medical Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac581f6b-cf0c-4b5f-b6ad-2c8638c33637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Nurse/Medical Support (JOBLUM) - FIRST HALF\n",
    "\n",
    "NURSE_TITLE_FIRST = []\n",
    "NURSE_COMPANY_FIRST = []\n",
    "NURSE_DATE_FIRST = []\n",
    "NURSE_LOCATION_FIRST = []\n",
    "NURSE_STATUS_FIRST = []\n",
    "NURSE_SALARY_FIRST = []\n",
    "NURSE_SALARY_MIN_FIRST = []\n",
    "NURSE_SALARY_MAX_FIRST = []\n",
    "NURSE_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-nurse-medical-support?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NURSE_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NURSE_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NURSE_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NURSE_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NURSE_STATUS_FIRST, NURSE_SALARY_FIRST, NURSE_SALARY_MIN_FIRST, \n",
    "              NURSE_SALARY_MAX_FIRST, NURSE_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8ac9f-5029-47ee-8cdb-443e236d4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Nurse/Medical Support (JOBLUM) - SECOND HALF\n",
    "\n",
    "NURSE_TITLE_SECOND = []\n",
    "NURSE_COMPANY_SECOND = []\n",
    "NURSE_DATE_SECOND = []\n",
    "NURSE_LOCATION_SECOND = []\n",
    "NURSE_STATUS_SECOND = []\n",
    "NURSE_SALARY_SECOND = []\n",
    "NURSE_SALARY_MIN_SECOND = []\n",
    "NURSE_SALARY_MAX_SECOND = []\n",
    "NURSE_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        NURSE_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        NURSE_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        NURSE_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        NURSE_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, NURSE_STATUS_SECOND, NURSE_SALARY_SECOND, NURSE_SALARY_MIN_SECOND, \n",
    "              NURSE_SALARY_MAX_SECOND, NURSE_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452193e-1aca-4c18-af01-3e0070c26c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Nurse/Medical Support (JOBLUM) \n",
    "\n",
    "NURSE_TITLE_LIST = np.concatenate((NURSE_TITLE_FIRST, NURSE_TITLE_SECOND))\n",
    "NURSE_COMPANY_LIST = np.concatenate((NURSE_COMPANY_FIRST, NURSE_COMPANY_SECOND))\n",
    "NURSE_DATE_LIST = np.concatenate((NURSE_DATE_FIRST, NURSE_DATE_SECOND))\n",
    "NURSE_LOCATION_LIST = np.concatenate((NURSE_LOCATION_FIRST, NURSE_LOCATION_SECOND))\n",
    "NURSE_STATUS_LIST = np.concatenate((NURSE_STATUS_FIRST, NURSE_STATUS_SECOND))\n",
    "NURSE_SALARY_LIST = np.concatenate((NURSE_SALARY_FIRST, NURSE_SALARY_SECOND))\n",
    "NURSE_SALARY_MIN_LIST = np.concatenate((NURSE_SALARY_MIN_FIRST, NURSE_SALARY_MIN_SECOND))\n",
    "NURSE_SALARY_MAX_LIST = np.concatenate((NURSE_SALARY_MAX_FIRST, NURSE_SALARY_MAX_SECOND))\n",
    "NURSE_DESCRIPTION_LIST = np.concatenate((NURSE_DESCRIPTION_FIRST, NURSE_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7c023-c934-4261-aed1-5942156d301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Nurse/Medical Support (JOBLUM) \n",
    "NURSE={'Website': \"Joblum\",\n",
    "      'Job Title': NURSE_TITLE_LIST, \n",
    "      'Category': \"Nurse/Medical Support\", \n",
    "      'Company': NURSE_COMPANY_LIST, \n",
    "      'Date Posted': NURSE_DATE_LIST, \n",
    "      'Location': NURSE_LOCATION_LIST, \n",
    "      'Status': NURSE_STATUS_LIST, \n",
    "      'Salary': NURSE_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': NURSE_DESCRIPTION_LIST,\n",
    "      'Min Salary': NURSE_SALARY_MIN_LIST,\n",
    "      'Max Salary': NURSE_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "NURSE_df = pd.DataFrame(data=NURSE)\n",
    "NURSE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5dc52-224c-4010-8eb8-5c91130b96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NURSE_df.to_csv ('Joblum Data\\JOBLUM-NURSE.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efeebbb-e337-462a-ab04-2b56fb24e466",
   "metadata": {},
   "source": [
    "### CATEGORY - Oil/Gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176851c-ff61-4fe3-bc96-12f6d5bff036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas (JOBLUM) - FIRST HALF\n",
    "\n",
    "OIL_TITLE_FIRST = []\n",
    "OIL_COMPANY_FIRST = []\n",
    "OIL_DATE_FIRST = []\n",
    "OIL_LOCATION_FIRST = []\n",
    "OIL_STATUS_FIRST = []\n",
    "OIL_SALARY_FIRST = []\n",
    "OIL_SALARY_MIN_FIRST = []\n",
    "OIL_SALARY_MAX_FIRST = []\n",
    "OIL_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-oil-gas?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OIL_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OIL_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OIL_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OIL_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OIL_STATUS_FIRST, OIL_SALARY_FIRST, OIL_SALARY_MIN_FIRST, \n",
    "              OIL_SALARY_MAX_FIRST, OIL_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28971a-d70d-4833-80a4-016132fa0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas (JOBLUM) - SECOND HALF\n",
    "\n",
    "OIL_TITLE_SECOND = []\n",
    "OIL_COMPANY_SECOND = []\n",
    "OIL_DATE_SECOND = []\n",
    "OIL_LOCATION_SECOND = []\n",
    "OIL_STATUS_SECOND = []\n",
    "OIL_SALARY_SECOND = []\n",
    "OIL_SALARY_MIN_SECOND = []\n",
    "OIL_SALARY_MAX_SECOND = []\n",
    "OIL_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OIL_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OIL_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OIL_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OIL_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OIL_STATUS_SECOND, OIL_SALARY_SECOND, OIL_SALARY_MIN_SECOND, \n",
    "              OIL_SALARY_MAX_SECOND, OIL_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef22b1d-aeed-4d2d-9e42-a243cf703b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Oil/Gas (JOBLUM) \n",
    "\n",
    "OIL_TITLE_LIST = np.concatenate((OIL_TITLE_FIRST, OIL_TITLE_SECOND))\n",
    "OIL_COMPANY_LIST = np.concatenate((OIL_COMPANY_FIRST, OIL_COMPANY_SECOND))\n",
    "OIL_DATE_LIST = np.concatenate((OIL_DATE_FIRST, OIL_DATE_SECOND))\n",
    "OIL_LOCATION_LIST = np.concatenate((OIL_LOCATION_FIRST, OIL_LOCATION_SECOND))\n",
    "OIL_STATUS_LIST = np.concatenate((OIL_STATUS_FIRST, OIL_STATUS_SECOND))\n",
    "OIL_SALARY_LIST = np.concatenate((OIL_SALARY_FIRST, OIL_SALARY_SECOND))\n",
    "OIL_SALARY_MIN_LIST = np.concatenate((OIL_SALARY_MIN_FIRST, OIL_SALARY_MIN_SECOND))\n",
    "OIL_SALARY_MAX_LIST = np.concatenate((OIL_SALARY_MAX_FIRST, OIL_SALARY_MAX_SECOND))\n",
    "OIL_DESCRIPTION_LIST = np.concatenate((OIL_DESCRIPTION_FIRST, OIL_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f6960-8098-4ab0-b33e-41a1864cf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Oil/Gas (JOBLUM) \n",
    "OIL={'Website': \"Joblum\",\n",
    "      'Job Title': OIL_TITLE_LIST, \n",
    "      'Category': \"Oil/Gas\", \n",
    "      'Company': OIL_COMPANY_LIST, \n",
    "      'Date Posted': OIL_DATE_LIST, \n",
    "      'Location': OIL_LOCATION_LIST, \n",
    "      'Status': OIL_STATUS_LIST, \n",
    "      'Salary': OIL_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': OIL_DESCRIPTION_LIST,\n",
    "      'Min Salary': OIL_SALARY_MIN_LIST,\n",
    "      'Max Salary': OIL_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "OIL_df = pd.DataFrame(data=OIL)\n",
    "OIL_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd9045-d031-4751-9501-d05adcab7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "OIL_df.to_csv ('Joblum Data\\JOBLUM-OIL.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fde51b-6586-4eff-aaf5-ef6e3119b702",
   "metadata": {},
   "source": [
    "### CATEGORY - Oil/Gas Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1af6ce-85ac-4a92-986a-91209dbaf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "OILENG_TITLE_FIRST = []\n",
    "OILENG_COMPANY_FIRST = []\n",
    "OILENG_DATE_FIRST = []\n",
    "OILENG_LOCATION_FIRST = []\n",
    "OILENG_STATUS_FIRST = []\n",
    "OILENG_SALARY_FIRST = []\n",
    "OILENG_SALARY_MIN_FIRST = []\n",
    "OILENG_SALARY_MAX_FIRST = []\n",
    "OILENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-oil-gas-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OILENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OILENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OILENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OILENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OILENG_STATUS_FIRST, OILENG_SALARY_FIRST, OILENG_SALARY_MIN_FIRST, \n",
    "              OILENG_SALARY_MAX_FIRST, OILENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d29995-d0a5-4ae0-83f2-6e531d82ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Oil/Gas Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "OILENG_TITLE_SECOND = []\n",
    "OILENG_COMPANY_SECOND = []\n",
    "OILENG_DATE_SECOND = []\n",
    "OILENG_LOCATION_SECOND = []\n",
    "OILENG_STATUS_SECOND = []\n",
    "OILENG_SALARY_SECOND = []\n",
    "OILENG_SALARY_MIN_SECOND = []\n",
    "OILENG_SALARY_MAX_SECOND = []\n",
    "OILENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        OILENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        OILENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        OILENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        OILENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, OILENG_STATUS_SECOND, OILENG_SALARY_SECOND, OILENG_SALARY_MIN_SECOND, \n",
    "              OILENG_SALARY_MAX_SECOND, OILENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3142178-9ae1-4000-a236-5aa1563d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Oil/Gas Engineering (JOBLUM) \n",
    "\n",
    "OILENG_TITLE_LIST = np.concatenate((OILENG_TITLE_FIRST, OILENG_TITLE_SECOND))\n",
    "OILENG_COMPANY_LIST = np.concatenate((OILENG_COMPANY_FIRST, OILENG_COMPANY_SECOND))\n",
    "OILENG_DATE_LIST = np.concatenate((OILENG_DATE_FIRST, OILENG_DATE_SECOND))\n",
    "OILENG_LOCATION_LIST = np.concatenate((OILENG_LOCATION_FIRST, OILENG_LOCATION_SECOND))\n",
    "OILENG_STATUS_LIST = np.concatenate((OILENG_STATUS_FIRST, OILENG_STATUS_SECOND))\n",
    "OILENG_SALARY_LIST = np.concatenate((OILENG_SALARY_FIRST, OILENG_SALARY_SECOND))\n",
    "OILENG_SALARY_MIN_LIST = np.concatenate((OILENG_SALARY_MIN_FIRST, OILENG_SALARY_MIN_SECOND))\n",
    "OILENG_SALARY_MAX_LIST = np.concatenate((OILENG_SALARY_MAX_FIRST, OILENG_SALARY_MAX_SECOND))\n",
    "OILENG_DESCRIPTION_LIST = np.concatenate((OILENG_DESCRIPTION_FIRST, OILENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5fef2-fd5a-4f5d-a4f3-947d11230f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Oil/Gas Engineering (JOBLUM) \n",
    "OILENG={'Website': \"Joblum\",\n",
    "      'Job Title': OILENG_TITLE_LIST, \n",
    "      'Category': \"Oil/Gas Engineering\", \n",
    "      'Company': OILENG_COMPANY_LIST, \n",
    "      'Date Posted': OILENG_DATE_LIST, \n",
    "      'Location': OILENG_LOCATION_LIST, \n",
    "      'Status': OILENG_STATUS_LIST, \n",
    "      'Salary': OILENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': OILENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': OILENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': OILENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "OILENG_df = pd.DataFrame(data=OILENG)\n",
    "OILENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699cf2b-373d-48cf-b09b-3682f333837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OILENG_df.to_csv ('Joblum Data\\JOBLUM-OILENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee9415-6548-4f42-85cf-8d8479133f0a",
   "metadata": {},
   "source": [
    "### CATEGORY - Other Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841193b-7a5b-4b47-8631-cb01d4082990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Other Engineering (JOBLUM) - FIRST HALF\n",
    "\n",
    "ENG_TITLE_FIRST = []\n",
    "ENG_COMPANY_FIRST = []\n",
    "ENG_DATE_FIRST = []\n",
    "ENG_LOCATION_FIRST = []\n",
    "ENG_STATUS_FIRST = []\n",
    "ENG_SALARY_FIRST = []\n",
    "ENG_SALARY_MIN_FIRST = []\n",
    "ENG_SALARY_MAX_FIRST = []\n",
    "ENG_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-other-engineering?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENG_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENG_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENG_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENG_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENG_STATUS_FIRST, ENG_SALARY_FIRST, ENG_SALARY_MIN_FIRST, \n",
    "              ENG_SALARY_MAX_FIRST, ENG_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77ee0f-dc65-4605-9861-b468460ab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Other Engineering (JOBLUM) - SECOND HALF\n",
    "\n",
    "ENG_TITLE_SECOND = []\n",
    "ENG_COMPANY_SECOND = []\n",
    "ENG_DATE_SECOND = []\n",
    "ENG_LOCATION_SECOND = []\n",
    "ENG_STATUS_SECOND = []\n",
    "ENG_SALARY_SECOND = []\n",
    "ENG_SALARY_MIN_SECOND = []\n",
    "ENG_SALARY_MAX_SECOND = []\n",
    "ENG_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        ENG_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        ENG_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        ENG_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        ENG_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, ENG_STATUS_SECOND, ENG_SALARY_SECOND, ENG_SALARY_MIN_SECOND, \n",
    "              ENG_SALARY_MAX_SECOND, ENG_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b2bef-9930-43eb-9f7c-e9afbb442adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Other Engineering (JOBLUM) \n",
    "\n",
    "ENG_TITLE_LIST = np.concatenate((ENG_TITLE_FIRST, ENG_TITLE_SECOND))\n",
    "ENG_COMPANY_LIST = np.concatenate((ENG_COMPANY_FIRST, ENG_COMPANY_SECOND))\n",
    "ENG_DATE_LIST = np.concatenate((ENG_DATE_FIRST, ENG_DATE_SECOND))\n",
    "ENG_LOCATION_LIST = np.concatenate((ENG_LOCATION_FIRST, ENG_LOCATION_SECOND))\n",
    "ENG_STATUS_LIST = np.concatenate((ENG_STATUS_FIRST, ENG_STATUS_SECOND))\n",
    "ENG_SALARY_LIST = np.concatenate((ENG_SALARY_FIRST, ENG_SALARY_SECOND))\n",
    "ENG_SALARY_MIN_LIST = np.concatenate((ENG_SALARY_MIN_FIRST, ENG_SALARY_MIN_SECOND))\n",
    "ENG_SALARY_MAX_LIST = np.concatenate((ENG_SALARY_MAX_FIRST, ENG_SALARY_MAX_SECOND))\n",
    "ENG_DESCRIPTION_LIST = np.concatenate((ENG_DESCRIPTION_FIRST, ENG_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bdfe6-f7e0-4f92-8ae4-e5f49a877fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Other Engineering (JOBLUM) \n",
    "ENG={'Website': \"Joblum\",\n",
    "      'Job Title': ENG_TITLE_LIST, \n",
    "      'Category': \"Other Engineering\", \n",
    "      'Company': ENG_COMPANY_LIST, \n",
    "      'Date Posted': ENG_DATE_LIST, \n",
    "      'Location': ENG_LOCATION_LIST, \n",
    "      'Status': ENG_STATUS_LIST, \n",
    "      'Salary': ENG_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': ENG_DESCRIPTION_LIST,\n",
    "      'Min Salary': ENG_SALARY_MIN_LIST,\n",
    "      'Max Salary': ENG_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "ENG_df = pd.DataFrame(data=ENG)\n",
    "ENG_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f845b-1230-45f8-8bf5-5ebae332d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENG_df.to_csv ('Joblum Data\\JOBLUM-ENG.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcccb6e-f6c1-47bd-a3db-4828397dc902",
   "metadata": {},
   "source": [
    "### CATEGORY - Pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09965896-bc9f-48f6-b011-f01f70464a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Pharmacy (JOBLUM) - FIRST HALF\n",
    "\n",
    "PHARMA_TITLE_FIRST = []\n",
    "PHARMA_COMPANY_FIRST = []\n",
    "PHARMA_DATE_FIRST = []\n",
    "PHARMA_LOCATION_FIRST = []\n",
    "PHARMA_STATUS_FIRST = []\n",
    "PHARMA_SALARY_FIRST = []\n",
    "PHARMA_SALARY_MIN_FIRST = []\n",
    "PHARMA_SALARY_MAX_FIRST = []\n",
    "PHARMA_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-pharmacy?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PHARMA_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PHARMA_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PHARMA_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PHARMA_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PHARMA_STATUS_FIRST, PHARMA_SALARY_FIRST, PHARMA_SALARY_MIN_FIRST, \n",
    "              PHARMA_SALARY_MAX_FIRST, PHARMA_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e8dd9-ca5b-4eeb-9119-214e379219a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Pharmacy (JOBLUM) - SECOND HALF\n",
    "\n",
    "PHARMA_TITLE_SECOND = []\n",
    "PHARMA_COMPANY_SECOND = []\n",
    "PHARMA_DATE_SECOND = []\n",
    "PHARMA_LOCATION_SECOND = []\n",
    "PHARMA_STATUS_SECOND = []\n",
    "PHARMA_SALARY_SECOND = []\n",
    "PHARMA_SALARY_MIN_SECOND = []\n",
    "PHARMA_SALARY_MAX_SECOND = []\n",
    "PHARMA_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PHARMA_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PHARMA_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PHARMA_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PHARMA_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PHARMA_STATUS_SECOND, PHARMA_SALARY_SECOND, PHARMA_SALARY_MIN_SECOND, \n",
    "              PHARMA_SALARY_MAX_SECOND, PHARMA_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe3cc7-68d4-4b4c-ae15-03ed8e4eafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Pharmacy (JOBLUM) \n",
    "\n",
    "PHARMA_TITLE_LIST = np.concatenate((PHARMA_TITLE_FIRST, PHARMA_TITLE_SECOND))\n",
    "PHARMA_COMPANY_LIST = np.concatenate((PHARMA_COMPANY_FIRST, PHARMA_COMPANY_SECOND))\n",
    "PHARMA_DATE_LIST = np.concatenate((PHARMA_DATE_FIRST, PHARMA_DATE_SECOND))\n",
    "PHARMA_LOCATION_LIST = np.concatenate((PHARMA_LOCATION_FIRST, PHARMA_LOCATION_SECOND))\n",
    "PHARMA_STATUS_LIST = np.concatenate((PHARMA_STATUS_FIRST, PHARMA_STATUS_SECOND))\n",
    "PHARMA_SALARY_LIST = np.concatenate((PHARMA_SALARY_FIRST, PHARMA_SALARY_SECOND))\n",
    "PHARMA_SALARY_MIN_LIST = np.concatenate((PHARMA_SALARY_MIN_FIRST, PHARMA_SALARY_MIN_SECOND))\n",
    "PHARMA_SALARY_MAX_LIST = np.concatenate((PHARMA_SALARY_MAX_FIRST, PHARMA_SALARY_MAX_SECOND))\n",
    "PHARMA_DESCRIPTION_LIST = np.concatenate((PHARMA_DESCRIPTION_FIRST, PHARMA_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fe73f-55d3-4f61-86c8-172f7b1efa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Pharmacy (JOBLUM) \n",
    "PHARMA={'Website': \"Joblum\",\n",
    "      'Job Title': PHARMA_TITLE_LIST, \n",
    "      'Category': \"Pharmacy\", \n",
    "      'Company': PHARMA_COMPANY_LIST, \n",
    "      'Date Posted': PHARMA_DATE_LIST, \n",
    "      'Location': PHARMA_LOCATION_LIST, \n",
    "      'Status': PHARMA_STATUS_LIST, \n",
    "      'Salary': PHARMA_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': PHARMA_DESCRIPTION_LIST,\n",
    "      'Min Salary': PHARMA_SALARY_MIN_LIST,\n",
    "      'Max Salary': PHARMA_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "PHARMA_df = pd.DataFrame(data=PHARMA)\n",
    "PHARMA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fecba2-a563-4586-9633-23513420217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHARMA_df.to_csv ('Joblum Data\\JOBLUM-PHARMA.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfcf16-065b-458b-928e-52b232eb8afb",
   "metadata": {},
   "source": [
    "### CATEGORY - Quality Assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2db70-8aaf-4a33-abcf-3e8f07659697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quality Assurance (JOBLUM) - FIRST HALF\n",
    "\n",
    "QUALITY_TITLE_FIRST = []\n",
    "QUALITY_COMPANY_FIRST = []\n",
    "QUALITY_DATE_FIRST = []\n",
    "QUALITY_LOCATION_FIRST = []\n",
    "QUALITY_STATUS_FIRST = []\n",
    "QUALITY_SALARY_FIRST = []\n",
    "QUALITY_SALARY_MIN_FIRST = []\n",
    "QUALITY_SALARY_MAX_FIRST = []\n",
    "QUALITY_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-quality-assurance?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUALITY_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUALITY_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUALITY_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUALITY_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUALITY_STATUS_FIRST, QUALITY_SALARY_FIRST, QUALITY_SALARY_MIN_FIRST, \n",
    "              QUALITY_SALARY_MAX_FIRST, QUALITY_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8783cf-1d23-4751-874a-0d57699e0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quality Assurance (JOBLUM) - SECOND HALF\n",
    "\n",
    "QUALITY_TITLE_SECOND = []\n",
    "QUALITY_COMPANY_SECOND = []\n",
    "QUALITY_DATE_SECOND = []\n",
    "QUALITY_LOCATION_SECOND = []\n",
    "QUALITY_STATUS_SECOND = []\n",
    "QUALITY_SALARY_SECOND = []\n",
    "QUALITY_SALARY_MIN_SECOND = []\n",
    "QUALITY_SALARY_MAX_SECOND = []\n",
    "QUALITY_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUALITY_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUALITY_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUALITY_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUALITY_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUALITY_STATUS_SECOND, QUALITY_SALARY_SECOND, QUALITY_SALARY_MIN_SECOND, \n",
    "              QUALITY_SALARY_MAX_SECOND, QUALITY_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dc453-e5d4-4c83-9e58-08ece37201db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Quality Assurance (JOBLUM) \n",
    "\n",
    "QUALITY_TITLE_LIST = np.concatenate((QUALITY_TITLE_FIRST, QUALITY_TITLE_SECOND))\n",
    "QUALITY_COMPANY_LIST = np.concatenate((QUALITY_COMPANY_FIRST, QUALITY_COMPANY_SECOND))\n",
    "QUALITY_DATE_LIST = np.concatenate((QUALITY_DATE_FIRST, QUALITY_DATE_SECOND))\n",
    "QUALITY_LOCATION_LIST = np.concatenate((QUALITY_LOCATION_FIRST, QUALITY_LOCATION_SECOND))\n",
    "QUALITY_STATUS_LIST = np.concatenate((QUALITY_STATUS_FIRST, QUALITY_STATUS_SECOND))\n",
    "QUALITY_SALARY_LIST = np.concatenate((QUALITY_SALARY_FIRST, QUALITY_SALARY_SECOND))\n",
    "QUALITY_SALARY_MIN_LIST = np.concatenate((QUALITY_SALARY_MIN_FIRST, QUALITY_SALARY_MIN_SECOND))\n",
    "QUALITY_SALARY_MAX_LIST = np.concatenate((QUALITY_SALARY_MAX_FIRST, QUALITY_SALARY_MAX_SECOND))\n",
    "QUALITY_DESCRIPTION_LIST = np.concatenate((QUALITY_DESCRIPTION_FIRST, QUALITY_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4af654-2b79-429d-af2b-e69e2a25ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Quality Assurance (JOBLUM) \n",
    "QUALITY={'Website': \"Joblum\",\n",
    "      'Job Title': QUALITY_TITLE_LIST, \n",
    "      'Category': \"Quality Assurance\", \n",
    "      'Company': QUALITY_COMPANY_LIST, \n",
    "      'Date Posted': QUALITY_DATE_LIST, \n",
    "      'Location': QUALITY_LOCATION_LIST, \n",
    "      'Status': QUALITY_STATUS_LIST, \n",
    "      'Salary': QUALITY_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': QUALITY_DESCRIPTION_LIST,\n",
    "      'Min Salary': QUALITY_SALARY_MIN_LIST,\n",
    "      'Max Salary': QUALITY_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "QUALITY_df = pd.DataFrame(data=QUALITY)\n",
    "QUALITY_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5969435-7411-4dbc-8d97-2f9f349a4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUALITY_df.to_csv ('Joblum Data\\JOBLUM-QUALITY.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a8fa2-df2e-4544-a929-56bbde9e7fbb",
   "metadata": {},
   "source": [
    "### CATEGORY - Quantity Surveying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed26af4-eba7-46d7-b63b-edd4c62b9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quantity Surveying (JOBLUM) - FIRST HALF\n",
    "\n",
    "QUANTITY_TITLE_FIRST = []\n",
    "QUANTITY_COMPANY_FIRST = []\n",
    "QUANTITY_DATE_FIRST = []\n",
    "QUANTITY_LOCATION_FIRST = []\n",
    "QUANTITY_STATUS_FIRST = []\n",
    "QUANTITY_SALARY_FIRST = []\n",
    "QUANTITY_SALARY_MIN_FIRST = []\n",
    "QUANTITY_SALARY_MAX_FIRST = []\n",
    "QUANTITY_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-quantity-surveying?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUANTITY_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUANTITY_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUANTITY_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUANTITY_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUANTITY_STATUS_FIRST, QUANTITY_SALARY_FIRST, QUANTITY_SALARY_MIN_FIRST, \n",
    "              QUANTITY_SALARY_MAX_FIRST, QUANTITY_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446955f-4e53-4954-9fdf-a45b575fc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Quantity Surveying (JOBLUM) - SECOND HALF\n",
    "\n",
    "QUANTITY_TITLE_SECOND = []\n",
    "QUANTITY_COMPANY_SECOND = []\n",
    "QUANTITY_DATE_SECOND = []\n",
    "QUANTITY_LOCATION_SECOND = []\n",
    "QUANTITY_STATUS_SECOND = []\n",
    "QUANTITY_SALARY_SECOND = []\n",
    "QUANTITY_SALARY_MIN_SECOND = []\n",
    "QUANTITY_SALARY_MAX_SECOND = []\n",
    "QUANTITY_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        QUANTITY_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        QUANTITY_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        QUANTITY_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        QUANTITY_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, QUANTITY_STATUS_SECOND, QUANTITY_SALARY_SECOND, QUANTITY_SALARY_MIN_SECOND, \n",
    "              QUANTITY_SALARY_MAX_SECOND, QUANTITY_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827fb9e-c0f4-4796-b0d7-0c43ffbe4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Quantity Surveying (JOBLUM) \n",
    "\n",
    "QUANTITY_TITLE_LIST = np.concatenate((QUANTITY_TITLE_FIRST, QUANTITY_TITLE_SECOND))\n",
    "QUANTITY_COMPANY_LIST = np.concatenate((QUANTITY_COMPANY_FIRST, QUANTITY_COMPANY_SECOND))\n",
    "QUANTITY_DATE_LIST = np.concatenate((QUANTITY_DATE_FIRST, QUANTITY_DATE_SECOND))\n",
    "QUANTITY_LOCATION_LIST = np.concatenate((QUANTITY_LOCATION_FIRST, QUANTITY_LOCATION_SECOND))\n",
    "QUANTITY_STATUS_LIST = np.concatenate((QUANTITY_STATUS_FIRST, QUANTITY_STATUS_SECOND))\n",
    "QUANTITY_SALARY_LIST = np.concatenate((QUANTITY_SALARY_FIRST, QUANTITY_SALARY_SECOND))\n",
    "QUANTITY_SALARY_MIN_LIST = np.concatenate((QUANTITY_SALARY_MIN_FIRST, QUANTITY_SALARY_MIN_SECOND))\n",
    "QUANTITY_SALARY_MAX_LIST = np.concatenate((QUANTITY_SALARY_MAX_FIRST, QUANTITY_SALARY_MAX_SECOND))\n",
    "QUANTITY_DESCRIPTION_LIST = np.concatenate((QUANTITY_DESCRIPTION_FIRST, QUANTITY_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623815d4-33f9-4dbf-878b-09e468d85c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Quantity Surveying (JOBLUM) \n",
    "QUANTITY={'Website': \"Joblum\",\n",
    "      'Job Title': QUANTITY_TITLE_LIST, \n",
    "      'Category': \"Quantity Surveying\", \n",
    "      'Company': QUANTITY_COMPANY_LIST, \n",
    "      'Date Posted': QUANTITY_DATE_LIST, \n",
    "      'Location': QUANTITY_LOCATION_LIST, \n",
    "      'Status': QUANTITY_STATUS_LIST, \n",
    "      'Salary': QUANTITY_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': QUANTITY_DESCRIPTION_LIST,\n",
    "      'Min Salary': QUANTITY_SALARY_MIN_LIST,\n",
    "      'Max Salary': QUANTITY_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Engineering\"}\n",
    "QUANTITY_df = pd.DataFrame(data=QUANTITY)\n",
    "QUANTITY_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea42fc-3278-42cb-8281-8b4b14c3761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTITY_df.to_csv ('Joblum Data\\JOBLUM-QUANTITY.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8102d1-a6f4-45af-a51c-62103b04786b",
   "metadata": {},
   "source": [
    "### CATEGORY - Science & Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b55de-6e23-41f2-8c4b-34ecf69f2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Science & Technology (JOBLUM) - FIRST HALF\n",
    "\n",
    "SNT_TITLE_FIRST = []\n",
    "SNT_COMPANY_FIRST = []\n",
    "SNT_DATE_FIRST = []\n",
    "SNT_LOCATION_FIRST = []\n",
    "SNT_STATUS_FIRST = []\n",
    "SNT_SALARY_FIRST = []\n",
    "SNT_SALARY_MIN_FIRST = []\n",
    "SNT_SALARY_MAX_FIRST = []\n",
    "SNT_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-science-amp-technology?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        SNT_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        SNT_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        SNT_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        SNT_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, SNT_STATUS_FIRST, SNT_SALARY_FIRST, SNT_SALARY_MIN_FIRST, \n",
    "              SNT_SALARY_MAX_FIRST, SNT_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69c353-ee62-46c8-ad84-7be6225931f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Science & Technology (JOBLUM) - SECOND HALF\n",
    "\n",
    "SNT_TITLE_SECOND = []\n",
    "SNT_COMPANY_SECOND = []\n",
    "SNT_DATE_SECOND = []\n",
    "SNT_LOCATION_SECOND = []\n",
    "SNT_STATUS_SECOND = []\n",
    "SNT_SALARY_SECOND = []\n",
    "SNT_SALARY_MIN_SECOND = []\n",
    "SNT_SALARY_MAX_SECOND = []\n",
    "SNT_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        SNT_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        SNT_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        SNT_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        SNT_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, SNT_STATUS_SECOND, SNT_SALARY_SECOND, SNT_SALARY_MIN_SECOND, \n",
    "              SNT_SALARY_MAX_SECOND, SNT_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e18408-afc7-4f5c-b7e9-8f9e5d45d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Science & Technology (JOBLUM) \n",
    "\n",
    "SNT_TITLE_LIST = np.concatenate((SNT_TITLE_FIRST, SNT_TITLE_SECOND))\n",
    "SNT_COMPANY_LIST = np.concatenate((SNT_COMPANY_FIRST, SNT_COMPANY_SECOND))\n",
    "SNT_DATE_LIST = np.concatenate((SNT_DATE_FIRST, SNT_DATE_SECOND))\n",
    "SNT_LOCATION_LIST = np.concatenate((SNT_LOCATION_FIRST, SNT_LOCATION_SECOND))\n",
    "SNT_STATUS_LIST = np.concatenate((SNT_STATUS_FIRST, SNT_STATUS_SECOND))\n",
    "SNT_SALARY_LIST = np.concatenate((SNT_SALARY_FIRST, SNT_SALARY_SECOND))\n",
    "SNT_SALARY_MIN_LIST = np.concatenate((SNT_SALARY_MIN_FIRST, SNT_SALARY_MIN_SECOND))\n",
    "SNT_SALARY_MAX_LIST = np.concatenate((SNT_SALARY_MAX_FIRST, SNT_SALARY_MAX_SECOND))\n",
    "SNT_DESCRIPTION_LIST = np.concatenate((SNT_DESCRIPTION_FIRST, SNT_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbc28f-d30a-4518-bf44-e6fec69f49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Science & Technology (JOBLUM) \n",
    "SNT={'Website': \"Joblum\",\n",
    "      'Job Title': SNT_TITLE_LIST, \n",
    "      'Category': \"Science & Technology\", \n",
    "      'Company': SNT_COMPANY_LIST, \n",
    "      'Date Posted': SNT_DATE_LIST, \n",
    "      'Location': SNT_LOCATION_LIST, \n",
    "      'Status': SNT_STATUS_LIST, \n",
    "      'Salary': SNT_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': SNT_DESCRIPTION_LIST,\n",
    "      'Min Salary': SNT_SALARY_MIN_LIST,\n",
    "      'Max Salary': SNT_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Science\"}\n",
    "SNT_df = pd.DataFrame(data=SNT)\n",
    "SNT_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed7f69-bc09-43e3-8351-2445d15d01d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNT_df.to_csv ('Joblum Data\\JOBLUM-SNT.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721054f8-aaec-41b1-bc22-7647f8410d0b",
   "metadata": {},
   "source": [
    "### CATEGORY - Practitioner/Medical Asst Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0e8b-9149-4d5f-b4ae-569542964091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Practitioner/Medical Asst Jobs (JOBLUM) - FIRST HALF\n",
    "\n",
    "PRAC_TITLE_FIRST = []\n",
    "PRAC_COMPANY_FIRST = []\n",
    "PRAC_DATE_FIRST = []\n",
    "PRAC_LOCATION_FIRST = []\n",
    "PRAC_STATUS_FIRST = []\n",
    "PRAC_SALARY_FIRST = []\n",
    "PRAC_SALARY_MIN_FIRST = []\n",
    "PRAC_SALARY_MAX_FIRST = []\n",
    "PRAC_DESCRIPTION_FIRST = []\n",
    "\n",
    "JOBLUM_URLs = 'https://ph.joblum.com/jobs-spec-practitioner-medical-asst?p='\n",
    "soup = getSoup(JOBLUM_URLs)\n",
    "NUM_JOBS = getNumJobs(soup)\n",
    "NUM_PAGES = getNumPages(NUM_JOBS)\n",
    "JOB_LINKS = getLinks(NUM_PAGES, JOBLUM_URLs)\n",
    "FIRST_HALF = math.ceil(NUM_PAGES/2)\n",
    "\n",
    "for i in range(FIRST_HALF):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PRAC_TITLE_FIRST.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PRAC_COMPANY_FIRST.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PRAC_DATE_FIRST.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PRAC_LOCATION_FIRST.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PRAC_STATUS_FIRST, PRAC_SALARY_FIRST, PRAC_SALARY_MIN_FIRST, \n",
    "              PRAC_SALARY_MAX_FIRST, PRAC_DESCRIPTION_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf95fc-3db1-47b2-a7ec-82dd33423ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Practitioner/Medical Asst Jobs (JOBLUM) - SECOND HALF\n",
    "\n",
    "PRAC_TITLE_SECOND = []\n",
    "PRAC_COMPANY_SECOND = []\n",
    "PRAC_DATE_SECOND = []\n",
    "PRAC_LOCATION_SECOND = []\n",
    "PRAC_STATUS_SECOND = []\n",
    "PRAC_SALARY_SECOND = []\n",
    "PRAC_SALARY_MIN_SECOND = []\n",
    "PRAC_SALARY_MAX_SECOND = []\n",
    "PRAC_DESCRIPTION_SECOND = []\n",
    "\n",
    "for i in range(FIRST_HALF, NUM_PAGES):\n",
    "    JOBLUM_SOUP = getSoup(JOB_LINKS[i])\n",
    "    URL = getJobURL(JOBLUM_SOUP)\n",
    "    JOB_TITLE_ARRAY = JOBLUM_SOUP.find_all('h2',{'class':'job-title'})\n",
    "    JOB_COMPANY_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'company-name'})\n",
    "    JOB_DATE_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'date date-desktop'})\n",
    "    JOB_LOCATION_ARRAY = JOBLUM_SOUP.find_all('span',{'class':'location location-desktop'})\n",
    "    for j in range(len(JOB_TITLE_ARRAY)):\n",
    "        PRAC_TITLE_SECOND.append(JOB_TITLE_ARRAY[j].text.strip())\n",
    "        PRAC_COMPANY_SECOND.append(JOB_COMPANY_ARRAY[j].text.strip())\n",
    "        PRAC_DATE_SECOND.append(datetime.strptime(JOB_DATE_ARRAY[j].text.strip(), '%B %d, %Y').date())\n",
    "        PRAC_LOCATION_SECOND.append(JOB_LOCATION_ARRAY[j].text.strip())\n",
    "    scrapeJob(URL, PRAC_STATUS_SECOND, PRAC_SALARY_SECOND, PRAC_SALARY_MIN_SECOND, \n",
    "              PRAC_SALARY_MAX_SECOND, PRAC_DESCRIPTION_SECOND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc880bdf-111c-4083-b287-63ced829944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining Scrape data of Practitioner/Medical Asst Jobs (JOBLUM) \n",
    "\n",
    "PRAC_TITLE_LIST = np.concatenate((PRAC_TITLE_FIRST, PRAC_TITLE_SECOND))\n",
    "PRAC_COMPANY_LIST = np.concatenate((PRAC_COMPANY_FIRST, PRAC_COMPANY_SECOND))\n",
    "PRAC_DATE_LIST = np.concatenate((PRAC_DATE_FIRST, PRAC_DATE_SECOND))\n",
    "PRAC_LOCATION_LIST = np.concatenate((PRAC_LOCATION_FIRST, PRAC_LOCATION_SECOND))\n",
    "PRAC_STATUS_LIST = np.concatenate((PRAC_STATUS_FIRST, PRAC_STATUS_SECOND))\n",
    "PRAC_SALARY_LIST = np.concatenate((PRAC_SALARY_FIRST, PRAC_SALARY_SECOND))\n",
    "PRAC_SALARY_MIN_LIST = np.concatenate((PRAC_SALARY_MIN_FIRST, PRAC_SALARY_MIN_SECOND))\n",
    "PRAC_SALARY_MAX_LIST = np.concatenate((PRAC_SALARY_MAX_FIRST, PRAC_SALARY_MAX_SECOND))\n",
    "PRAC_DESCRIPTION_LIST = np.concatenate((PRAC_DESCRIPTION_FIRST, PRAC_DESCRIPTION_SECOND))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3b3d4-7376-4673-959b-ca20c23e17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Data Frame for Practitioner/Medical Asst Jobs (JOBLUM) \n",
    "PRAC={'Website': \"Joblum\",\n",
    "      'Job Title': PRAC_TITLE_LIST, \n",
    "      'Category': \"Practitioner/Medical Asst Jobs\", \n",
    "      'Company': PRAC_COMPANY_LIST, \n",
    "      'Date Posted': PRAC_DATE_LIST, \n",
    "      'Location': PRAC_LOCATION_LIST, \n",
    "      'Status': PRAC_STATUS_LIST, \n",
    "      'Salary': PRAC_SALARY_LIST,\n",
    "      'Education': \"Not Specified / In Description\",\n",
    "      'Years of Work Experience': \"Not Specified / In Description\",\n",
    "      'Job Description': PRAC_DESCRIPTION_LIST,\n",
    "      'Min Salary': PRAC_SALARY_MIN_LIST,\n",
    "      'Max Salary': PRAC_SALARY_MAX_LIST,\n",
    "      'Min Years of Work Experience': \"Not Specified\",\n",
    "      'Max Years of Work Experience': \"Not Specified\",\n",
    "      'Field': \"Medicine\"}\n",
    "PRAC_df = pd.DataFrame(data=PRAC)\n",
    "PRAC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615dcfbc-f04b-4d09-b8f6-2e2cdf0f44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRAC_df.to_csv ('Joblum Data\\JOBLUM-PRAC.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "87e3197c-376b-4243-a9fd-6600df8479b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STAT_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-ef29aae8b4f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#concatenate all df (JOBLUM)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#The usage of this code is only applicable if the user was able to run the whole notbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m JOBLUM_df = pd.concat([STAT_df, AGRI_df, ARCHI_df, AVI_df, BIOMED_df, BIOTECH_df, \n\u001b[0m\u001b[0;32m      4\u001b[0m                        \u001b[0mCHEMENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCHEM_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIVILENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCONSTRUCTION_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDIAGNOSIS_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        \u001b[0mDOCTOR_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELEC_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECTRO_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mELECTROENG_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mENVI_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STAT_df' is not defined"
     ]
    }
   ],
   "source": [
    "#concatenate all df (JOBLUM) \n",
    "#The usage of this code is only applicable if the user was able to run the whole notbook\n",
    "JOBLUM_df = pd.concat([STAT_df, AGRI_df, ARCHI_df, AVI_df, BIOMED_df, BIOTECH_df, \n",
    "                       CHEMENG_df, CHEM_df, CIVILENG_df, CONSTRUCTION_df, DIAGNOSIS_df, \n",
    "                       DOCTOR_df, ELEC_df, ELECENG_df, ELECTRO_df, ELECTROENG_df, ENVI_df, \n",
    "                       ENVIENG_df, NUTRI_df, GEO_df, INDUSENG_df, IT_HARDWARE_df, IT_SYS_df, \n",
    "                       IT_SOFTWARE_df, MAINTENANCE_df, MECH_df, MECHENG_df, NURSE_df, OIL_df, \n",
    "                       OILENG_df, ENG_df, PHARMA_df, QUALITY_df, QUANTITY_df, SNT_df, PRAC_df], \n",
    "                      ignore_index=True, sort=False)\n",
    "JOBLUM_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
