{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "other-replication",
   "metadata": {},
   "source": [
    "# Job Portal - Pinoy Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-sphere",
   "metadata": {},
   "source": [
    "## Imports used\n",
    "\n",
    "* `os` - a module that provides functions to interact with the operating system.\n",
    "* `pandas` - is a tool that helps analyze data.\n",
    "* `numpy` - Library that contains multiple functions that help ease the work with arrays, matrices, and alike to better reassemble data.\n",
    "* `json` - enables import and export from and to JSON files\n",
    "* `re` - Short for Regular Expressions, help recognize patterns on strings of data and is used to orderly reassemble them.\n",
    "* `gensim` - Library that efficiently handles large, unmanaged text collections of data.\n",
    "* `nltk` - Short for Natural Language Toolkit. It helps the program to apply human language data to statistical natural language.\n",
    "* `requests` - Requests allows the program to send HTTP requests easily.\n",
    "* `Seaborn` - A library in python that is used to better visualize data through drawing informative graphs.\n",
    "* `math` - Imported library that allows quick computations of mathematical tasks\n",
    "* `gensim.utils` `simple_preprocess` - used to preprocess text by making them lower-cased, and transforming the words to their original form (de-tokenizing)\n",
    "* `gensim.parsing.preprocessing` `STOPWORDS` - stop words common words that do not have value and are often removed in pre-processing\n",
    "* `gensim` `corpora` - used to work with corpus and words\n",
    "* `gensim` `models` - used for topic modelling and model training\n",
    "* `nltk.stem` `WordNetLemmatizer` - used for grouping similar strings together\n",
    "* `bs4` `BeautifulSoup` - library used to web scrape HTML from websites\n",
    "* `datetime` `datetime` - An imported module in python to create an object that properly resembles date and time. Used for converting string of time into datetime format to month, day, and year.\n",
    "* `datetime` `timedelta` - used for finding delta of time ago with time scraped if date has minutes, hours, days, or weeks ago\n",
    "* `dateutil.relativedelta` `relativedelta` - used for finding delta of time ago with time scraped if date has months and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indian-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import requests\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14071684-7c18-4b95-9dca-0f3c30070ad0",
   "metadata": {},
   "source": [
    "\n",
    "Web scraping https://pinoyjobs.ph/ using Beautiful Soup.\n",
    "    \n",
    "### Fetch Data from the Website's Jobs in 'IT, Programming, Systems & Network' category (for testing)\n",
    "\n",
    "For Pinoy Jobs dataset, the Beautiful Soup method will be used to manually extract raw html text data from the website specified in the URL. This will return the html only of the relevant category that the group is interested in. This will return a list of related job results which will be further examined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://pinoyjobs.ph/job-hiring/category/it-programming-systems-networks/\"\n",
    "page=requests.get(URL)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-cream",
   "metadata": {},
   "source": [
    "### Get the Job URLS\n",
    "\n",
    "Taking a closer look at the html code of the website results, it can be observed that the jobs list only a few details of the job like the date, location, title, and the company looking. However the description where we can find further relevant details is cut off from view. Therefore, to be able to fully view the desrcription, the pages should be accessed and the data then extracted.\n",
    "Fortunately, we can also find the links of the job posts also. We now put all the links we can find into a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobURL_List = []\n",
    "i=0\n",
    "for div in soup.find_all('div', class_='card-content'):\n",
    "     for a in div.find_all('a', href=True):\n",
    "        jobURL_List.append(a['href'])\n",
    "        ##print(jobURL_List[1]\n",
    "        print (\"Found the URL:\", a['href'])\n",
    "        i=i+1\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-trade",
   "metadata": {},
   "source": [
    "### HTML Parsing of the Job Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobURL = jobURL_List[0]\n",
    "print(jobURL)\n",
    "jobPage=requests.get(jobURL)\n",
    "soupJobs = BeautifulSoup(jobPage.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-russia",
   "metadata": {},
   "source": [
    "### Getting the Job Title\n",
    "\n",
    "From the list of job URLS, the respective data listed below will be fetched from each one of the URLS in list jobURL_List to get the following:\n",
    "\n",
    "- Job_Title : The title of the job position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Title = soupJobs.findAll(\"h1\", {\"itemprop\": \"title\"})\n",
    "print (Job_Title[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-portugal",
   "metadata": {},
   "source": [
    "### Getting the Job Employment Type\n",
    " - Job_employmentType : Full-time/Part-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-missouri",
   "metadata": {},
   "source": [
    "Job_employmentType = soupJobs.findAll(\"li\", {\"itemprop\": \"employmentType\"})\n",
    "print (Job_employmentType[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-vertex",
   "metadata": {},
   "source": [
    "### Getting the Job Location\n",
    "- Job_jobLocation : The location where the position is stationed in, in some cases the employer writes this as Anywhere/Online or other terms for home-based jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_jobLocation = soupJobs.find_all('li',{\"itemprop\": \"addressLocality\"})\n",
    "print (Job_jobLocation[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-keyboard",
   "metadata": {},
   "source": [
    "### Getting the Date Posted\n",
    "- Job_dateposted : The date and time when the job was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_dateposted = soupJobs.find_all('li',{\"itemprop\": \"datePosted\"})\n",
    "print (Job_dateposted[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-yemen",
   "metadata": {},
   "source": [
    "### Getting the Job Salary\n",
    "- Job_salary : The expected salary in Philippine Peso (PHP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_salary = soupJobs.find_all('i', {\"itemprop\": \"salary\"})\n",
    "print(Job_salary[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-candy",
   "metadata": {},
   "source": [
    "for div in soupJobs.find_all('div', {\"itemprop\": \"description\"}):\n",
    "        Job_desc = div.find_all(\"p\")\n",
    "        for i in range(len(Job_desc)):\n",
    "            print (Job_desc[i].text)### Getting the Job Description\n",
    " - Job_desc : A detailed job description containing the requirements and responsibilities included in the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "for div in soupJobs.find_all('div', {\"itemprop\": \"description\"}):\n",
    "        Job_desc = div.find_all(\"p\")\n",
    "        for i in range(len(Job_desc)):\n",
    "            print (Job_desc[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-morning",
   "metadata": {},
   "source": [
    "### Extracting the Data\n",
    "\n",
    "Putting it all together, the job posts in categories:\n",
    "- Engineering, Construction, Electrical\n",
    "- IT, Programming, Systems & Networks\n",
    "- Manufacturing Production\n",
    "- Nursing, Medical, Dental Health\n",
    "- Sciences, Lab Research\n",
    "- Web Development, Design, HTML, SEO\n",
    "\n",
    "will be fetched and turned into a dataframe with the following variables:   \n",
    "- Job Type\n",
    "- Job Title\n",
    "- Employment Type\n",
    "- Office Location\n",
    "- Date Posted\n",
    "- Description\n",
    "- Salary\n",
    "- Job Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "PinoyJobs_URL_List = []\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/engineering-construction-electrical/\")\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/it-programming-systems-networks/\")\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/manufacturing-production/\")\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/nursing-medical-dental-health/\")\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/sciences-lab-research/\")\n",
    "PinoyJobs_URL_List.append(\"https://pinoyjobs.ph/job-hiring/category/web-development-design-html-seo/\")\n",
    "job_title_list = []\n",
    "job_employment_type_list = []\n",
    "job_jobLocation_list = []\n",
    "job_dateposted_list = []\n",
    "job_desc_list = []\n",
    "job_salary_list = []\n",
    "job_location_list = []\n",
    "job_type_list = []\n",
    "comapny_name_list = []\n",
    "for web_URL in range(len(PinoyJobs_URL_List)):\n",
    "    URL = PinoyJobs_URL_List[web_URL]\n",
    "    #print(PinoyJobs_URL_List[web_URL])\n",
    "    page=requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for ul in soup.find_all('ul', {\"class\": \"pagination hide-on-small-only\"}):\n",
    "        page_num = ul.find_all(\"li\")\n",
    "        max_page = int(page_num[len(page_num)-2].text.strip())\n",
    "        #print(page_num[len(page_num)-2].text)\n",
    "    for web_pages in range(0,max_page): ##max_page\n",
    "        URL = PinoyJobs_URL_List[web_URL] + \"page/{}/\".format(web_pages)\n",
    "        page=requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        jobURL_List = []\n",
    "        for div in soup.find_all('div', class_='card-content'):\n",
    "             for a in div.find_all('a', href=True):\n",
    "                jobURL_List.append(a['href'])\n",
    "                ##print (\"Found the URL:\", a['href'])\n",
    "        for k in range(len(jobURL_List)): ##len(jobURL_List)\n",
    "            jobURL = jobURL_List[k]\n",
    "            ##print(jobURL)\n",
    "            jobPage=requests.get(jobURL)\n",
    "            soupJobs = BeautifulSoup(jobPage.content, 'html.parser')\n",
    "\n",
    "            job_type = soup.findAll(\"h1\")\n",
    "            #print(job_type[0].text)\n",
    "            job_type_list.append(job_type[0].text)\n",
    "\n",
    "            Job_title = soupJobs.findAll(\"h1\", {\"itemprop\": \"title\"})\n",
    "            #print (titleinfo[0].text)\n",
    "            job_title_list.append(Job_title[0].text)\n",
    "\n",
    "            Company_name = soupJobs.find_all('h5',{\"itemprop\": \"hiringOrganization\"})\n",
    "            ##print (Company_name[0].text)\n",
    "            comapny_name_list.append(Company_name[0].text)\n",
    "\n",
    "            Job_employmentType = soupJobs.findAll(\"li\", {\"itemprop\": \"employmentType\"})\n",
    "            #print (Job_employmentType[0].text)\n",
    "            job_employment_type_list.append(Job_employmentType[0].text)\n",
    "\n",
    "            Job_jobLocation = soupJobs.find_all('li',{\"itemprop\": \"addressLocality\"})\n",
    "            #print (Job_jobLocation[0].text)\n",
    "            job_jobLocation_list.append(Job_jobLocation[0].text)\n",
    "\n",
    "            Job_dateposted = soupJobs.find_all('li',{\"itemprop\": \"datePosted\"})\n",
    "            #print (Job_dateposted[0].text)\n",
    "            job_dateposted_list.append(Job_dateposted[0].text)\n",
    "\n",
    "            Job_desc =  soupJobs.find_all('div', {\"itemprop\": \"description\"})\n",
    "            #print (Job_desc[0].text)\n",
    "            job_desc_list.append(Job_desc[0].text)\n",
    "\n",
    "            Job_salary = soupJobs.find_all('i', {\"itemprop\": \"salary\"})\n",
    "            #print(Job_salary[0].text)\n",
    "            job_salary_list.append(Job_salary[0].text)\n",
    "\n",
    "            Job_location = soupJobs.find_all('i')\n",
    "            #print (Job_location[len(Job_location)-1].text)\n",
    "            job_location_list.append(Job_location[len(Job_location)-1].text)\n",
    "\n",
    "    jobs_data={'Website': 'Pinoy Jobs',\n",
    "               'Job Title': job_title_list, \n",
    "               'Category': job_type_list,\n",
    "               'Company': comapny_name_list,\n",
    "               'Date Posted': job_dateposted_list,\n",
    "               'Location': job_location_list,\n",
    "               'Status': job_employment_type_list, \n",
    "               'Salary': job_salary_list,\n",
    "               #'Office Location': job_jobLocation_list, \n",
    "               'Education': \"Not Specified / In Description\",\n",
    "               'Years of Work Experience': \"Not Specified / In Description\",\n",
    "               'Job Description': job_desc_list,\n",
    "               }\n",
    "    pinoy_jobs_df = pd.DataFrame(data=jobs_data)\n",
    "    #Website \tJob Title \tCategory \tCompany \tDate Posted \tLocation \tStatus \tSalary \tEducation \tJob Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-world",
   "metadata": {},
   "source": [
    "### Parsing to JSON File\n",
    "\n",
    "Store the cleaned gathered data into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save DF to JSON file\n",
    "data = pinoy_jobs_df.to_json(orient='records')\n",
    "parsed = json.loads(data)\n",
    "json.dumps(parsed, indent=4) \n",
    "with open('pinoy_jobs.json', 'w') as json_file:\n",
    "    json.dump(parsed, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
