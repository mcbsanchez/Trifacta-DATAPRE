{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-climb",
   "metadata": {},
   "source": [
    "# Job Portal - Monster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-austin",
   "metadata": {},
   "source": [
    "## Imports used\n",
    "\n",
    "* `os` - a module that provides functions to interact with the operating system.\n",
    "* `pandas` - is a tool that helps analyze data.\n",
    "* `numpy` - Library that contains multiple functions that help ease the work with arrays, matrices, and alike to better reassemble data.\n",
    "* `json` - enables import and export from and to JSON files\n",
    "* `re` - Short for Regular Expressions, help recognize patterns on strings of data and is used to orderly reassemble them.\n",
    "* `gensim` - Library that efficiently handles large, unmanaged text collections of data.\n",
    "* `nltk` - Short for Natural Language Toolkit. It helps the program to apply human language data to statistical natural language.\n",
    "* `requests` - Requests allows the program to send HTTP requests easily.\n",
    "* `Seaborn` - A library in python that is used to better visualize data through drawing informative graphs.\n",
    "* `math` - Imported library that allows quick computations of mathematical tasks\n",
    "* `gensim.utils` `simple_preprocess` - used to preprocess text by making them lower-cased, and transforming the words to their original form (de-tokenizing)\n",
    "* `gensim.parsing.preprocessing` `STOPWORDS` - stop words common words that do not have value and are often removed in pre-processing\n",
    "* `gensim` `corpora` - used to work with corpus and words\n",
    "* `gensim` `models` - used for topic modelling and model training\n",
    "* `nltk.stem` `WordNetLemmatizer` - used for grouping similar strings together\n",
    "* `bs4` `BeautifulSoup` - library used to web scrape HTML from websites\n",
    "* `datetime` `datetime` - An imported module in python to create an object that properly resembles date and time. Used for converting string of time into datetime format to month, day, and year.\n",
    "* `datetime` `timedelta` - used for finding delta of time ago with time scraped if date has minutes, hours, days, or weeks ago\n",
    "* `dateutil.relativedelta` `relativedelta` - used for finding delta of time ago with time scraped if date has months and years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "political-chapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "import requests\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "today = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-tutorial",
   "metadata": {},
   "source": [
    "Monster is another job listing site that has been around for more than 20 years, providing a \"job board\" globally for job seeking, career management, recruitment, and talent management products and services. They are also one of the companies that take advantage of technology to create and deliver the best receruiting media, technologies, and platforms for connecting jobs and people via helping hire and find people jobs.\n",
    "\n",
    "## Web Scraping Data\n",
    "\n",
    "### Selecting Categories\n",
    "\n",
    "For web scraping the different jobs available, we selected job links in monster.com.ph's site that are related to the STEM course.monster_ph_df_json_1 = pd.read_json(r'Monster Data/monster_ph_IT.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sixth-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The List of Categories Relevant to the Paper\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\\\n",
    "           'AppleWebKit/537.36 (KHTML, like Gecko) '\\\n",
    "           'Chrome/75.0.3770.80 Safari/537.36'}\n",
    "Category_URL_List = ['https://www.monster.com.ph/search/it-computers-hardware-networking-jobs',\n",
    "                     'https://www.monster.com.ph/search/it-computers-software-jobs',\n",
    "                     'https://www.monster.com.ph/search/engineering-design-jobs',\n",
    "                     'https://www.monster.com.ph/search/electrical-switchgears-jobs',\n",
    "                     'https://www.monster.com.ph/search/chemicals-petrochemicals-jobs', \n",
    "                     'https://www.monster.com.ph/search/construction-engineering-jobs',\n",
    "                     'https://www.monster.com.ph/search/hospitals-healthcare-diagnostics-jobs']\n",
    "                     \n",
    "#Category_URL_List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-bicycle",
   "metadata": {},
   "source": [
    "### Parse HTML data\n",
    "\n",
    "We then try to get the all of the listed job URLs within that category via Parsing the data with Beautiful Soup. The links provided from `Category_URL_List` are then used to scrape through the various individual job listings and are given the following features in a dataframe:\n",
    "* `Website` - Monster PH\n",
    "* `Job Title` - name of the job listed\n",
    "* `Category` - job category based on Monster PH\n",
    "* `Company` - employer company\n",
    "* `Date Posted` - date job was posted\n",
    "* `Location` - where the work site is located\n",
    "* `Status` - employment type (Full/Part time)\n",
    "* `Salary` - not specified \n",
    "* `Education` - highest level of education attainment required\n",
    "* `Years of Work Experience` - years of working experience required\n",
    "* `Job Description` - body text description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kekw in range(len(Category_URL_List)):\n",
    "    #URL Page Gatherer\n",
    "    print (Category_URL_List[kekw])\n",
    "    isNext = True\n",
    "    isNext_Page = 1\n",
    "    isPer_Page = []\n",
    "    #kekw = 6\n",
    "    URL = Category_URL_List[kekw]\n",
    "    page=requests.get(URL,headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    while isNext:\n",
    "        page=requests.get(URL,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        for div in soup.findAll('div', {\"class\": \"srp-navigation\"}):\n",
    "            Test = div.find_all(\"button\")\n",
    "        if len(Test) > 1:\n",
    "            isOutput = Test[1].get_text().replace('\\n','').replace(' ','').replace('\\t','')\n",
    "            if isOutput == \"Next\":\n",
    "                #print(isOutput)\n",
    "                isNext_Page = isNext_Page + 1\n",
    "                #print(isNext_Page)\n",
    "                URL = Category_URL_List[kekw] + \"-{}?\".format(isNext_Page)\n",
    "                #print(URL)\n",
    "                isPer_Page.append(Category_URL_List[kekw] + \"-{}?\".format(isNext_Page))\n",
    "            else:\n",
    "                #print(isOutput)\n",
    "                #print(isNext_Page)\n",
    "                isNext = False\n",
    "        elif (len(Test) == 1):\n",
    "            isOutput = Test[0].get_text().replace('\\n','').replace(' ','').replace('\\t','')\n",
    "            if isOutput == \"Next\":\n",
    "                #print(isOutput)\n",
    "                #print(isNext_Page)\n",
    "                URL = Category_URL_List[kekw] + \"-{}?\".format(isNext_Page)\n",
    "                #print(URL)\n",
    "                isPer_Page.append(Category_URL_List[kekw] + \"-{}?\".format(isNext_Page))\n",
    "                isNext_Page = isNext_Page + 1\n",
    "            elif isOutput == \"Previous\":\n",
    "                #print(Test[0].get_text().replace('\\n',''))\n",
    "                #print (\"Test\")\n",
    "                #print(isNext_Page)\n",
    "                isNext = False\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    #Gathering Data \n",
    "    job_title_list = []\n",
    "    job_employment_type_list = []\n",
    "    job_jobLocation_list = []\n",
    "    job_dateposted_list = []\n",
    "    job_desc_list = []\n",
    "    job_salary_list = []\n",
    "    job_location_list = []\n",
    "    job_type_list = []\n",
    "    comapny_name_list = []\n",
    "    job_years_list = []\n",
    "    job_education_list = []\n",
    "\n",
    "    #The number of pages\n",
    "    for kek in range(0,len(isPer_Page)): ##len(isPer_Page)\n",
    "        #print(\"Page Number: \",kek)\n",
    "        #Getting the Job URL per page\n",
    "        URL = isPer_Page[kek]\n",
    "        page=requests.get(URL,headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        jobURL_List = []\n",
    "        i=0\n",
    "        for div in soup.find_all('div', class_='job-tittle'):\n",
    "            isSponsored = div.find_all('div', class_='sponsr') \n",
    "            if(len(isSponsored)):\n",
    "                print(\"do nothing\")\n",
    "            else:\n",
    "                for h3 in div.find_all('h3', class_='medium'):\n",
    "                     for a in h3.find_all('a', href=True):\n",
    "                        jobURL_List.append(a['href'])\n",
    "                        i=i+1\n",
    "\n",
    "        #Getting the Info of the JOB                \n",
    "\n",
    "        for i in range(len(jobURL_List)):\n",
    "            Monster_URL = jobURL_List[i]\n",
    "            Monster_page=requests.get(Monster_URL,headers=headers)\n",
    "            Monstersoup = BeautifulSoup(Monster_page.content, 'html.parser')\n",
    "            for div in Monstersoup.findAll('div', {\"class\": \"job-tittle detail-job-tittle jd-mt-0\"}):\n",
    "                    Job_Title = div.findAll('h1')\n",
    "                    Company_name = div.findAll('a')\n",
    "                    Office_Location = div.findAll('span',{\"class\": \"loc jd-loc\"} )\n",
    "                    Salary = div.findAll('span',{\"class\": \"package\"} )\n",
    "                    Years_WE = div.findAll(\"span\",{\"class\": \"loc\"})\n",
    "            Job_location =  Monstersoup.findAll(\"div\",{\"class\": \"posted-update pl5 w100\"})\n",
    "            Date_posted = Monstersoup.findAll('span',{\"class\": \"posted seprator pLR-10\"} )\n",
    "            Job_Description = Monstersoup.findAll('div', {\"class\": \"card-body card-body-apply pd20\"})\n",
    "            Job_detail = Monstersoup.findAll('div', {\"class\": \"job-detail-list\"})\n",
    "            #Job_Education = Monstersoup.findAll('a', {\"href\": \"https://www.monster.com.ph/search/bachelors-degree-jobs\"})\n",
    "            Job_Education = Monstersoup.findAll('div', {\"class\": \"job-detail-list\"})\n",
    "\n",
    "            #Data Appending\n",
    "            try:\n",
    "                job_title_list.append(Job_Title[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                comapny_name_list.append(\"Error\")\n",
    "\n",
    "\n",
    "            try:\n",
    "                comapny_name_list.append(Company_name[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                comapny_name_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_employment_type_list.append(Job_detail[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_employment_type_list.append(\"Not Specified\")\n",
    "            try:    \n",
    "                job_type_list.append(Job_detail[1].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_type_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_jobLocation_list.append(Office_Location[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_jobLocation_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_dateposted_list.append(Date_posted[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_dateposted_list.append(\"Not Specified\")\n",
    "\n",
    "            try:            \n",
    "                job_desc_list.append(Job_Description[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_desc_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_salary_list.append(Salary[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_salary_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_location_list.append(Job_location[0].get_text().replace('\\n',' '))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_location_list.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_years_list.append(Years_WE[1].get_text().replace('\\n',''))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_years_list.append.append(\"Not Specified\")\n",
    "\n",
    "            try:\n",
    "                job_education_list.append(Job_Education[5].get_text().replace('\\n',''))\n",
    "            except:\n",
    "                #print(\"An exception occurred\") \n",
    "                #print(Job_Title[0].get_text().replace('\\n',' '))\n",
    "                #print(jobURL_List[i]) \n",
    "                job_education_list.append(\"Not Specified\")\n",
    "\n",
    "            jobs_data={'Website:': \"Monster PH\",\n",
    "                       'Job Title': job_title_list, \n",
    "                       'Category': job_type_list,\n",
    "                       'Company': comapny_name_list,\n",
    "                       'Date Posted': job_dateposted_list, \n",
    "                       'Location': job_location_list,\n",
    "                       'Status': job_employment_type_list, \n",
    "                       'Salary': job_salary_list,\n",
    "                       'Education': job_education_list,\n",
    "                       'Years of Work Experience': job_years_list,\n",
    "                       'Job Description': job_desc_list,\n",
    "                       'Office Location': job_jobLocation_list, \n",
    "                       }\n",
    "            monster_jobs_df = pd.DataFrame(data=jobs_data)\n",
    "\n",
    "    if(kekw == 0):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_IT_HW.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 1):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_IT_SW.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 2):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_ENG_DE.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 3):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_ELEC_SG.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 4):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_CHEM_ENG.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 5):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_CON_ENG.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n",
    "    elif(kekw == 6):\n",
    "        data = monster_jobs_df.to_json(orient='records')\n",
    "        parsed = json.loads(data)\n",
    "        json.dumps(parsed, indent=4) \n",
    "        with open('monster_ph_HEALTH.json', 'w') as json_file:\n",
    "            json.dump(parsed, json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
